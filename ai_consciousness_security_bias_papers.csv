title,authors,published,summary,url
System 2 reasoning capabilities are nigh,Scott C. Lowe,2024-10-04,"In recent years, machine learning models have made strides towards human-like
reasoning capabilities from several directions. In this work, we review the
current state of the literature and describe the remaining steps to achieve a
neural model which can perform System 2 reasoning analogous to a human. We
argue that if current models are insufficient to be classed as performing
reasoning, there remains very little additional progress needed to attain that
goal.",http://arxiv.org/pdf/2410.03662v1
Geometric Representation Condition Improves Equivariant Molecule Generation,"Zian Li, Cai Zhou, Xiyuan Wang, Xingang Peng, Muhan Zhang",2024-10-04,"Recent advancements in molecular generative models have demonstrated
substantial potential in accelerating scientific discovery, particularly in
drug design. However, these models often face challenges in generating
high-quality molecules, especially in conditional scenarios where specific
molecular properties must be satisfied. In this work, we introduce GeoRCG, a
general framework to enhance the performance of molecular generative models by
integrating geometric representation conditions. We decompose the molecule
generation process into two stages: first, generating an informative geometric
representation; second, generating a molecule conditioned on the
representation. Compared to directly generating a molecule, the relatively
easy-to-generate representation in the first-stage guides the second-stage
generation to reach a high-quality molecule in a more goal-oriented and much
faster way. Leveraging EDM as the base generator, we observe significant
quality improvements in unconditional molecule generation on the widely-used
QM9 and GEOM-DRUG datasets. More notably, in the challenging conditional
molecular generation task, our framework achieves an average 31\% performance
improvement over state-of-the-art approaches, highlighting the superiority of
conditioning on semantically rich geometric representations over conditioning
on individual property values as in previous approaches. Furthermore, we show
that, with such representation guidance, the number of diffusion steps can be
reduced to as small as 100 while maintaining superior generation quality than
that achieved with 1,000 steps, thereby significantly accelerating the
generation process.",http://arxiv.org/pdf/2410.03655v1
GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs,"Pu Hua, Minghuan Liu, Annabella Macaluso, Yunfeng Lin, Weinan Zhang, Huazhe Xu, Lirui Wang",2024-10-04,"Robotic simulation today remains challenging to scale up due to the human
efforts required to create diverse simulation tasks and scenes.
Simulation-trained policies also face scalability issues as many sim-to-real
methods focus on a single task. To address these challenges, this work proposes
GenSim2, a scalable framework that leverages coding LLMs with multi-modal and
reasoning capabilities for complex and realistic simulation task creation,
including long-horizon tasks with articulated objects. To automatically
generate demonstration data for these tasks at scale, we propose planning and
RL solvers that generalize within object categories. The pipeline can generate
data for up to 100 articulated tasks with 200 objects and reduce the required
human efforts. To utilize such data, we propose an effective multi-task
language-conditioned policy architecture, dubbed proprioceptive point-cloud
transformer (PPT), that learns from the generated demonstrations and exhibits
strong sim-to-real zero-shot transfer. Combining the proposed pipeline and the
policy architecture, we show a promising usage of GenSim2 that the generated
data can be used for zero-shot transfer or co-train with real-world collected
data, which enhances the policy performance by 20% compared with training
exclusively on limited real data.",http://arxiv.org/pdf/2410.03645v1
What Matters for Model Merging at Scale?,"Prateek Yadav, Tu Vu, Jonathan Lai, Alexandra Chronopoulou, Manaal Faruqui, Mohit Bansal, Tsendsuren Munkhdalai",2024-10-04,"Model merging aims to combine multiple expert models into a more capable
single model, offering benefits such as reduced storage and serving costs,
improved generalization, and support for decentralized model development.
Despite its promise, previous studies have primarily focused on merging a few
small models. This leaves many unanswered questions about the effect of scaling
model size and how it interplays with other key factors -- like the base model
quality and number of expert models -- , to affect the merged model's
performance. This work systematically evaluates the utility of model merging at
scale, examining the impact of these different factors. We experiment with
merging fully fine-tuned models using 4 popular merging methods -- Averaging,
Task~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B
parameters and merging up to 8 different expert models. We evaluate the merged
models on both held-in tasks, i.e., the expert's training tasks, and zero-shot
generalization to unseen held-out tasks. Our experiments provide several new
insights about model merging at scale and the interplay between different
factors. First, we find that merging is more effective when experts are created
from strong base models, i.e., models with good zero-shot performance. Second,
larger models facilitate easier merging. Third merging consistently improves
generalization capabilities. Notably, when merging 8 large expert models, the
merged models often generalize better compared to the multitask trained models.
Fourth, we can better merge more expert models when working with larger models.
Fifth, different merging methods behave very similarly at larger scales.
Overall, our findings shed light on some interesting properties of model
merging while also highlighting some limitations. We hope that this study will
serve as a reference point on large-scale merging for upcoming research.",http://arxiv.org/pdf/2410.03617v1
TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation,"Jonathan Cook, Tim Rocktäschel, Jakob Foerster, Dennis Aumiller, Alex Wang",2024-10-04,"Given the widespread adoption and usage of Large Language Models (LLMs), it
is crucial to have flexible and interpretable evaluations of their
instruction-following ability. Preference judgments between model outputs have
become the de facto evaluation standard, despite distilling complex,
multi-faceted preferences into a single ranking. Furthermore, as human
annotation is slow and costly, LLMs are increasingly used to make these
judgments, at the expense of reliability and interpretability. In this work, we
propose TICK (Targeted Instruct-evaluation with ChecKlists), a fully automated,
interpretable evaluation protocol that structures evaluations with
LLM-generated, instruction-specific checklists. We first show that, given an
instruction, LLMs can reliably produce high-quality, tailored evaluation
checklists that decompose the instruction into a series of YES/NO questions.
Each question asks whether a candidate response meets a specific requirement of
the instruction. We demonstrate that using TICK leads to a significant increase
(46.4% $\to$ 52.2%) in the frequency of exact agreements between LLM judgements
and human preferences, as compared to having an LLM directly score an output.
We then show that STICK (Self-TICK) can be used to improve generation quality
across multiple benchmarks via self-refinement and Best-of-N selection. STICK
self-refinement on LiveBench reasoning tasks leads to an absolute gain of
$+$7.8%, whilst Best-of-N selection with STICK attains $+$6.3% absolute
improvement on the real-world instruction dataset, WildBench. In light of this,
structured, multi-faceted self-improvement is shown to be a promising way to
further advance LLM capabilities. Finally, by providing LLM-generated
checklists to human evaluators tasked with directly scoring LLM responses to
WildBench instructions, we notably increase inter-annotator agreement (0.194
$\to$ 0.256).",http://arxiv.org/pdf/2410.03608v1
Understanding Reasoning in Chain-of-Thought from the Hopfieldian View,"Lijie Hu, Liang Liu, Shu Yang, Xin Chen, Zhen Tan, Muhammad Asif Ali, Mengdi Li, Di Wang",2024-10-04,"Large Language Models have demonstrated remarkable abilities across various
tasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to
enhance reasoning capabilities. However, existing research primarily focuses on
improving performance, lacking a comprehensive framework to explain and
understand the fundamental factors behind CoT's success. To bridge this gap, we
introduce a novel perspective grounded in the Hopfieldian view of cognition in
cognitive neuroscience. We establish a connection between CoT reasoning and key
cognitive elements such as stimuli, actions, neural populations, and
representation spaces. From our view, we can understand the reasoning process
as the movement between these representation spaces. Building on this insight,
we develop a method for localizing reasoning errors in the response of CoTs.
Moreover, we propose the Representation-of-Thought (RoT) framework, which
leverages the robustness of low-dimensional representation spaces to enhance
the robustness of the reasoning process in CoTs. Experimental results
demonstrate that RoT improves the robustness and interpretability of CoT
reasoning while offering fine-grained control over the reasoning process.",http://arxiv.org/pdf/2410.03595v1
Training on more Reachable Tasks for Generalisation in Reinforcement Learning,"Max Weltevrede, Caroline Horsch, Matthijs T. J. Spaan, Wendelin Böhmer",2024-10-04,"In multi-task reinforcement learning, agents train on a fixed set of tasks
and have to generalise to new ones. Recent work has shown that increased
exploration improves this generalisation, but it remains unclear why exactly
that is. In this paper, we introduce the concept of reachability in multi-task
reinforcement learning and show that an initial exploration phase increases the
number of reachable tasks the agent is trained on. This, and not the increased
exploration, is responsible for the improved generalisation, even to
unreachable tasks. Inspired by this, we propose a novel method Explore-Go that
implements such an exploration phase at the beginning of each episode.
Explore-Go only modifies the way experience is collected and can be used with
most existing on-policy or off-policy reinforcement learning algorithms. We
demonstrate the effectiveness of our method when combined with some popular
algorithms and show an increase in generalisation performance across several
environments.",http://arxiv.org/pdf/2410.03565v1
færdXel: An Expert System for Danish Traffic Law,"Luís Cruz-Filipe, Jonas Vistrup",2024-10-04,"We present f{\ae}rdXel, a tool for symbolic reasoning in the domain of Danish
traffic law. f{\ae}rdXel combines techniques from logic programming with a
novel interface that allows users to navigate through its reasoning process,
thereby ensuring the system's trustworthiness. A preliminary empirical
evaluation indicates that this work is seen as very promising, and has the
potential to become a foundation for real-world AI tools supporting
professionals in the Danish legal sector.",http://arxiv.org/pdf/2410.03560v1
Ward: Provable RAG Dataset Inference via LLM Watermarks,"Nikola Jovanović, Robin Staab, Maximilian Baader, Martin Vechev",2024-10-04,"Retrieval-Augmented Generation (RAG) improves LLMs by enabling them to
incorporate external data during generation. This raises concerns for data
owners regarding unauthorized use of their content in RAG systems. Despite its
importance, the challenge of detecting such unauthorized usage remains
underexplored, with existing datasets and methodologies from adjacent fields
being ill-suited for its study. In this work, we take several steps to bridge
this gap. First, we formalize this problem as (black-box) RAG Dataset Inference
(RAG-DI). To facilitate research on this challenge, we further introduce a
novel dataset specifically designed for benchmarking RAG-DI methods under
realistic conditions, and propose a set of baseline approaches. Building on
this foundation, we introduce Ward, a RAG-DI method based on LLM watermarks
that enables data owners to obtain rigorous statistical guarantees regarding
the usage of their dataset in a RAG system. In our experimental evaluation, we
show that Ward consistently outperforms all baselines across many challenging
settings, achieving higher accuracy, superior query efficiency and robustness.
Our work provides a foundation for future studies of RAG-DI and highlights LLM
watermarks as a promising approach to this problem.",http://arxiv.org/pdf/2410.03537v1
A Probabilistic Perspective on Unlearning and Alignment for Large Language Models,"Yan Scholten, Stephan Günnemann, Leo Schwinn",2024-10-04,"Comprehensive evaluation of Large Language Models (LLMs) is an open research
problem. Existing evaluations rely on deterministic point estimates generated
via greedy decoding. However, we find that deterministic evaluations fail to
capture the whole output distribution of a model, yielding inaccurate
estimations of model capabilities. This is particularly problematic in critical
contexts such as unlearning and alignment, where precise model evaluations are
crucial. To remedy this, we introduce the first formal probabilistic evaluation
framework in LLMs. Namely, we derive novel metrics with high-probability
guarantees concerning the output distribution of a model. Our metrics are
application-independent and allow practitioners to make more reliable estimates
about model capabilities before deployment. Through a case study focused on
unlearning, we reveal that deterministic evaluations falsely indicate
successful unlearning, whereas our probabilistic evaluations demonstrate that
most if not all of the supposedly unlearned information remains accessible in
these models. Additionally, we propose a novel unlearning loss based on entropy
optimization and adaptive temperature scaling, which significantly improves
unlearning in probabilistic settings on recent benchmarks. Our proposed shift
from point estimates to probabilistic evaluations of output distributions
represents an important step toward comprehensive evaluations of LLMs.
https://github.com/yascho/probabilistic-unlearning",http://arxiv.org/pdf/2410.03523v1
FedStein: Enhancing Multi-Domain Federated Learning Through James-Stein Estimator,"Sunny Gupta, Nikita Jangid, Amit Sethi",2024-10-04,"Federated Learning (FL) facilitates data privacy by enabling collaborative
in-situ training across decentralized clients. Despite its inherent advantages,
FL faces significant challenges of performance and convergence when dealing
with data that is not independently and identically distributed (non-i.i.d.).
While previous research has primarily addressed the issue of skewed label
distribution across clients, this study focuses on the less explored challenge
of multi-domain FL, where client data originates from distinct domains with
varying feature distributions. We introduce a novel method designed to address
these challenges FedStein: Enhancing Multi-Domain Federated Learning Through
the James-Stein Estimator. FedStein uniquely shares only the James-Stein (JS)
estimates of batch normalization (BN) statistics across clients, while
maintaining local BN parameters. The non-BN layer parameters are exchanged via
standard FL techniques. Extensive experiments conducted across three datasets
and multiple models demonstrate that FedStein surpasses existing methods such
as FedAvg and FedBN, with accuracy improvements exceeding 14% in certain
domains leading to enhanced domain generalization. The code is available at
https://github.com/sunnyinAI/FedStein",http://arxiv.org/pdf/2410.03499v1
Generative Artificial Intelligence for Navigating Synthesizable Chemical Space,"Wenhao Gao, Shitong Luo, Connor W. Coley",2024-10-04,"We introduce SynFormer, a generative modeling framework designed to
efficiently explore and navigate synthesizable chemical space. Unlike
traditional molecular generation approaches, we generate synthetic pathways for
molecules to ensure that designs are synthetically tractable. By incorporating
a scalable transformer architecture and a diffusion module for building block
selection, SynFormer surpasses existing models in synthesizable molecular
design. We demonstrate SynFormer's effectiveness in two key applications: (1)
local chemical space exploration, where the model generates synthesizable
analogs of a reference molecule, and (2) global chemical space exploration,
where the model aims to identify optimal molecules according to a black-box
property prediction oracle. Additionally, we demonstrate the scalability of our
approach via the improvement in performance as more computational resources
become available. With our code and trained models openly available, we hope
that SynFormer will find use across applications in drug discovery and
materials science.",http://arxiv.org/pdf/2410.03494v1
A Multimodal Framework for Deepfake Detection,"Kashish Gandhi, Prutha Kulkarni, Taran Shah, Piyush Chaudhari, Meera Narvekar, Kranti Ghag",2024-10-04,"The rapid advancement of deepfake technology poses a significant threat to
digital media integrity. Deepfakes, synthetic media created using AI, can
convincingly alter videos and audio to misrepresent reality. This creates risks
of misinformation, fraud, and severe implications for personal privacy and
security. Our research addresses the critical issue of deepfakes through an
innovative multimodal approach, targeting both visual and auditory elements.
This comprehensive strategy recognizes that human perception integrates
multiple sensory inputs, particularly visual and auditory information, to form
a complete understanding of media content. For visual analysis, a model that
employs advanced feature extraction techniques was developed, extracting nine
distinct facial characteristics and then applying various machine learning and
deep learning models. For auditory analysis, our model leverages
mel-spectrogram analysis for feature extraction and then applies various
machine learning and deep learningmodels. To achieve a combined analysis, real
and deepfake audio in the original dataset were swapped for testing purposes
and ensured balanced samples. Using our proposed models for video and audio
classification i.e. Artificial Neural Network and VGG19, the overall sample is
classified as deepfake if either component is identified as such. Our
multimodal framework combines visual and auditory analyses, yielding an
accuracy of 94%.",http://arxiv.org/pdf/2410.03487v1
Group Fairness in Peer Review,"Haris Aziz, Evi Micha, Nisarg Shah",2024-10-04,"Large conferences such as NeurIPS and AAAI serve as crossroads of various AI
fields, since they attract submissions from a vast number of communities.
However, in some cases, this has resulted in a poor reviewing experience for
some communities, whose submissions get assigned to less qualified reviewers
outside of their communities. An often-advocated solution is to break up any
such large conference into smaller conferences, but this can lead to isolation
of communities and harm interdisciplinary research. We tackle this challenge by
introducing a notion of group fairness, called the core, which requires that
every possible community (subset of researchers) to be treated in a way that
prevents them from unilaterally benefiting by withdrawing from a large
conference.
  We study a simple peer review model, prove that it always admits a reviewing
assignment in the core, and design an efficient algorithm to find one such
assignment. We use real data from CVPR and ICLR conferences to compare our
algorithm to existing reviewing assignment algorithms on a number of metrics.",http://arxiv.org/pdf/2410.03474v1
Vulnerability Detection via Topological Analysis of Attention Maps,"Pavel Snopov, Andrey Nikolaevich Golubinskiy",2024-10-04,"Recently, deep learning (DL) approaches to vulnerability detection have
gained significant traction. These methods demonstrate promising results, often
surpassing traditional static code analysis tools in effectiveness.
  In this study, we explore a novel approach to vulnerability detection
utilizing the tools from topological data analysis (TDA) on the attention
matrices of the BERT model. Our findings reveal that traditional machine
learning (ML) techniques, when trained on the topological features extracted
from these attention matrices, can perform competitively with pre-trained
language models (LLMs) such as CodeBERTa. This suggests that TDA tools,
including persistent homology, are capable of effectively capturing semantic
information critical for identifying vulnerabilities.",http://arxiv.org/pdf/2410.03470v1
Diffusion State-Guided Projected Gradient for Inverse Problems,"Rayhan Zirvi, Bahareh Tolooshams, Anima Anandkumar",2024-10-04,"Recent advancements in diffusion models have been effective in learning data
priors for solving inverse problems. They leverage diffusion sampling steps for
inducing a data prior while using a measurement guidance gradient at each step
to impose data consistency. For general inverse problems, approximations are
needed when an unconditionally trained diffusion model is used since the
measurement likelihood is intractable, leading to inaccurate posterior
sampling. In other words, due to their approximations, these methods fail to
preserve the generation process on the data manifold defined by the diffusion
prior, leading to artifacts in applications such as image restoration. To
enhance the performance and robustness of diffusion models in solving inverse
problems, we propose Diffusion State-Guided Projected Gradient (DiffStateGrad),
which projects the measurement gradient onto a subspace that is a low-rank
approximation of an intermediate state of the diffusion process. DiffStateGrad,
as a module, can be added to a wide range of diffusion-based inverse solvers to
improve the preservation of the diffusion process on the prior manifold and
filter out artifact-inducing components. We highlight that DiffStateGrad
improves the robustness of diffusion models in terms of the choice of
measurement guidance step size and noise while improving the worst-case
performance. Finally, we demonstrate that DiffStateGrad improves upon the
state-of-the-art on linear and nonlinear image restoration inverse problems.",http://arxiv.org/pdf/2410.03463v1
How Toxicity Classifiers and Large Language Models Respond to Ableism,"Mahika Phutane, Ananya Seelam, Aditya Vashistha",2024-10-04,"People with disabilities (PwD) regularly encounter ableist hate and
microaggressions online. While online platforms use machine learning models to
moderate online harm, there is little research investigating how these models
interact with ableism. In this paper, we curated a dataset of 100 social media
comments targeted towards PwD, and recruited 160 participants to rate and
explain how toxic and ableist these comments were. We then prompted
state-of-the art toxicity classifiers (TCs) and large language models (LLMs) to
rate and explain the harm. Our analysis revealed that TCs and LLMs rated
toxicity significantly lower than PwD, but LLMs rated ableism generally on par
with PwD. However, ableism explanations by LLMs overlooked emotional harm, and
lacked specificity and acknowledgement of context, important facets of PwD
explanations. Going forward, we discuss challenges in designing
disability-aware toxicity classifiers, and advocate for the shift from ableism
detection to ableism interpretation and explanation.",http://arxiv.org/pdf/2410.03448v1
A General Framework for Producing Interpretable Semantic Text Embeddings,"Yiqun Sun, Qiang Huang, Yixuan Tang, Anthony K. H. Tung, Jun Yu",2024-10-04,"Semantic text embedding is essential to many tasks in Natural Language
Processing (NLP). While black-box models are capable of generating high-quality
embeddings, their lack of interpretability limits their use in tasks that
demand transparency. Recent approaches have improved interpretability by
leveraging domain-expert-crafted or LLM-generated questions, but these methods
rely heavily on expert input or well-prompt design, which restricts their
generalizability and ability to generate discriminative questions across a wide
range of tasks. To address these challenges, we introduce \algo{CQG-MBQA}
(Contrastive Question Generation - Multi-task Binary Question Answering), a
general framework for producing interpretable semantic text embeddings across
diverse tasks. Our framework systematically generates highly discriminative,
low cognitive load yes/no questions through the \algo{CQG} method and answers
them efficiently with the \algo{MBQA} model, resulting in interpretable
embeddings in a cost-effective manner. We validate the effectiveness and
interpretability of \algo{CQG-MBQA} through extensive experiments and ablation
studies, demonstrating that it delivers embedding quality comparable to many
advanced black-box models while maintaining inherently interpretability.
Additionally, \algo{CQG-MBQA} outperforms other interpretable text embedding
methods across various downstream tasks.",http://arxiv.org/pdf/2410.03435v1
EB-NeRD: A Large-Scale Dataset for News Recommendation,"Johannes Kruse, Kasper Lindskow, Saikishore Kalloori, Marco Polignano, Claudio Pomo, Abhishek Srivastava, Anshuk Uppal, Michael Riis Andersen, Jes Frellsen",2024-10-04,"Personalized content recommendations have been pivotal to the content
experience in digital media from video streaming to social networks. However,
several domain specific challenges have held back adoption of recommender
systems in news publishing. To address these challenges, we introduce the
Ekstra Bladet News Recommendation Dataset (EB-NeRD). The dataset encompasses
data from over a million unique users and more than 37 million impression logs
from Ekstra Bladet. It also includes a collection of over 125,000 Danish news
articles, complete with titles, abstracts, bodies, and metadata, such as
categories. EB-NeRD served as the benchmark dataset for the RecSys '24
Challenge, where it was demonstrated how the dataset can be used to address
both technical and normative challenges in designing effective and responsible
recommender systems for news publishing. The dataset is available at:
https://recsys.eb.dk.",http://arxiv.org/pdf/2410.03432v1
Cayley Graph Propagation,"JJ Wilson, Maya Bechler-Speicher, Petar Veličković",2024-10-04,"In spite of the plethora of success stories with graph neural networks (GNNs)
on modelling graph-structured data, they are notoriously vulnerable to
over-squashing, whereby tasks necessitate the mixing of information between
distance pairs of nodes. To address this problem, prior work suggests rewiring
the graph structure to improve information flow. Alternatively, a significant
body of research has dedicated itself to discovering and precomputing
bottleneck-free graph structures to ameliorate over-squashing. One well
regarded family of bottleneck-free graphs within the mathematical community are
expander graphs, with prior work$\unicode{x2014}$Expander Graph Propagation
(EGP)$\unicode{x2014}$proposing the use of a well-known expander graph
family$\unicode{x2014}$the Cayley graphs of the $\mathrm{SL}(2,\mathbb{Z}_n)$
special linear group$\unicode{x2014}$as a computational template for GNNs.
However, in EGP the computational graphs used are truncated to align with a
given input graph. In this work, we show that truncation is detrimental to the
coveted expansion properties. Instead, we propose CGP, a method to propagate
information over a complete Cayley graph structure, thereby ensuring it is
bottleneck-free to better alleviate over-squashing. Our empirical evidence
across several real-world datasets not only shows that CGP recovers significant
improvements as compared to EGP, but it is also akin to or outperforms
computationally complex graph rewiring techniques.",http://arxiv.org/pdf/2410.03424v1
Towards Real-time Intrahepatic Vessel Identification in Intraoperative Ultrasound-Guided Liver Surgery,"Karl-Philippe Beaudet, Alexandros Karargyris, Sidaty El Hadramy, Stéphane Cotin, Jean-Paul Mazellier, Nicolas Padoy, Juan Verde",2024-10-04,"While laparoscopic liver resection is less prone to complications and
maintains patient outcomes compared to traditional open surgery, its complexity
hinders widespread adoption due to challenges in representing the liver's
internal structure. Laparoscopic intraoperative ultrasound offers efficient,
cost-effective and radiation-free guidance. Our objective is to aid physicians
in identifying internal liver structures using laparoscopic intraoperative
ultrasound. We propose a patient-specific approach using preoperative 3D
ultrasound liver volume to train a deep learning model for real-time
identification of portal tree and branch structures. Our personalized AI model,
validated on ex vivo swine livers, achieved superior precision (0.95) and
recall (0.93) compared to surgeons, laying groundwork for precise vessel
identification in ultrasound-based liver resection. Its adaptability and
potential clinical impact promise to advance surgical interventions and improve
patient care.",http://arxiv.org/pdf/2410.03420v1
Comparative study of regression vs pairwise models for surrogate-based heuristic optimisation,"Pablo S. Naharro, Pablo Toharia, Antonio LaTorre, José-María Peña",2024-10-04,"Heuristic optimisation algorithms explore the search space by sampling
solutions, evaluating their fitness, and biasing the search in the direction of
promising solutions. However, in many cases, this fitness function involves
executing expensive computational calculations, drastically reducing the
reasonable number of evaluations. In this context, surrogate models have
emerged as an excellent alternative to alleviate these computational problems.
This paper addresses the formulation of surrogate problems as both regression
models that approximate fitness (surface surrogate models) and a novel way to
connect classification models (pairwise surrogate models). The pairwise
approach can be directly exploited by some algorithms, such as Differential
Evolution, in which the fitness value is not actually needed to drive the
search, and it is sufficient to know whether a solution is better than another
one or not. Based on these modelling approaches, we have conducted a
multidimensional analysis of surrogate models under different configurations:
different machine learning algorithms (regularised regression, neural networks,
decision trees, boosting methods, and random forests), different surrogate
strategies (encouraging diversity or relaxing prediction thresholds), and
compare them for both surface and pairwise surrogate models. The experimental
part of the article includes the benchmark problems already proposed for the
SOCO2011 competition in continuous optimisation and a simulation problem
included in the recent GECCO2021 Industrial Challenge. This paper shows that
the performance of the overall search, when using online machine learning-based
surrogate models, depends not only on the accuracy of the predictive model but
also on both the kind of bias towards positive or negative cases and how the
optimisation uses those predictions to decide whether to execute the actual
fitness function.",http://arxiv.org/pdf/2410.03409v1
EBES: Easy Benchmarking for Event Sequences,"Dmitry Osin, Igor Udovichenko, Viktor Moskvoretskii, Egor Shvetsov, Evgeny Burnaev",2024-10-04,"Event sequences, characterized by irregular sampling intervals and a mix of
categorical and numerical features, are common data structures in various
real-world domains such as healthcare, finance, and user interaction logs.
Despite advances in temporal data modeling techniques, there is no standardized
benchmarks for evaluating their performance on event sequences. This
complicates result comparison across different papers due to varying evaluation
protocols, potentially misleading progress in this field. We introduce EBES, a
comprehensive benchmarking tool with standardized evaluation scenarios and
protocols, focusing on regression and classification problems with
sequence-level targets. Our library simplifies benchmarking, dataset addition,
and method integration through a unified interface. It includes a novel
synthetic dataset and provides preprocessed real-world datasets, including the
largest publicly available banking dataset. Our results provide an in-depth
analysis of datasets, identifying some as unsuitable for model comparison. We
investigate the importance of modeling temporal and sequential components, as
well as the robustness and scaling properties of the models. These findings
highlight potential directions for future research. Our benchmark aim is to
facilitate reproducible research, expediting progress and increasing real-world
impacts.",http://arxiv.org/pdf/2410.03399v1
GraphCroc: Cross-Correlation Autoencoder for Graph Structural Reconstruction,"Shijin Duan, Ruyi Ding, Jiaxing He, Aidong Adam Ding, Yunsi Fei, Xiaolin Xu",2024-10-04,"Graph-structured data is integral to many applications, prompting the
development of various graph representation methods. Graph autoencoders (GAEs),
in particular, reconstruct graph structures from node embeddings. Current GAE
models primarily utilize self-correlation to represent graph structures and
focus on node-level tasks, often overlooking multi-graph scenarios. Our
theoretical analysis indicates that self-correlation generally falls short in
accurately representing specific graph features such as islands, symmetrical
structures, and directional edges, particularly in smaller or multiple graph
contexts. To address these limitations, we introduce a cross-correlation
mechanism that significantly enhances the GAE representational capabilities.
Additionally, we propose GraphCroc, a new GAE that supports flexible encoder
architectures tailored for various downstream tasks and ensures robust
structural reconstruction, through a mirrored encoding-decoding process. This
model also tackles the challenge of representation bias during optimization by
implementing a loss-balancing strategy. Both theoretical analysis and numerical
evaluations demonstrate that our methodology significantly outperforms existing
self-correlation-based GAEs in graph structure reconstruction.",http://arxiv.org/pdf/2410.03396v1
Predicting perturbation targets with causal differential networks,"Menghua Wu, Umesh Padia, Sean H. Murphy, Regina Barzilay, Tommi Jaakkola",2024-10-04,"Rationally identifying variables responsible for changes to a biological
system can enable myriad applications in disease understanding and cell
engineering. From a causality perspective, we are given two datasets generated
by the same causal model, one observational (control) and one interventional
(perturbed). The goal is to isolate the subset of measured variables (e.g.
genes) that were the targets of the intervention, i.e. those whose conditional
independencies have changed. Knowing the causal graph would limit the search
space, allowing us to efficiently pinpoint these variables. However, current
algorithms that infer causal graphs in the presence of unknown intervention
targets scale poorly to the hundreds or thousands of variables in biological
data, as they must jointly search the combinatorial spaces of graphs and
consistent intervention targets. In this work, we propose a causality-inspired
approach for predicting perturbation targets that decouples the two search
steps. First, we use an amortized causal discovery model to separately infer
causal graphs from the observational and interventional datasets. Then, we
learn to map these paired graphs to the sets of variables that were intervened
upon, in a supervised learning framework. This approach consistently
outperforms baselines for perturbation modeling on seven single-cell
transcriptomics datasets, each with thousands of measured variables. We also
demonstrate significant improvements over six causal discovery algorithms in
predicting intervention targets across a variety of tractable, synthetic
datasets.",http://arxiv.org/pdf/2410.03380v1
Mitigating Adversarial Perturbations for Deep Reinforcement Learning via Vector Quantization,"Tung M. Luu, Thanh Nguyen, Tee Joshua Tian Jin, Sungwoon Kim, Chang D. Yoo",2024-10-04,"Recent studies reveal that well-performing reinforcement learning (RL) agents
in training often lack resilience against adversarial perturbations during
deployment. This highlights the importance of building a robust agent before
deploying it in the real world. Most prior works focus on developing robust
training-based procedures to tackle this problem, including enhancing the
robustness of the deep neural network component itself or adversarially
training the agent on strong attacks. In this work, we instead study an input
transformation-based defense for RL. Specifically, we propose using a variant
of vector quantization (VQ) as a transformation for input observations, which
is then used to reduce the space of adversarial attacks during testing,
resulting in the transformed observations being less affected by attacks. Our
method is computationally efficient and seamlessly integrates with adversarial
training, further enhancing the robustness of RL agents against adversarial
attacks. Through extensive experiments in multiple environments, we demonstrate
that using VQ as the input transformation effectively defends against
adversarial attacks on the agent's observations.",http://arxiv.org/pdf/2410.03376v1
SoundSignature: What Type of Music Do You Like?,"Brandon James Carone, Pablo Ripollés",2024-10-04,"SoundSignature is a music application that integrates a custom OpenAI
Assistant to analyze users' favorite songs. The system incorporates
state-of-the-art Music Information Retrieval (MIR) Python packages to combine
extracted acoustic/musical features with the assistant's extensive knowledge of
the artists and bands. Capitalizing on this combined knowledge, SoundSignature
leverages semantic audio and principles from the emerging Internet of Sounds
(IoS) ecosystem, integrating MIR with AI to provide users with personalized
insights into the acoustic properties of their music, akin to a musical
preference personality report. Users can then interact with the chatbot to
explore deeper inquiries about the acoustic analyses performed and how they
relate to their musical taste. This interactivity transforms the application,
acting not only as an informative resource about familiar and/or favorite
songs, but also as an educational platform that enables users to deepen their
understanding of musical features, music theory, acoustic properties commonly
used in signal processing, and the artists behind the music. Beyond general
usability, the application also incorporates several well-established
open-source musician-specific tools, such as a chord recognition algorithm
(CREMA), a source separation algorithm (DEMUCS), and an audio-to-MIDI converter
(basic-pitch). These features allow users without coding skills to access
advanced, open-source music processing algorithms simply by interacting with
the chatbot (e.g., can you give me the stems of this song?). In this paper, we
highlight the application's innovative features and educational potential, and
present findings from a pilot user study that evaluates its efficacy and
usability.",http://arxiv.org/pdf/2410.03375v1
Make Interval Bound Propagation great again,"Patryk Krukowski, Daniel Wilczak, Jacek Tabor, Anna Bielawska, Przemysław Spurek",2024-10-04,"In various scenarios motivated by real life, such as medical data analysis,
autonomous driving, and adversarial training, we are interested in robust deep
networks. A network is robust when a relatively small perturbation of the input
cannot lead to drastic changes in output (like change of class, etc.). This
falls under the broader scope field of Neural Network Certification (NNC). Two
crucial problems in NNC are of profound interest to the scientific community:
how to calculate the robustness of a given pre-trained network and how to
construct robust networks. The common approach to constructing robust networks
is Interval Bound Propagation (IBP). This paper demonstrates that IBP is
sub-optimal in the first case due to its susceptibility to the wrapping effect.
Even for linear activation, IBP gives strongly sub-optimal bounds.
Consequently, one should use strategies immune to the wrapping effect to obtain
bounds close to optimal ones. We adapt two classical approaches dedicated to
strict computations -- Dubleton Arithmetic and Affine Arithmetic -- to mitigate
the wrapping effect in neural networks. These techniques yield precise results
for networks with linear activation functions, thus resisting the wrapping
effect. As a result, we achieve bounds significantly closer to the optimal
level than IBPs.",http://arxiv.org/pdf/2410.03373v1
Influence-oriented Personalized Federated Learning,"Yue Tan, Guodong Long, Jing Jiang, Chengqi Zhang",2024-10-04,"Traditional federated learning (FL) methods often rely on fixed weighting for
parameter aggregation, neglecting the mutual influence by others. Hence, their
effectiveness in heterogeneous data contexts is limited. To address this
problem, we propose an influence-oriented federated learning framework, namely
FedC^2I, which quantitatively measures Client-level and Class-level Influence
to realize adaptive parameter aggregation for each client. Our core idea is to
explicitly model the inter-client influence within an FL system via the
well-crafted influence vector and influence matrix. The influence vector
quantifies client-level influence, enables clients to selectively acquire
knowledge from others, and guides the aggregation of feature representation
layers. Meanwhile, the influence matrix captures class-level influence in a
more fine-grained manner to achieve personalized classifier aggregation. We
evaluate the performance of FedC^2I against existing federated learning methods
under non-IID settings and the results demonstrate the superiority of our
method.",http://arxiv.org/pdf/2410.03315v1
Five Years of COVID-19 Discourse on Instagram: A Labeled Instagram Dataset of Over Half a Million Posts for Multilingual Sentiment Analysis,Nirmalya Thakur,2024-10-04,"The work presented in this paper makes three scientific contributions with a
specific focus on mining and analysis of COVID-19-related posts on Instagram.
First, it presents a multilingual dataset of 500,153 Instagram posts about
COVID-19 published between January 2020 and September 2024. This dataset,
available at https://dx.doi.org/10.21227/d46p-v480, contains Instagram posts in
161 different languages as well as 535,021 distinct hashtags. After the
development of this dataset, multilingual sentiment analysis was performed,
which involved classifying each post as positive, negative, or neutral. The
results of sentiment analysis are presented as a separate attribute in this
dataset. Second, it presents the results of performing sentiment analysis per
year from 2020 to 2024. The findings revealed the trends in sentiment related
to COVID-19 on Instagram since the beginning of the pandemic. For instance,
between 2020 and 2024, the sentiment trends show a notable shift, with positive
sentiment decreasing from 38.35% to 28.69%, while neutral sentiment rising from
44.19% to 58.34%. Finally, the paper also presents findings of
language-specific sentiment analysis. This analysis highlighted similar and
contrasting trends of sentiment across posts published in different languages
on Instagram. For instance, out of all English posts, 49.68% were positive,
14.84% were negative, and 35.48% were neutral. In contrast, among Hindi posts,
4.40% were positive, 57.04% were negative, and 38.56% were neutral, reflecting
distinct differences in the sentiment distribution between these two languages.",http://arxiv.org/pdf/2410.03293v1
Enhanced Transformer architecture for in-context learning of dynamical systems,"Matteo Rufolo, Dario Piga, Gabriele Maroni, Marco Forgione",2024-10-04,"Recently introduced by some of the authors, the in-context identification
paradigm aims at estimating, offline and based on synthetic data, a meta-model
that describes the behavior of a whole class of systems. Once trained, this
meta-model is fed with an observed input/output sequence (context) generated by
a real system to predict its behavior in a zero-shot learning fashion. In this
paper, we enhance the original meta-modeling framework through three key
innovations: by formulating the learning task within a probabilistic framework;
by managing non-contiguous context and query windows; and by adopting recurrent
patching to effectively handle long context sequences. The efficacy of these
modifications is demonstrated through a numerical example focusing on the
Wiener-Hammerstein system class, highlighting the model's enhanced performance
and scalability.",http://arxiv.org/pdf/2410.03291v1
Manikin-Recorded Cardiopulmonary Sounds Dataset Using Digital Stethoscope,"Yasaman Torabi, Shahram Shirani, James P. Reilly",2024-10-04,"Heart and lung sounds are crucial for healthcare monitoring. Recent
improvements in stethoscope technology have made it possible to capture patient
sounds with enhanced precision. In this dataset, we used a digital stethoscope
to capture both heart and lung sounds, including individual and mixed
recordings. To our knowledge, this is the first dataset to offer both separate
and mixed cardiorespiratory sounds. The recordings were collected from a
clinical manikin, a patient simulator designed to replicate human physiological
conditions, generating clean heart and lung sounds at different body locations.
This dataset includes both normal sounds and various abnormalities (i.e.,
murmur, atrial fibrillation, tachycardia, atrioventricular block, third and
fourth heart sound, wheezing, crackles, rhonchi, pleural rub, and gurgling
sounds). The dataset includes audio recordings of chest examinations performed
at different anatomical locations, as determined by specialist nurses. Each
recording has been enhanced using frequency filters to highlight specific sound
types. This dataset is useful for applications in artificial intelligence, such
as automated cardiopulmonary disease detection, sound classification,
unsupervised separation techniques, and deep learning algorithms related to
audio signal processing.",http://arxiv.org/pdf/2410.03280v1
Test-time Adaptation for Regression by Subspace Alignment,"Kazuki Adachi, Shin'ya Yamaguchi, Atsutoshi Kumagai, Tomoki Hamagami",2024-10-04,"This paper investigates test-time adaptation (TTA) for regression, where a
regression model pre-trained in a source domain is adapted to an unknown target
distribution with unlabeled target data. Although regression is one of the
fundamental tasks in machine learning, most of the existing TTA methods have
classification-specific designs, which assume that models output
class-categorical predictions, whereas regression models typically output only
single scalar values. To enable TTA for regression, we adopt a feature
alignment approach, which aligns the feature distributions between the source
and target domains to mitigate the domain gap. However, we found that naive
feature alignment employed in existing TTA methods for classification is
ineffective or even worse for regression because the features are distributed
in a small subspace and many of the raw feature dimensions have little
significance to the output. For an effective feature alignment in TTA for
regression, we propose Significant-subspace Alignment (SSA). SSA consists of
two components: subspace detection and dimension weighting. Subspace detection
finds the feature subspace that is representative and significant to the
output. Then, the feature alignment is performed in the subspace during TTA.
Meanwhile, dimension weighting raises the importance of the dimensions of the
feature subspace that have greater significance to the output. We
experimentally show that SSA outperforms various baselines on real-world
datasets.",http://arxiv.org/pdf/2410.03263v1
How much can we forget about Data Contamination?,"Sebastian Bordt, Suraj Srinivas, Valentyn Boreiko, Ulrike von Luxburg",2024-10-04,"The leakage of benchmark data into the training data has emerged as a
significant challenge for evaluating the capabilities of large language models
(LLMs). In this work, we use experimental evidence and theoretical estimates to
challenge the common assumption that small-scale contamination renders
benchmark evaluations invalid. First, we experimentally quantify the magnitude
of benchmark overfitting based on scaling along three dimensions: The number of
model parameters (up to 1.6B), the number of times an example is seen (up to
144), and the number of training tokens (up to 40B). We find that if model and
data follow the Chinchilla scaling laws, minor contamination indeed leads to
overfitting. At the same time, even 144 times of contamination can be forgotten
if the training data is scaled beyond five times Chinchilla, a regime
characteristic of many modern LLMs. We then derive a simple theory of example
forgetting via cumulative weight decay. It allows us to bound the number of
gradient steps required to forget past data for any training run where we know
the hyperparameters of AdamW. This indicates that many LLMs, including Llama 3,
have forgotten the data seen at the beginning of training. Experimentally, we
demonstrate that forgetting occurs faster than what is predicted by our bounds.
Taken together, our results suggest that moderate amounts of contamination can
be forgotten at the end of realistically scaled training runs.",http://arxiv.org/pdf/2410.03249v1
AutoPenBench: Benchmarking Generative Agents for Penetration Testing,"Luca Gioacchini, Marco Mellia, Idilio Drago, Alexander Delsanto, Giuseppe Siracusano, Roberto Bifulco",2024-10-04,"Generative AI agents, software systems powered by Large Language Models
(LLMs), are emerging as a promising approach to automate cybersecurity tasks.
Among the others, penetration testing is a challenging field due to the task
complexity and the diverse strategies to simulate cyber-attacks. Despite
growing interest and initial studies in automating penetration testing with
generative agents, there remains a significant gap in the form of a
comprehensive and standard framework for their evaluation and development. This
paper introduces AutoPenBench, an open benchmark for evaluating generative
agents in automated penetration testing. We present a comprehensive framework
that includes 33 tasks, each representing a vulnerable system that the agent
has to attack. Tasks are of increasing difficulty levels, including in-vitro
and real-world scenarios. We assess the agent performance with generic and
specific milestones that allow us to compare results in a standardised manner
and understand the limits of the agent under test. We show the benefits of
AutoPenBench by testing two agent architectures: a fully autonomous and a
semi-autonomous supporting human interaction. We compare their performance and
limitations. For example, the fully autonomous agent performs unsatisfactorily
achieving a 21% Success Rate (SR) across the benchmark, solving 27% of the
simple tasks and only one real-world task. In contrast, the assisted agent
demonstrates substantial improvements, with 64% of SR. AutoPenBench allows us
also to observe how different LLMs like GPT-4o or OpenAI o1 impact the ability
of the agents to complete the tasks. We believe that our benchmark fills the
gap with a standard and flexible framework to compare penetration testing
agents on a common ground. We hope to extend AutoPenBench along with the
research community by making it available under
https://github.com/lucagioacchini/auto-pen-bench.",http://arxiv.org/pdf/2410.03225v1
Looking into Concept Explanation Methods for Diabetic Retinopathy Classification,"Andrea M. Storås, Josefine V. Sundgaard",2024-10-04,"Diabetic retinopathy is a common complication of diabetes, and monitoring the
progression of retinal abnormalities using fundus imaging is crucial. Because
the images must be interpreted by a medical expert, it is infeasible to screen
all individuals with diabetes for diabetic retinopathy. Deep learning has shown
impressive results for automatic analysis and grading of fundus images. One
drawback is, however, the lack of interpretability, which hampers the
implementation of such systems in the clinic. Explainable artificial
intelligence methods can be applied to explain the deep neural networks.
Explanations based on concepts have shown to be intuitive for humans to
understand, but have not yet been explored in detail for diabetic retinopathy
grading. This work investigates and compares two concept-based explanation
techniques for explaining deep neural networks developed for automatic
diagnosis of diabetic retinopathy: Quantitative Testing with Concept Activation
Vectors and Concept Bottleneck Models. We found that both methods have
strengths and weaknesses, and choice of method should take the available data
and the end user's preferences into account.",http://arxiv.org/pdf/2410.03188v1
EXAQ: Exponent Aware Quantization For LLMs Acceleration,"Moran Shkolnik, Maxim Fishman, Brian Chmiel, Hilla Ben-Yaacov, Ron Banner, Kfir Yehuda Levy",2024-10-04,"Quantization has established itself as the primary approach for decreasing
the computational and storage expenses associated with Large Language Models
(LLMs) inference. The majority of current research emphasizes quantizing
weights and activations to enable low-bit general-matrix-multiply (GEMM)
operations, with the remaining non-linear operations executed at higher
precision. In our study, we discovered that following the application of these
techniques, the primary bottleneck in LLMs inference lies in the softmax layer.
The softmax operation comprises three phases: exponent calculation,
accumulation, and normalization, Our work focuses on optimizing the first two
phases. We propose an analytical approach to determine the optimal clipping
value for the input to the softmax function, enabling sub-4-bit quantization
for LLMs inference. This method accelerates the calculations of both $e^x$ and
$\sum(e^x)$ with minimal to no accuracy degradation. For example, in
LLaMA1-30B, we achieve baseline performance with 2-bit quantization on the
well-known ""Physical Interaction: Question Answering"" (PIQA) dataset
evaluation. This ultra-low bit quantization allows, for the first time, an
acceleration of approximately 4x in the accumulation phase. The combination of
accelerating both $e^x$ and $\sum(e^x)$ results in a 36.9% acceleration in the
softmax operation.",http://arxiv.org/pdf/2410.03185v1
Autoregressive Moving-average Attention Mechanism for Time Series Forecasting,"Jiecheng Lu, Xu Han, Yan Sun, Shihao Yang",2024-10-04,"We propose an Autoregressive (AR) Moving-average (MA) attention structure
that can adapt to various linear attention mechanisms, enhancing their ability
to capture long-range and local temporal patterns in time series. In this
paper, we first demonstrate that, for the time series forecasting (TSF) task,
the previously overlooked decoder-only autoregressive Transformer model can
achieve results comparable to the best baselines when appropriate tokenization
and training methods are applied. Moreover, inspired by the ARMA model from
statistics and recent advances in linear attention, we introduce the full ARMA
structure into existing autoregressive attention mechanisms. By using an
indirect MA weight generation method, we incorporate the MA term while
maintaining the time complexity and parameter size of the underlying efficient
attention models. We further explore how indirect parameter generation can
produce implicit MA weights that align with the modeling requirements for local
temporal impacts. Experimental results show that incorporating the ARMA
structure consistently improves the performance of various AR attentions on TSF
tasks, achieving state-of-the-art results.",http://arxiv.org/pdf/2410.03159v1
Mathematical Formalism for Memory Compression in Selective State Space Models,Siddhanth Bhat,2024-10-04,"State space models (SSMs) have emerged as a powerful framework for modelling
long-range dependencies in sequence data. Unlike traditional recurrent neural
networks (RNNs) and convolutional neural networks (CNNs), SSMs offer a
structured and stable approach to sequence modelling, leveraging principles
from control theory and dynamical systems. However, a key challenge in sequence
modelling is compressing long-term dependencies into a compact hidden state
representation without losing critical information.
  In this paper, we develop a rigorous mathematical framework for understanding
memory compression in selective state space models. We introduce a selective
gating mechanism that dynamically filters and updates the hidden state based on
input relevance, allowing for efficient memory compression. We formalize the
trade-off between memory efficiency and information retention using
information-theoretic tools, such as mutual information and rate-distortion
theory. Our analysis provides theoretical bounds on the amount of information
that can be compressed without sacrificing model performance.
  We also derive theorems that prove the stability and convergence of the
hidden state in selective SSMs, ensuring reliable long-term memory retention.
Computational complexity analysis reveals that selective SSMs offer significant
improvements in memory efficiency and processing speed compared to traditional
RNN-based models. Through empirical validation on sequence modelling tasks such
as time-series forecasting and natural language processing, we demonstrate that
selective SSMs achieve state-of-the-art performance while using less memory and
computational resources.",http://arxiv.org/pdf/2410.03158v1
MELODI: Exploring Memory Compression for Long Contexts,"Yinpeng Chen, DeLesley Hutchins, Aren Jansen, Andrey Zhmoginov, David Racz, Jesper Andersen",2024-10-04,"We present MELODI, a novel memory architecture designed to efficiently
process long documents using short context windows. The key principle behind
MELODI is to represent short-term and long-term memory as a hierarchical
compression scheme across both network layers and context windows.
Specifically, the short-term memory is achieved through recurrent compression
of context windows across multiple layers, ensuring smooth transitions between
windows. In contrast, the long-term memory performs further compression within
a single middle layer and aggregates information across context windows,
effectively consolidating crucial information from the entire history. Compared
to a strong baseline - the Memorizing Transformer employing dense attention
over a large long-term memory (64K key-value pairs) - our method demonstrates
superior performance on various long-context datasets while remarkably reducing
the memory footprint by a factor of 8.",http://arxiv.org/pdf/2410.03156v1
Remaining Useful Life Prediction: A Study on Multidimensional Industrial Signal Processing and Efficient Transfer Learning Based on Large Language Models,"Yan Chen, Cheng Liu",2024-10-04,"Remaining useful life (RUL) prediction is crucial for maintaining modern
industrial systems, where equipment reliability and operational safety are
paramount. Traditional methods, based on small-scale deep learning or
physical/statistical models, often struggle with complex, multidimensional
sensor data and varying operating conditions, limiting their generalization
capabilities. To address these challenges, this paper introduces an innovative
regression framework utilizing large language models (LLMs) for RUL prediction.
By leveraging the modeling power of LLMs pre-trained on corpus data, the
proposed model can effectively capture complex temporal dependencies and
improve prediction accuracy. Extensive experiments on the Turbofan engine's RUL
prediction task show that the proposed model surpasses state-of-the-art (SOTA)
methods on the challenging FD002 and FD004 subsets and achieves near-SOTA
results on the other subsets. Notably, different from previous research, our
framework uses the same sliding window length and all sensor signals for all
subsets, demonstrating strong consistency and generalization. Moreover,
transfer learning experiments reveal that with minimal target domain data for
fine-tuning, the model outperforms SOTA methods trained on full target domain
data. This research highlights the significant potential of LLMs in industrial
signal processing and RUL prediction, offering a forward-looking solution for
health management in future intelligent industrial systems.",http://arxiv.org/pdf/2410.03134v1
Autoregressive Action Sequence Learning for Robotic Manipulation,"Xinyu Zhang, Yuhan Liu, Haonan Chang, Liam Schramm, Abdeslam Boularias",2024-10-04,"Autoregressive models have demonstrated remarkable success in natural
language processing. In this work, we design a simple yet effective
autoregressive architecture for robotic manipulation tasks. We propose the
Chunking Causal Transformer (CCT), which extends the next-single-token
prediction of causal transformers to support multi-token prediction in a single
pass. Further, we design a novel attention interleaving strategy that allows
CCT to be trained efficiently with teacher-forcing. Based on CCT, we propose
the Autoregressive Policy (ARP) model, which learns to generate action
sequences autoregressively. We find that action sequence learning enables
better leverage of the underlying causal relationships in robotic tasks. We
evaluate ARP across diverse robotic manipulation environments, including
Push-T, ALOHA, and RLBench, and show that it outperforms the state-of-the-art
methods in all tested environments, while being more efficient in computation
and parameter sizes. Video demonstrations, our source code, and the models of
ARP can be found at http://github.com/mlzxy/arp.",http://arxiv.org/pdf/2410.03132v1
AIME: AI System Optimization via Multiple LLM Evaluators,"Bhrij Patel, Souradip Chakraborty, Wesley A. Suttle, Mengdi Wang, Amrit Singh Bedi, Dinesh Manocha",2024-10-04,"Text-based AI system optimization typically involves a feedback loop scheme
where a single LLM generates an evaluation in natural language of the current
output to improve the next iteration's output. However, in this work, we
empirically demonstrate that for a practical and complex task (code generation)
with multiple criteria to evaluate, utilizing only one LLM evaluator tends to
let errors in generated code go undetected, thus leading to incorrect
evaluations and ultimately suboptimal test case performance. Motivated by this
failure case, we assume there exists an optimal evaluation policy that samples
an evaluation between response and ground truth. We then theoretically prove
that a linear combination of multiple evaluators can approximate this optimal
policy. From this insight, we propose AI system optimization via Multiple LLM
Evaluators (AIME). AIME is an evaluation protocol that utilizes multiple LLMs
that each independently generate an evaluation on separate criteria and then
combine them via concatenation. We provide an extensive empirical study showing
AIME outperforming baseline methods in code generation tasks, with up to $62\%$
higher error detection rate and up to $16\%$ higher success rate than a single
LLM evaluation protocol on LeetCodeHard and HumanEval datasets. We also show
that the selection of the number of evaluators and which criteria to utilize is
non-trivial as it can impact pact success rate by up to $12\%$.",http://arxiv.org/pdf/2410.03131v1
ARB-LLM: Alternating Refined Binarizations for Large Language Models,"Zhiteng Li, Xianglong Yan, Tianao Zhang, Haotong Qin, Dong Xie, Jiang Tian, zhongchao shi, Linghe Kong, Yulun Zhang, Xiaokang Yang",2024-10-04,"Large Language Models (LLMs) have greatly pushed forward advancements in
natural language processing, yet their high memory and computational demands
hinder practical deployment. Binarization, as an effective compression
technique, can shrink model weights to just 1 bit, significantly reducing the
high demands on computation and memory. However, current binarization methods
struggle to narrow the distribution gap between binarized and full-precision
weights, while also overlooking the column deviation in LLM weight
distribution. To tackle these issues, we propose ARB-LLM, a novel 1-bit
post-training quantization (PTQ) technique tailored for LLMs. To narrow the
distribution shift between binarized and full-precision weights, we first
design an alternating refined binarization (ARB) algorithm to progressively
update the binarization parameters, which significantly reduces the
quantization error. Moreover, considering the pivot role of calibration data
and the column deviation in LLM weights, we further extend ARB to ARB-X and
ARB-RC. In addition, we refine the weight partition strategy with column-group
bitmap (CGB), which further enhance performance. Equipping ARB-X and ARB-RC
with CGB, we obtain ARB-LLM$_\text{X}$ and ARB-LLM$_\text{RC}$ respectively,
which significantly outperform state-of-the-art (SOTA) binarization methods for
LLMs. As a binary PTQ method, our ARB-LLM$_\text{RC}$ is the first to surpass
FP16 models of the same size. The code and models will be available at
https://github.com/ZHITENGLI/ARB-LLM.",http://arxiv.org/pdf/2410.03129v1
Understanding Decision Subjects' Engagement with and Perceived Fairness of AI Models When Opportunities of Qualification Improvement Exist,"Meric Altug Gemalmaz, Ming Yin",2024-10-04,"We explore how an AI model's decision fairness affects people's engagement
with and perceived fairness of the model if they are subject to its decisions,
but could repeatedly and strategically respond to these decisions. Two types of
strategic responses are considered -- people could determine whether to
continue interacting with the model, and whether to invest in themselves to
improve their chance of future favorable decisions from the model. Via three
human-subject experiments, we found that in decision subjects' strategic,
repeated interactions with an AI model, the model's decision fairness does not
change their willingness to interact with the model or to improve themselves,
even when the model exhibits unfairness on salient protected attributes.
However, decision subjects still perceive the AI model to be less fair when it
systematically biases against their group, especially if the difficulty of
improving one's qualification for the favorable decision is larger for the
lowly-qualified people.",http://arxiv.org/pdf/2410.03126v1
RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning,"Zihao Zhao, Yuchen Yang, Yijiang Li, Yinzhi Cao",2024-10-04,"The ripple effect poses a significant challenge in knowledge editing for
large language models. Namely, when a single fact is edited, the model
struggles to accurately update the related facts in a sequence, which is
evaluated by multi-hop questions linked to a chain of related facts. Recent
strategies have moved away from traditional parameter updates to more flexible,
less computation-intensive methods, proven to be more effective in addressing
the ripple effect. In-context learning (ICL) editing uses a simple
demonstration `Imagine that + new fact` to guide LLMs, but struggles with
complex multi-hop questions as the new fact alone fails to specify the chain of
facts involved in such scenarios. Besides, memory-based editing maintains
additional storage for all edits and related facts, requiring continuous
updates to stay effective. As a result of these design limitations, the
challenge remains, with the highest accuracy being only 33.8% on the MQuAKE-cf
benchmarks for Vicuna-7B. To address this, we propose RippleCOT, a novel ICL
editing approach integrating Chain-of-Thought (COT) reasoning. RippleCOT
structures demonstrations as `newfact, question, thought, answer`,
incorporating a thought component to identify and decompose the multi-hop logic
within questions. This approach effectively guides the model through complex
multi-hop questions with chains of related facts. Comprehensive experiments
demonstrate that RippleCOT significantly outperforms the state-of-the-art on
the ripple effect, achieving accuracy gains ranging from 7.8% to 87.1%.",http://arxiv.org/pdf/2410.03122v1
ProcBench: Benchmark for Multi-Step Reasoning and Following Procedure,"Ippei Fujisawa, Sensho Nobe, Hiroki Seto, Rina Onda, Yoshiaki Uchida, Hiroki Ikoma, Pei-Chun Chien, Ryota Kanai",2024-10-04,"Reasoning is central to a wide range of intellectual activities, and while
the capabilities of large language models (LLMs) continue to advance, their
performance in reasoning tasks remains limited. The processes and mechanisms
underlying reasoning are not yet fully understood, but key elements include
path exploration, selection of relevant knowledge, and multi-step inference.
Problems are solved through the synthesis of these components. In this paper,
we propose a benchmark that focuses on a specific aspect of reasoning ability:
the direct evaluation of multi-step inference. To this end, we design a special
reasoning task where multi-step inference is specifically focused by largely
eliminating path exploration and implicit knowledge utilization. Our dataset
comprises pairs of explicit instructions and corresponding questions, where the
procedures necessary for solving the questions are entirely detailed within the
instructions. This setup allows models to solve problems solely by following
the provided directives. By constructing problems that require varying numbers
of steps to solve and evaluating responses at each step, we enable a thorough
assessment of state-of-the-art LLMs' ability to follow instructions. To ensure
the robustness of our evaluation, we include multiple distinct tasks.
Furthermore, by comparing accuracy across tasks, utilizing step-aware metrics,
and applying separately defined measures of complexity, we conduct experiments
that offer insights into the capabilities and limitations of LLMs in reasoning
tasks. Our findings have significant implications for the development of LLMs
and highlight areas for future research in advancing their reasoning abilities.
Our dataset is available at
\url{https://huggingface.co/datasets/ifujisawa/procbench} and code at
\url{https://github.com/ifujisawa/proc-bench}.",http://arxiv.org/pdf/2410.03117v1
LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive Compression Strategy,"Rongzhi Zhang, Kuang Wang, Liyuan Liu, Shuohang Wang, Hao Cheng, Chao Zhang, Yelong Shen",2024-10-04,"The Key-Value (KV) cache is a crucial component in serving transformer-based
autoregressive large language models (LLMs), enabling faster inference by
storing previously computed KV vectors. However, its memory consumption scales
linearly with sequence length and batch size, posing a significant bottleneck
in LLM deployment. Existing approaches to mitigate this issue include: (1)
efficient attention variants integrated in upcycling stages, which requires
extensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache
compression at test time, primarily through token eviction policies, which
often overlook inter-layer dependencies and can be task-specific.
  This paper introduces an orthogonal approach to KV cache compression. We
propose a low-rank approximation of KV weight matrices, allowing for plug-in
integration with existing transformer-based LLMs without model retraining. To
effectively compress KV cache at the weight level, we adjust for layerwise
sensitivity and introduce a progressive compression strategy, which is
supported by our theoretical analysis on how compression errors accumulate in
deep networks. Our method is designed to function without model tuning in
upcycling stages or task-specific profiling in test stages. Extensive
experiments with LLaMA models ranging from 8B to 70B parameters across various
tasks show that our approach significantly reduces the GPU memory footprint
while maintaining performance.",http://arxiv.org/pdf/2410.03111v1
Mamba in Vision: A Comprehensive Survey of Techniques and Applications,"Md Maklachur Rahman, Abdullah Aman Tutul, Ankur Nath, Lamyanba Laishram, Soon Ki Jung, Tracy Hammond",2024-10-04,"Mamba is emerging as a novel approach to overcome the challenges faced by
Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) in computer
vision. While CNNs excel at extracting local features, they often struggle to
capture long-range dependencies without complex architectural modifications. In
contrast, ViTs effectively model global relationships but suffer from high
computational costs due to the quadratic complexity of their self-attention
mechanisms. Mamba addresses these limitations by leveraging Selective
Structured State Space Models to effectively capture long-range dependencies
with linear computational complexity. This survey analyzes the unique
contributions, computational benefits, and applications of Mamba models while
also identifying challenges and potential future research directions. We
provide a foundational resource for advancing the understanding and growth of
Mamba models in computer vision. An overview of this work is available at
https://github.com/maklachur/Mamba-in-Computer-Vision.",http://arxiv.org/pdf/2410.03105v1
Strategic Insights from Simulation Gaming of AI Race Dynamics,"Ross Gruetzemacher, Shahar Avin, James Fox, Alexander K Saeri",2024-10-04,"We present insights from ""Intelligence Rising"", a scenario exploration
exercise about possible AI futures. Drawing on the experiences of facilitators
who have overseen 43 games over a four-year period, we illuminate recurring
patterns, strategies, and decision-making processes observed during gameplay.
Our analysis reveals key strategic considerations about AI development
trajectories in this simulated environment, including: the destabilising
effects of AI races, the crucial role of international cooperation in
mitigating catastrophic risks, the challenges of aligning corporate and
national interests, and the potential for rapid, transformative change in AI
capabilities. We highlight places where we believe the game has been effective
in exposing participants to the complexities and uncertainties inherent in AI
governance. Key recurring gameplay themes include the emergence of
international agreements, challenges to the robustness of such agreements, the
critical role of cybersecurity in AI development, and the potential for
unexpected crises to dramatically alter AI trajectories. By documenting these
insights, we aim to provide valuable foresight for policymakers, industry
leaders, and researchers navigating the complex landscape of AI development and
governance.",http://arxiv.org/pdf/2410.03092v1
Integrating Natural Language Prompting Tasks in Introductory Programming Courses,"Chris Kerslake, Paul Denny, David H Smith IV, James Prather, Juho Leinonen, Andrew Luxton-Reilly, Stephen MacNeil",2024-10-04,"Introductory programming courses often emphasize mastering syntax and basic
constructs before progressing to more complex and interesting programs. This
bottom-up approach can be frustrating for novices, shifting the focus away from
problem solving and potentially making computing less appealing to a broad
range of students. The rise of generative AI for code production could
partially address these issues by fostering new skills via interaction with AI
models, including constructing high-level prompts and evaluating code that is
automatically generated. In this experience report, we explore the inclusion of
two prompt-focused activities in an introductory course, implemented across
four labs in a six-week module. The first requires students to solve
computational problems by writing natural language prompts, emphasizing
problem-solving over syntax. The second involves students crafting prompts to
generate code equivalent to provided fragments, to foster an understanding of
the relationship between prompts and code. Most of the students in the course
had reported finding programming difficult to learn, often citing frustrations
with syntax and debugging. We found that self-reported difficulty with learning
programming had a strong inverse relationship with performance on traditional
programming assessments such as tests and projects, as expected. However,
performance on the natural language tasks was less strongly related to
self-reported difficulty, suggesting they may target different skills. Learning
how to communicate with AI coding models is becoming an important skill, and
natural language prompting tasks may appeal to a broad range of students.",http://arxiv.org/pdf/2410.03063v1
Towards an Improved Metric for Evaluating Disentangled Representations,"Sahib Julka, Yashu Wang, Michael Granitzer",2024-10-04,"Disentangled representation learning plays a pivotal role in making
representations controllable, interpretable and transferable. Despite its
significance in the domain, the quest for reliable and consistent quantitative
disentanglement metric remains a major challenge. This stems from the
utilisation of diverse metrics measuring different properties and the potential
bias introduced by their design. Our work undertakes a comprehensive
examination of existing popular disentanglement evaluation metrics, comparing
them in terms of measuring aspects of disentanglement (viz. Modularity,
Compactness, and Explicitness), detecting the factor-code relationship, and
describing the degree of disentanglement. We propose a new framework for
quantifying disentanglement, introducing a metric entitled \emph{EDI}, that
leverages the intuitive concept of \emph{exclusivity} and improved factor-code
relationship to minimize ad-hoc decisions. An in-depth analysis reveals that
EDI measures essential properties while offering more stability than existing
metrics, advocating for its adoption as a standardised approach.",http://arxiv.org/pdf/2410.03056v1
Permissive Information-Flow Analysis for Large Language Models,"Shoaib Ahmed Siddiqui, Radhika Gaonkar, Boris Köpf, David Krueger, Andrew Paverd, Ahmed Salem, Shruti Tople, Lukas Wutschitz, Menglin Xia, Santiago Zanella-Béguelin",2024-10-04,"Large Language Models (LLMs) are rapidly becoming commodity components of
larger software systems. This poses natural security and privacy problems:
poisoned data retrieved from one component can change the model's behavior and
compromise the entire system, including coercing the model to spread
confidential data to untrusted components. One promising approach is to tackle
this problem at the system level via dynamic information flow (aka taint)
tracking. Unfortunately, the traditional approach of propagating the most
restrictive input label to the output is too conservative for applications
where LLMs operate on inputs retrieved from diverse sources. In this paper, we
propose a novel, more permissive approach to propagate information flow labels
through LLM queries. The key idea behind our approach is to propagate only the
labels of the samples that were influential in generating the model output and
to eliminate the labels of unnecessary input. We implement and investigate the
effectiveness of two variations of this approach, based on (i) prompt-based
retrieval augmentation, and (ii) a $k$-nearest-neighbors language model. We
compare these with the baseline of an introspection-based influence estimator
that directly asks the language model to predict the output label. The results
obtained highlight the superiority of our prompt-based label propagator, which
improves the label in more than 85% of the cases in an LLM agent setting. These
findings underscore the practicality of permissive label propagation for
retrieval augmentation.",http://arxiv.org/pdf/2410.03055v1
Scalable Frame-based Construction of Sociocultural NormBases for Socially-Aware Dialogues,"Shilin Qu, Weiqing Wang, Xin Zhou, Haolan Zhan, Zhuang Li, Lizhen Qu, Linhao Luo, Yuan-Fang Li, Gholamreza Haffari",2024-10-04,"Sociocultural norms serve as guiding principles for personal conduct in
social interactions, emphasizing respect, cooperation, and appropriate
behavior, which is able to benefit tasks including conversational information
retrieval, contextual information retrieval and retrieval-enhanced machine
learning. We propose a scalable approach for constructing a Sociocultural Norm
(SCN) Base using Large Language Models (LLMs) for socially aware dialogues. We
construct a comprehensive and publicly accessible Chinese Sociocultural
NormBase. Our approach utilizes socially aware dialogues, enriched with
contextual frames, as the primary data source to constrain the generating
process and reduce the hallucinations. This enables extracting of high-quality
and nuanced natural-language norm statements, leveraging the pragmatic
implications of utterances with respect to the situation. As real dialogue
annotated with gold frames are not readily available, we propose using
synthetic data. Our empirical results show: (i) the quality of the SCNs derived
from synthetic data is comparable to that from real dialogues annotated with
gold frames, and (ii) the quality of the SCNs extracted from real data,
annotated with either silver (predicted) or gold frames, surpasses that without
the frame annotations. We further show the effectiveness of the extracted SCNs
in a RAG-based (Retrieval-Augmented Generation) model to reason about multiple
downstream dialogue tasks.",http://arxiv.org/pdf/2410.03049v1
Revealing the Unseen: Guiding Personalized Diffusion Models to Expose Training Data,"Xiaoyu Wu, Jiaru Zhang, Steven Wu",2024-10-03,"Diffusion Models (DMs) have evolved into advanced image generation tools,
especially for few-shot fine-tuning where a pretrained DM is fine-tuned on a
small set of images to capture specific styles or objects. Many people upload
these personalized checkpoints online, fostering communities such as Civitai
and HuggingFace. However, model owners may overlook the potential risks of data
leakage by releasing their fine-tuned checkpoints. Moreover, concerns regarding
copyright violations arise when unauthorized data is used during fine-tuning.
In this paper, we ask: ""Can training data be extracted from these fine-tuned
DMs shared online?"" A successful extraction would present not only data leakage
threats but also offer tangible evidence of copyright infringement. To answer
this, we propose FineXtract, a framework for extracting fine-tuning data. Our
method approximates fine-tuning as a gradual shift in the model's learned
distribution -- from the original pretrained DM toward the fine-tuning data. By
extrapolating the models before and after fine-tuning, we guide the generation
toward high-probability regions within the fine-tuned data distribution. We
then apply a clustering algorithm to extract the most probable images from
those generated using this extrapolated guidance. Experiments on DMs fine-tuned
with datasets such as WikiArt, DreamBooth, and real-world checkpoints posted
online validate the effectiveness of our method, extracting approximately 20%
of fine-tuning data in most cases, significantly surpassing baseline
performance.",http://arxiv.org/pdf/2410.03039v1
CounterQuill: Investigating the Potential of Human-AI Collaboration in Online Counterspeech Writing,"Xiaohan Ding, Kaike Ping, Uma Sushmitha Gunturi, Buse Carik, Sophia Stil, Lance T Wilhelm, Taufiq Daryanto, James Hawdon, Sang Won Lee, Eugenia H Rho",2024-10-03,"Online hate speech has become increasingly prevalent on social media
platforms, causing harm to individuals and society. While efforts have been
made to combat this issue through content moderation, the potential of
user-driven counterspeech as an alternative solution remains underexplored.
Existing counterspeech methods often face challenges such as fear of
retaliation and skill-related barriers. To address these challenges, we
introduce CounterQuill, an AI-mediated system that assists users in composing
effective and empathetic counterspeech. CounterQuill provides a three-step
process: (1) a learning session to help users understand hate speech and
counterspeech; (2) a brainstorming session that guides users in identifying key
elements of hate speech and exploring counterspeech strategies; and (3) a
co-writing session that enables users to draft and refine their counterspeech
with CounterQuill. We conducted a within-subjects user study with 20
participants to evaluate CounterQuill in comparison to ChatGPT. Results show
that CounterQuill's guidance and collaborative writing process provided users a
stronger sense of ownership over their co-authored counterspeech. Users
perceived CounterQuill as a writing partner and thus were more willing to post
the co-written counterspeech online compared to the one written with ChatGPT.",http://arxiv.org/pdf/2410.03032v1
Flow Matching with Gaussian Process Priors for Probabilistic Time Series Forecasting,"Marcel Kollovieh, Marten Lienen, David Lüdke, Leo Schwinn, Stephan Günnemann",2024-10-03,"Recent advancements in generative modeling, particularly diffusion models,
have opened new directions for time series modeling, achieving state-of-the-art
performance in forecasting and synthesis. However, the reliance of
diffusion-based models on a simple, fixed prior complicates the generative
process since the data and prior distributions differ significantly. We
introduce TSFlow, a conditional flow matching (CFM) model for time series that
simplifies the generative problem by combining Gaussian processes, optimal
transport paths, and data-dependent prior distributions. By incorporating
(conditional) Gaussian processes, TSFlow aligns the prior distribution more
closely with the temporal structure of the data, enhancing both unconditional
and conditional generation. Furthermore, we propose conditional prior sampling
to enable probabilistic forecasting with an unconditionally trained model. In
our experimental evaluation on eight real-world datasets, we demonstrate the
generative capabilities of TSFlow, producing high-quality unconditional
samples. Finally, we show that both conditionally and unconditionally trained
models achieve competitive results in forecasting benchmarks, surpassing other
methods on 6 out of 8 datasets.",http://arxiv.org/pdf/2410.03024v1
Is Your Paper Being Reviewed by an LLM? Investigating AI Text Detectability in Peer Review,"Sungduk Yu, Man Luo, Avinash Madasu, Vasudev Lal, Phillip Howard",2024-10-03,"Peer review is a critical process for ensuring the integrity of published
scientific research. Confidence in this process is predicated on the assumption
that experts in the relevant domain give careful consideration to the merits of
manuscripts which are submitted for publication. With the recent rapid
advancements in the linguistic capabilities of large language models (LLMs), a
new potential risk to the peer review process is that negligent reviewers will
rely on LLMs to perform the often time consuming process of reviewing a paper.
In this study, we investigate the ability of existing AI text detection
algorithms to distinguish between peer reviews written by humans and different
state-of-the-art LLMs. Our analysis shows that existing approaches fail to
identify many GPT-4o written reviews without also producing a high number of
false positive classifications. To address this deficiency, we propose a new
detection approach which surpasses existing methods in the identification of
GPT-4o written peer reviews at low levels of false positive classifications.
Our work reveals the difficulty of accurately identifying AI-generated text at
the individual review level, highlighting the urgent need for new tools and
methods to detect this type of unethical application of generative AI.",http://arxiv.org/pdf/2410.03019v1
"Transforming Teachers' Roles and Agencies in the Era of Generative AI: Perceptions, Acceptance, Knowledge, and Practices",Xiaoming Zhai,2024-10-03,"This paper explores the transformative impact of Generative Artificial
Intelligence (GenAI) on teachers' roles and agencies in education, presenting a
comprehensive framework that addresses teachers' perceptions, knowledge,
acceptance, and practices of GenAI. As GenAI technologies, such as ChatGPT,
become increasingly integrated into educational settings, teachers are required
to adapt to evolving classroom dynamics, where AI plays a significant role in
content creation, personalized learning, and student engagement. However,
existing literature often treats these factors in isolation, overlooking how
they collectively influence teachers' ability to effectively integrate GenAI
into their pedagogical practices. This paper fills this gap by proposing a
framework that categorizes teachers into four roles -- Observer, Adopter,
Collaborator, and Innovator -- each representing different levels of GenAI
engagement, outlining teachers' agencies in GenAI classrooms. By highlighting
the need for continuous professional development and institutional support, we
demonstrate how teachers can evolve from basic GenAI users to co-creators of
knowledge alongside GenAI systems. The findings emphasize that for GenAI to
reach its full educational potential, teachers must not only accept and
understand its capabilities but also integrate it deeply into their teaching
strategies. This study contributes to the growing literature on GenAI in
education, offering practical implications for supporting teachers in
navigating the complexities of GenAI adoption.",http://arxiv.org/pdf/2410.03018v1
Differentiation and Specialization of Attention Heads via the Refined Local Learning Coefficient,"George Wang, Jesse Hoogland, Stan van Wingerden, Zach Furman, Daniel Murfet",2024-10-03,"We introduce refined variants of the Local Learning Coefficient (LLC), a
measure of model complexity grounded in singular learning theory, to study the
development of internal structure in transformer language models during
training. By applying these \textit{refined LLCs} (rLLCs) to individual
components of a two-layer attention-only transformer, we gain novel insights
into the progressive differentiation and specialization of attention heads. Our
methodology reveals how attention heads differentiate into distinct functional
roles over the course of training, analyzes the types of data these heads
specialize to process, and discovers a previously unidentified multigram
circuit. These findings demonstrate that rLLCs provide a principled,
quantitative toolkit for \textit{developmental interpretability}, which aims to
understand models through their evolution across the learning process. More
broadly, this work takes a step towards establishing the correspondence between
data distributional structure, geometric properties of the loss landscape,
learning dynamics, and emergent computational structures in neural networks.",http://arxiv.org/pdf/2410.02984v1
An explainable approach to detect case law on housing and eviction issues within the HUDOC database,"Mohammad Mohammadi, Martijn Wieling, Michel Vols",2024-10-03,"Case law is instrumental in shaping our understanding of human rights,
including the right to adequate housing. The HUDOC database provides access to
the textual content of case law from the European Court of Human Rights
(ECtHR), along with some metadata. While this metadata includes valuable
information, such as the application number and the articles addressed in a
case, it often lacks detailed substantive insights, such as the specific issues
a case covers. This underscores the need for detailed analysis to extract such
information. However, given the size of the database - containing over 40,000
cases - an automated solution is essential.
  In this study, we focus on the right to adequate housing and aim to build
models to detect cases related to housing and eviction issues. Our experiments
show that the resulting models not only provide performance comparable to more
sophisticated approaches but are also interpretable, offering explanations for
their decisions by highlighting the most influential words. The application of
these models led to the identification of new cases that were initially
overlooked during data collection. This suggests that NLP approaches can be
effectively applied to categorise case law based on the specific issues they
address.",http://arxiv.org/pdf/2410.02978v1
F-Fidelity: A Robust Framework for Faithfulness Evaluation of Explainable AI,"Xu Zheng, Farhad Shirani, Zhuomin Chen, Chaohao Lin, Wei Cheng, Wenbo Guo, Dongsheng Luo",2024-10-03,"Recent research has developed a number of eXplainable AI (XAI) techniques.
Although extracting meaningful insights from deep learning models, how to
properly evaluate these XAI methods remains an open problem. The most widely
used approach is to perturb or even remove what the XAI method considers to be
the most important features in an input and observe the changes in the output
prediction. This approach although efficient suffers the Out-of-Distribution
(OOD) problem as the perturbed samples may no longer follow the original data
distribution. A recent method RemOve And Retrain (ROAR) solves the OOD issue by
retraining the model with perturbed samples guided by explanations. However,
the training may not always converge given the distribution difference.
Furthermore, using the model retrained based on XAI methods to evaluate these
explainers may cause information leakage and thus lead to unfair comparisons.
We propose Fine-tuned Fidelity F-Fidelity, a robust evaluation framework for
XAI, which utilizes i) an explanation-agnostic fine-tuning strategy, thus
mitigating the information leakage issue and ii) a random masking operation
that ensures that the removal step does not generate an OOD input. We designed
controlled experiments with state-of-the-art (SOTA) explainers and their
degraded version to verify the correctness of our framework. We conducted
experiments on multiple data structures, such as images, time series, and
natural language. The results demonstrate that F-Fidelity significantly
improves upon prior evaluation metrics in recovering the ground-truth ranking
of the explainers. Furthermore, we show both theoretically and empirically
that, given a faithful explainer, F-Fidelity metric can be used to compute the
sparsity of influential input components, i.e., to extract the true explanation
size.",http://arxiv.org/pdf/2410.02970v1
Label-Free Subjective Player Experience Modelling via Let's Play Videos,"Dave Goel, Athar Mahmoudi-Nejad, Matthew Guzdial",2024-10-03,"Player Experience Modelling (PEM) is the study of AI techniques applied to
modelling a player's experience within a video game. PEM development can be
labour-intensive, requiring expert hand-authoring or specialized data
collection. In this work, we propose a novel PEM development approach,
approximating player experience from gameplay video. We evaluate this approach
predicting affect in the game Angry Birds via a human subject study. We
validate that our PEM can strongly correlate with self-reported and sensor
measures of affect, demonstrating the potential of this approach.",http://arxiv.org/pdf/2410.02967v1
AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML,"Patara Trirat, Wonyong Jeong, Sung Ju Hwang",2024-10-03,"Automated machine learning (AutoML) accelerates AI development by automating
tasks in the development pipeline, such as optimal model search and
hyperparameter tuning. Existing AutoML systems often require technical
expertise to set up complex tools, which is in general time-consuming and
requires a large amount of human effort. Therefore, recent works have started
exploiting large language models (LLM) to lessen such burden and increase the
usability of AutoML frameworks via a natural language interface, allowing
non-expert users to build their data-driven solutions. These methods, however,
are usually designed only for a particular process in the AI development
pipeline and do not efficiently use the inherent capacity of the LLMs. This
paper proposes AutoML-Agent, a novel multi-agent framework tailored for
full-pipeline AutoML, i.e., from data retrieval to model deployment.
AutoML-Agent takes user's task descriptions, facilitates collaboration between
specialized LLM agents, and delivers deployment-ready models. Unlike existing
work, instead of devising a single plan, we introduce a retrieval-augmented
planning strategy to enhance exploration to search for more optimal plans. We
also decompose each plan into sub-tasks (e.g., data preprocessing and neural
network design) each of which is solved by a specialized agent we build via
prompting executing in parallel, making the search process more efficient.
Moreover, we propose a multi-stage verification to verify executed results and
guide the code generation LLM in implementing successful solutions. Extensive
experiments on seven downstream tasks using fourteen datasets show that
AutoML-Agent achieves a higher success rate in automating the full AutoML
process, yielding systems with good performance throughout the diverse domains.",http://arxiv.org/pdf/2410.02958v1
"AiBAT: Artificial Intelligence/Instructions for Build, Assembly, and Test","Benjamin Nuernberger, Anny Liu, Heather Stefanini, Richard Otis, Amanda Towler, R. Peter Dillon",2024-10-03,"Instructions for Build, Assembly, and Test (IBAT) refers to the process used
whenever any operation is conducted on hardware, including tests, assembly, and
maintenance. Currently, the generation of IBAT documents is time-intensive, as
users must manually reference and transfer information from engineering
diagrams and parts lists into IBAT instructions. With advances in machine
learning and computer vision, however, it is possible to have an artificial
intelligence (AI) model perform the partial filling of the IBAT template,
freeing up engineer time for more highly skilled tasks. AiBAT is a novel system
for assisting users in authoring IBATs. It works by first analyzing assembly
drawing documents, extracting information and parsing it, and then filling in
IBAT templates with the extracted information. Such assisted authoring has
potential to save time and reduce cost. This paper presents an overview of the
AiBAT system, including promising preliminary results and discussion on future
work.",http://arxiv.org/pdf/2410.02955v1
LLMCO2: Advancing Accurate Carbon Footprint Prediction for LLM Inferences,"Zhenxiao Fu, Fan Chen, Shan Zhou, Haitong Li, Lei Jiang",2024-10-03,"Throughout its lifecycle, a large language model (LLM) generates a
substantially larger carbon footprint during inference than training. LLM
inference requests vary in batch size, prompt length, and token generation
number, while cloud providers employ different GPU types and quantities to meet
diverse service-level objectives for accuracy and latency. It is crucial for
both users and cloud providers to have a tool that quickly and accurately
estimates the carbon impact of LLM inferences based on a combination of
inference request and hardware configurations before execution. Estimating the
carbon footprint of LLM inferences is more complex than training due to lower
and highly variable model FLOPS utilization, rendering previous equation-based
models inaccurate. Additionally, existing machine learning (ML) prediction
methods either lack accuracy or demand extensive training data, as they
inadequately handle the distinct prefill and decode phases, overlook
hardware-specific features, and inefficiently sample uncommon inference
configurations. We introduce \coo, a graph neural network (GNN)-based model
that greatly improves the accuracy of LLM inference carbon footprint
predictions compared to previous methods.",http://arxiv.org/pdf/2410.02950v1
SymmetricDiffusers: Learning Discrete Diffusion on Finite Symmetric Groups,"Yongxing Zhang, Donglin Yang, Renjie Liao",2024-10-03,"Finite symmetric groups $S_n$ are essential in fields such as combinatorics,
physics, and chemistry. However, learning a probability distribution over $S_n$
poses significant challenges due to its intractable size and discrete nature.
In this paper, we introduce SymmetricDiffusers, a novel discrete diffusion
model that simplifies the task of learning a complicated distribution over
$S_n$ by decomposing it into learning simpler transitions of the reverse
diffusion using deep neural networks. We identify the riffle shuffle as an
effective forward transition and provide empirical guidelines for selecting the
diffusion length based on the theory of random walks on finite groups.
Additionally, we propose a generalized Plackett-Luce (PL) distribution for the
reverse transition, which is provably more expressive than the PL distribution.
We further introduce a theoretically grounded ""denoising schedule"" to improve
sampling and learning efficiency. Extensive experiments show that our model
achieves state-of-the-art or comparable performances on solving tasks including
sorting 4-digit MNIST images, jigsaw puzzles, and traveling salesman problems.
Our code is released at https://github.com/NickZhang53/SymmetricDiffusers.",http://arxiv.org/pdf/2410.02942v1
Streamlining Conformal Information Retrieval via Score Refinement,"Yotam Intrator, Ori Kelner, Regev Cohen, Roman Goldenberg, Ehud Rivlin, Daniel Freedman",2024-10-03,"Information retrieval (IR) methods, like retrieval augmented generation, are
fundamental to modern applications but often lack statistical guarantees.
Conformal prediction addresses this by retrieving sets guaranteed to include
relevant information, yet existing approaches produce large-sized sets,
incurring high computational costs and slow response times. In this work, we
introduce a score refinement method that applies a simple monotone
transformation to retrieval scores, leading to significantly smaller conformal
sets while maintaining their statistical guarantees. Experiments on various
BEIR benchmarks validate the effectiveness of our approach in producing compact
sets containing relevant information.",http://arxiv.org/pdf/2410.02914v1
Fine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation,"Xianzhi Li, Ran Zmigrod, Zhiqiang Ma, Xiaomo Liu, Xiaodan Zhu",2024-10-03,"Language models are capable of memorizing detailed patterns and information,
leading to a double-edged effect: they achieve impressive modeling performance
on downstream tasks with the stored knowledge but also raise significant
privacy concerns. Traditional differential privacy based training approaches
offer robust safeguards by employing a uniform noise distribution across all
parameters. However, this overlooks the distinct sensitivities and
contributions of individual parameters in privacy protection and often results
in suboptimal models. To address these limitations, we propose ANADP, a novel
algorithm that adaptively allocates additive noise based on the importance of
model parameters. We demonstrate that ANADP narrows the performance gap between
regular fine-tuning and traditional DP fine-tuning on a series of datasets
while maintaining the required privacy constraints.",http://arxiv.org/pdf/2410.02912v1
The Role of Deductive and Inductive Reasoning in Large Language Models,"Chengkun Cai, Xu Zhao, Haoliang Liu, Zhongyu Jiang, Tianfang Zhang, Zongkai Wu, Jenq-Neng Hwang, Lei Li",2024-10-03,"Large Language Models (LLMs) have achieved substantial progress in artificial
intelligence, particularly in reasoning tasks. However, their reliance on
static prompt structures, coupled with limited dynamic reasoning capabilities,
often constrains their adaptability to complex and evolving problem spaces. In
this paper, we propose the Deductive and InDuctive(DID) method, which enhances
LLM reasoning by dynamically integrating both deductive and inductive reasoning
within the prompt construction process. Drawing inspiration from cognitive
science, the DID approach mirrors human adaptive reasoning mechanisms, offering
a flexible framework that allows the model to adjust its reasoning pathways
based on task context and performance. We empirically validate the efficacy of
DID on established datasets such as AIW and MR-GSM8K, as well as on our custom
dataset, Holiday Puzzle, which presents tasks about different holiday date
calculating challenges. By leveraging DID's hybrid prompt strategy, we
demonstrate significant improvements in both solution accuracy and reasoning
quality, achieved without imposing substantial computational overhead. Our
findings suggest that DID provides a more robust and cognitively aligned
framework for reasoning in LLMs, contributing to the development of advanced
LLM-driven problem-solving strategies informed by cognitive science models.",http://arxiv.org/pdf/2410.02892v1
Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos,"Jianrui Zhang, Mu Cai, Yong Jae Lee",2024-10-03,"There has been growing sentiment recently that modern large multimodal models
(LMMs) have addressed most of the key challenges related to short video
comprehension. As a result, both academia and industry are gradually shifting
their attention towards the more complex challenges posed by understanding
long-form videos. However, is this really the case? Our studies indicate that
LMMs still lack many fundamental reasoning capabilities even when dealing with
short videos. We introduce Vinoground, a temporal counterfactual LMM evaluation
benchmark encompassing 1000 short and natural video-caption pairs. We
demonstrate that existing LMMs severely struggle to distinguish temporal
differences between different actions and object transformations. For example,
the best model GPT-4o only obtains ~50% on our text and video scores, showing a
large gap compared to the human baseline of ~90%. All open-source multimodal
models and CLIP-based models perform much worse, producing mostly random chance
performance. Through this work, we shed light onto the fact that temporal
reasoning in short videos is a problem yet to be fully solved. The dataset and
evaluation code are available at https://vinoground.github.io.",http://arxiv.org/pdf/2410.02763v1
FakeShield: Explainable Image Forgery Detection and Localization via Multi-modal Large Language Models,"Zhipei Xu, Xuanyu Zhang, Runyi Li, Zecheng Tang, Qing Huang, Jian Zhang",2024-10-03,"The rapid development of generative AI is a double-edged sword, which not
only facilitates content creation but also makes image manipulation easier and
more difficult to detect. Although current image forgery detection and
localization (IFDL) methods are generally effective, they tend to face two
challenges: \textbf{1)} black-box nature with unknown detection principle,
\textbf{2)} limited generalization across diverse tampering methods (e.g.,
Photoshop, DeepFake, AIGC-Editing). To address these issues, we propose the
explainable IFDL task and design FakeShield, a multi-modal framework capable of
evaluating image authenticity, generating tampered region masks, and providing
a judgment basis based on pixel-level and image-level tampering clues.
Additionally, we leverage GPT-4o to enhance existing IFDL datasets, creating
the Multi-Modal Tamper Description dataSet (MMTD-Set) for training FakeShield's
tampering analysis capabilities. Meanwhile, we incorporate a Domain Tag-guided
Explainable Forgery Detection Module (DTE-FDM) and a Multi-modal Forgery
Localization Module (MFLM) to address various types of tamper detection
interpretation and achieve forgery localization guided by detailed textual
descriptions. Extensive experiments demonstrate that FakeShield effectively
detects and localizes various tampering techniques, offering an explainable and
superior solution compared to previous IFDL methods.",http://arxiv.org/pdf/2410.02761v1
CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt Optimization for Text Generation,"Han He, Qianchu Liu, Lei Xu, Chaitanya Shivade, Yi Zhang, Sundararajan Srinivasan, Katrin Kirchhoff",2024-10-03,"Large language models (LLMs) can generate fluent summaries across domains
using prompting techniques, reducing the need to train models for summarization
applications. However, crafting effective prompts that guide LLMs to generate
summaries with the appropriate level of detail and writing style remains a
challenge. In this paper, we explore the use of salient information extracted
from the source document to enhance summarization prompts. We show that adding
keyphrases in prompts can improve ROUGE F1 and recall, making the generated
summaries more similar to the reference and more complete. The number of
keyphrases can control the precision-recall trade-off. Furthermore, our
analysis reveals that incorporating phrase-level salient information is
superior to word- or sentence-level. However, the impact on hallucination is
not universally positive across LLMs. To conduct this analysis, we introduce
Keyphrase Signal Extractor (CriSPO), a lightweight model that can be finetuned
to extract salient keyphrases. By using CriSPO, we achieve consistent ROUGE
improvements across datasets and open-weight and proprietary LLMs without any
LLM customization. Our findings provide insights into leveraging salient
information in building prompt-based summarization systems.",http://arxiv.org/pdf/2410.02748v1
Neutral residues: revisiting adapters for model extension,"Franck Signe Talla, Herve Jegou, Edouard Grave",2024-10-03,"We address the problem of extending a pretrained large language model to a
new domain that was not seen at training time, like adding a language for which
the original model has seen no or little training data. Popular solutions like
fine-tuning or low-rank adaptation are successful at domain adaptation, but
formally they do not add any extra capacity and degrade the performance in the
original domain.
  Our paper analyzes this extension problem under three angles: data,
architecture and training procedure, which are advantageously considered
jointly. In particular, we improve adapters and make it possible to learn an
entire new language while ensuring that the output of the neural network is
almost unchanged in the original domain. For this purpose, we modify the new
residual blocks in a way that leads each new residual block to output
near-zeros in the original domain.
  This solution of neutral residues, which borrows architectural components
from mixture of experts, is effective: with only 20% extra learnable weights
compared to an original model trained on English, we get results that are
significantly better than concurrent approaches (fine-tuning, low-rank or
vanilla adapters) in terms of the trade-off between learning a new language and
not forgetting English.",http://arxiv.org/pdf/2410.02744v1
Salient Information Prompting to Steer Content in Prompt-based Abstractive Summarization,"Lei Xu, Mohammed Asad Karim, Saket Dingliwal, Aparna Elangovan",2024-10-03,"Large language models (LLMs) can generate fluent summaries across domains
using prompting techniques, reducing the need to train models for summarization
applications. However, crafting effective prompts that guide LLMs to generate
summaries with the appropriate level of detail and writing style remains a
challenge. In this paper, we explore the use of salient information extracted
from the source document to enhance summarization prompts. We show that adding
keyphrases in prompts can improve ROUGE F1 and recall, making the generated
summaries more similar to the reference and more complete. The number of
keyphrases can control the precision-recall trade-off. Furthermore, our
analysis reveals that incorporating phrase-level salient information is
superior to word- or sentence-level. However, the impact on hallucination is
not universally positive across LLMs. To conduct this analysis, we introduce
Keyphrase Signal Extractor (SigExt), a lightweight model that can be finetuned
to extract salient keyphrases. By using SigExt, we achieve consistent ROUGE
improvements across datasets and open-weight and proprietary LLMs without any
LLM customization. Our findings provide insights into leveraging salient
information in building prompt-based summarization systems.",http://arxiv.org/pdf/2410.02741v1
Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models,"Zhengfeng Lai, Vasileios Saveris, Chen Chen, Hong-You Chen, Haotian Zhang, Bowen Zhang, Juan Lao Tebar, Wenze Hu, Zhe Gan, Peter Grasch, Meng Cao, Yinfei Yang",2024-10-03,"Recent advancements in multimodal models highlight the value of rewritten
captions for improving performance, yet key challenges remain. For example,
while synthetic captions often provide superior quality and image-text
alignment, it is not clear whether they can fully replace AltTexts: the role of
synthetic captions and their interaction with original web-crawled AltTexts in
pre-training is still not well understood. Moreover, different multimodal
foundation models may have unique preferences for specific caption formats, but
efforts to identify the optimal captions for each model remain limited. In this
work, we propose a novel, controllable, and scalable captioning pipeline
designed to generate diverse caption formats tailored to various multimodal
models. By examining Short Synthetic Captions (SSC) towards Dense Synthetic
Captions (DSC+) as case studies, we systematically explore their effects and
interactions with AltTexts across models such as CLIP, multimodal LLMs, and
diffusion models. Our findings reveal that a hybrid approach that keeps both
synthetic captions and AltTexts can outperform the use of synthetic captions
alone, improving both alignment and performance, with each model demonstrating
preferences for particular caption formats. This comprehensive analysis
provides valuable insights into optimizing captioning strategies, thereby
advancing the pre-training of multimodal foundation models.",http://arxiv.org/pdf/2410.02740v1
"Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation","Rohin Manvi, Anikait Singh, Stefano Ermon",2024-10-03,"Inference-time computation is a powerful paradigm to enhance the performance
of large language models (LLMs), with Best-of-N sampling being a widely used
technique. However, this method is computationally expensive, requiring both
(1) an external reward model and (2) the generation of multiple samples. In
this work, we introduce a new generative self-evaluation scheme designed to
adaptively reduce the number of generated samples while maintaining or even
improving performance. We use a generative reward model formulation, allowing
the LLM to predict mid-generation the probability that restarting the
generation will yield a better response. These predictions are obtained without
an external reward model and can be used to decide whether or not to generate
more samples, prune unpromising samples early on, or to pick the best sample.
This capability is very inexpensive as it involves generating a single
predefined token. Trained using a dataset constructed with real unfiltered
LMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval
increases from 21% to 34% with 16 samples and math performance on GSM8K
improves from 84% to 91%. By sampling only when the LLM determines that it is
beneficial to do so and adaptively adjusting temperature annealing, we
demonstrate that 74% of the improvement from using 16 samples can be achieved
with only 1.2 samples on average. We further demonstrate that 50-75% of samples
can be pruned early in generation with minimal degradation in performance.
Overall, our methods enable more efficient and scalable compute utilization
during inference for LLMs.",http://arxiv.org/pdf/2410.02725v1
Large Language Models as Markov Chains,"Oussama Zekri, Ambroise Odonnat, Abdelhakim Benechehab, Linus Bleistein, Nicolas Boullé, Ievgen Redko",2024-10-03,"Large language models (LLMs) have proven to be remarkably efficient, both
across a wide range of natural language processing tasks and well beyond them.
However, a comprehensive theoretical analysis of the origins of their
impressive performance remains elusive. In this paper, we approach this
challenging task by drawing an equivalence between generic autoregressive
language models with vocabulary of size $T$ and context window of size $K$ and
Markov chains defined on a finite state space of size $\mathcal{O}(T^K)$. We
derive several surprising findings related to the existence of a stationary
distribution of Markov chains that capture the inference power of LLMs, their
speed of convergence to it, and the influence of the temperature on the latter.
We then prove pre-training and in-context generalization bounds and show how
the drawn equivalence allows us to enrich their interpretation. Finally, we
illustrate our theoretical guarantees with experiments on several recent LLMs
to highlight how they capture the behavior observed in practice.",http://arxiv.org/pdf/2410.02724v1
Measurements with Noise: Bayesian Optimization for Co-optimizing Noise and Property Discovery in Automated Experiments,"Boris N. Slautin, Yu Liu, Jan Dec, Vladimir V. Shvartsman, Doru C. Lupascu, Maxim Ziatdinov, Sergei V. Kalinin",2024-10-03,"We have developed a Bayesian optimization (BO) workflow that integrates
intra-step noise optimization into automated experimental cycles. Traditional
BO approaches in automated experiments focus on optimizing experimental
trajectories but often overlook the impact of measurement noise on data quality
and cost. Our proposed framework simultaneously optimizes both the target
property and the associated measurement noise by introducing time as an
additional input parameter, thereby balancing the signal-to-noise ratio and
experimental duration. Two approaches are explored: a reward-driven noise
optimization and a double-optimization acquisition function, both enhancing the
efficiency of automated workflows by considering noise and cost within the
optimization process. We validate our method through simulations and real-world
experiments using Piezoresponse Force Microscopy (PFM), demonstrating the
successful optimization of measurement duration and property exploration. Our
approach offers a scalable solution for optimizing multiple variables in
automated experimental workflows, improving data quality, and reducing resource
expenditure in materials science and beyond.",http://arxiv.org/pdf/2410.02717v1
Selective Attention Improves Transformer,"Yaniv Leviathan, Matan Kalman, Yossi Matias",2024-10-03,"Unneeded elements in the attention's context degrade performance. We
introduce Selective Attention, a simple parameter-free change to the standard
attention mechanism which reduces attention to unneeded elements. Selective
attention improves language modeling performance in a variety of model sizes
and context lengths. For example, a range of transformers trained with the
language modeling objective on C4 with selective attention perform equivalently
to standard transformers with ~2X more heads and parameters in their attention
modules. Selective attention also allows decreasing the size of the attention's
context buffer, leading to meaningful reductions in the memory and compute
requirements during inference. For example, transformers with 100M parameters
trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and
47X less memory for their attention module, respectively, when equipped with
selective attention, as those without selective attention, with the same
validation perplexity.",http://arxiv.org/pdf/2410.02703v1
Discovering Clues of Spoofed LM Watermarks,"Thibaud Gloaguen, Nikola Jovanović, Robin Staab, Martin Vechev",2024-10-03,"LLM watermarks stand out as a promising way to attribute ownership of
LLM-generated text. One threat to watermark credibility comes from spoofing
attacks, where an unauthorized third party forges the watermark, enabling it to
falsely attribute arbitrary texts to a particular LLM. While recent works have
demonstrated that state-of-the-art schemes are in fact vulnerable to spoofing,
they lack deeper qualitative analysis of the texts produced by spoofing
methods. In this work, we for the first time reveal that there are observable
differences between genuine and spoofed watermark texts. Namely, we show that
regardless of their underlying approach, all current spoofing methods
consistently leave observable artifacts in spoofed texts, indicative of
watermark forgery. We build upon these findings to propose rigorous statistical
tests that reliably reveal the presence of such artifacts, effectively
discovering that a watermark was spoofed. Our experimental evaluation shows
high test power across all current spoofing methods, providing insights into
their fundamental limitations, and suggesting a way to mitigate this threat.",http://arxiv.org/pdf/2410.02693v1
DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life,"Yu Ying Chiu, Liwei Jiang, Yejin Choi",2024-10-03,"As we increasingly seek guidance from LLMs for decision-making in daily life,
many of these decisions are not clear-cut and depend significantly on the
personal values and ethical standards of the users. We present DailyDilemmas, a
dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma
includes two possible actions and with each action, the affected parties and
human values invoked. Based on these dilemmas, we consolidated a set of human
values across everyday topics e.g., interpersonal relationships, workplace, and
environmental issues. We evaluated LLMs on these dilemmas to determine what
action they will take and the values represented by these actions. Then, we
analyzed these values through the lens of five popular theories inspired by
sociology, psychology and philosophy. These theories are: World Value Survey,
Moral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and
Plutchik Wheel of Emotion. We find that LLMs are most aligned with the
self-expression over survival values in terms of World Value Survey, care over
loyalty in Moral Foundation Theory. Interestingly, we find large preferences
differences in models for some core values such as truthfulness e.g.,
Mixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to
select it by 9.4%. We also study the recent guidance released by OpenAI
(ModelSpec), and Anthropic (Constitutional AI) to understand how their released
principles reflect their actual value prioritization when facing nuanced moral
reasoning in daily-life settings. We find that end users cannot effectively
steer such prioritization using system prompts.",http://arxiv.org/pdf/2410.02683v1
"CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring the (Lack of) Cultural Knowledge of LLMs","Yu Ying Chiu, Liwei Jiang, Bill Yuchen Lin, Chan Young Park, Shuyue Stella Li, Sahithya Ravi, Mehar Bhatia, Maria Antoniak, Yulia Tsvetkov, Vered Shwartz, Yejin Choi",2024-10-03,"To make large language models (LLMs) more helpful across diverse cultures, it
is essential to have effective cultural knowledge benchmarks to measure and
track our progress. Effective benchmarks need to be robust, diverse, and
challenging. We introduce CulturalBench: a set of 1,227 human-written and
human-verified questions for effectively assessing LLMs' cultural knowledge,
covering 45 global regions including the underrepresented ones like Bangladesh,
Zimbabwe, and Peru. Questions - each verified by five independent annotators -
span 17 diverse topics ranging from food preferences to greeting etiquettes. We
evaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard which
share the same questions but asked differently. We find that LLMs are sensitive
to such difference in setups (e.g., GPT-4o with 27.3% difference). Compared to
human performance (92.6% accuracy), CulturalBench-Hard is more challenging for
frontier LLMs with the best performing model (GPT-4o) at only 61.5% and the
worst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with
tricky questions that have multiple correct answers (e.g., What utensils do the
Chinese usually use?), revealing a tendency to converge to a single answer. Our
results also indicate that OpenAI GPT-4o substantially outperform other
proprietary and open source models in questions related to all but one region
(Oceania). Nonetheless, all models consistently underperform on questions
related to South America and the Middle East.",http://arxiv.org/pdf/2410.02677v1
FAN: Fourier Analysis Networks,"Yihong Dong, Ge Li, Yongding Tao, Xue Jiang, Kechi Zhang, Jia Li, Jing Su, Jun Zhang, Jingjing Xu",2024-10-03,"Despite the remarkable success achieved by neural networks, particularly
those represented by MLP and Transformer, we reveal that they exhibit potential
flaws in the modeling and reasoning of periodicity, i.e., they tend to memorize
the periodic data rather than genuinely understanding the underlying principles
of periodicity. However, periodicity is a crucial trait in various forms of
reasoning and generalization, underpinning predictability across natural and
engineered systems through recurring patterns in observations. In this paper,
we propose FAN, a novel network architecture based on Fourier Analysis, which
empowers the ability to efficiently model and reason about periodic phenomena.
By introducing Fourier Series, the periodicity is naturally integrated into the
structure and computational processes of the neural network, thus achieving a
more accurate expression and prediction of periodic patterns. As a promising
substitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in
various models with fewer parameters and FLOPs. Through extensive experiments,
we demonstrate the effectiveness of FAN in modeling and reasoning about
periodic functions, and the superiority and generalizability of FAN across a
range of real-world tasks, including symbolic formula representation, time
series forecasting, and language modeling.",http://arxiv.org/pdf/2410.02675v1
AlphaIntegrator: Transformer Action Search for Symbolic Integration Proofs,"Mert Ünsal, Timon Gehr, Martin Vechev",2024-10-03,"We present the first correct-by-construction learning-based system for
step-by-step mathematical integration. The key idea is to learn a policy,
represented by a GPT transformer model, which guides the search for the right
mathematical integration rule, to be carried out by a symbolic solver.
Concretely, we introduce a symbolic engine with axiomatically correct actions
on mathematical expressions, as well as the first dataset for step-by-step
integration. Our GPT-style transformer model, trained on this synthetic data,
demonstrates strong generalization by surpassing its own data generator in
accuracy and efficiency, using 50% fewer search steps. Our experimental results
with SoTA LLMs also demonstrate that the standard approach of fine-tuning LLMs
on a set of question-answer pairs is insufficient for solving this mathematical
task. This motivates the importance of discovering creative methods for
combining LLMs with symbolic reasoning engines, of which our work is an
instance.",http://arxiv.org/pdf/2410.02666v1
Scalable Simulation-free Entropic Unbalanced Optimal Transport,"Jaemoo Choi, Jaewoong Choi",2024-10-03,"The Optimal Transport (OT) problem investigates a transport map that connects
two distributions while minimizing a given cost function. Finding such a
transport map has diverse applications in machine learning, such as generative
modeling and image-to-image translation. In this paper, we introduce a scalable
and simulation-free approach for solving the Entropic Unbalanced Optimal
Transport (EUOT) problem. We derive the dynamical form of this EUOT problem,
which is a generalization of the Schr\""odinger bridges (SB) problem. Based on
this, we derive dual formulation and optimality conditions of the EUOT problem
from the stochastic optimal control interpretation. By leveraging these
properties, we propose a simulation-free algorithm to solve EUOT, called
Simulation-free EUOT (SF-EUOT). While existing SB models require expensive
simulation costs during training and evaluation, our model achieves
simulation-free training and one-step generation by utilizing the reciprocal
property. Our model demonstrates significantly improved scalability in
generative modeling and image-to-image translation tasks compared to previous
SB methods.",http://arxiv.org/pdf/2410.02656v1
CAX: Cellular Automata Accelerated in JAX,"Maxence Faldor, Antoine Cully",2024-10-03,"Cellular automata have become a cornerstone for investigating emergence and
self-organization across diverse scientific disciplines, spanning neuroscience,
artificial life, and theoretical physics. However, the absence of a
hardware-accelerated cellular automata library limits the exploration of new
research directions, hinders collaboration, and impedes reproducibility. In
this work, we introduce CAX (Cellular Automata Accelerated in JAX), a
high-performance and flexible open-source library designed to accelerate
cellular automata research. CAX offers cutting-edge performance and a modular
design through a user-friendly interface, and can support both discrete and
continuous cellular automata with any number of dimensions. We demonstrate
CAX's performance and flexibility through a wide range of benchmarks and
applications. From classic models like elementary cellular automata and
Conway's Game of Life to advanced applications such as growing neural cellular
automata and self-classifying MNIST digits, CAX speeds up simulations up to
2,000 times faster. Furthermore, we demonstrate CAX's potential to accelerate
research by presenting a collection of three novel cellular automata
experiments, each implemented in just a few lines of code thanks to the
library's modular architecture. Notably, we show that a simple one-dimensional
cellular automaton can outperform GPT-4 on the 1D-ARC challenge.",http://arxiv.org/pdf/2410.02651v1
Inverse Entropic Optimal Transport Solves Semi-supervised Learning via Data Likelihood Maximization,"Mikhail Persiianov, Arip Asadulaev, Nikita Andreev, Nikita Starodubcev, Dmitry Baranchuk, Anastasis Kratsios, Evgeny Burnaev, Alexander Korotin",2024-10-03,"Learning conditional distributions $\pi^*(\cdot|x)$ is a central problem in
machine learning, which is typically approached via supervised methods with
paired data $(x,y) \sim \pi^*$. However, acquiring paired data samples is often
challenging, especially in problems such as domain translation. This
necessitates the development of $\textit{semi-supervised}$ models that utilize
both limited paired data and additional unpaired i.i.d. samples $x \sim
\pi^*_x$ and $y \sim \pi^*_y$ from the marginal distributions. The usage of
such combined data is complex and often relies on heuristic approaches. To
tackle this issue, we propose a new learning paradigm that integrates both
paired and unpaired data $\textbf{seamlessly}$ through the data likelihood
maximization techniques. We demonstrate that our approach also connects
intriguingly with inverse entropic optimal transport (OT). This finding allows
us to apply recent advances in computational OT to establish a $\textbf{light}$
learning algorithm to get $\pi^*(\cdot|x)$. Furthermore, we demonstrate through
empirical tests that our method effectively learns conditional distributions
using paired and unpaired data simultaneously.",http://arxiv.org/pdf/2410.02628v1
Achieving Fairness in Predictive Process Analytics via Adversarial Learning (Extended Version),"Massimiliano de Leoni, Alessandro Padella",2024-10-03,"Predictive business process analytics has become important for organizations,
offering real-time operational support for their processes. However, these
algorithms often perform unfair predictions because they are based on biased
variables (e.g., gender or nationality), namely variables embodying
discrimination. This paper addresses the challenge of integrating a debiasing
phase into predictive business process analytics to ensure that predictions are
not influenced by biased variables. Our framework leverages on adversial
debiasing is evaluated on four case studies, showing a significant reduction in
the contribution of biased variables to the predicted value. The proposed
technique is also compared with the state of the art in fairness in process
mining, illustrating that our framework allows for a more enhanced level of
fairness, while retaining a better prediction quality.",http://arxiv.org/pdf/2410.02618v1
IndicSentEval: How Effectively do Multilingual Transformer Models encode Linguistic Properties for Indic Languages?,"Akhilesh Aravapalli, Mounika Marreddy, Subba Reddy Oota, Radhika Mamidi, Manish Gupta",2024-10-03,"Transformer-based models have revolutionized the field of natural language
processing. To understand why they perform so well and to assess their
reliability, several studies have focused on questions such as: Which
linguistic properties are encoded by these models, and to what extent? How
robust are these models in encoding linguistic properties when faced with
perturbations in the input text? However, these studies have mainly focused on
BERT and the English language. In this paper, we investigate similar questions
regarding encoding capability and robustness for 8 linguistic properties across
13 different perturbations in 6 Indic languages, using 9 multilingual
Transformer models (7 universal and 2 Indic-specific). To conduct this study,
we introduce a novel multilingual benchmark dataset, IndicSentEval, containing
approximately $\sim$47K sentences. Surprisingly, our probing analysis of
surface, syntactic, and semantic properties reveals that while almost all
multilingual models demonstrate consistent encoding performance for English,
they show mixed results for Indic languages. As expected, Indic-specific
multilingual models capture linguistic properties in Indic languages better
than universal models. Intriguingly, universal models broadly exhibit better
robustness compared to Indic-specific models, particularly under perturbations
such as dropping both nouns and verbs, dropping only verbs, or keeping only
nouns. Overall, this study provides valuable insights into probing and
perturbation-specific strengths and weaknesses of popular multilingual
Transformer-based models for different Indic languages. We make our code and
dataset publicly available [https://tinyurl.com/IndicSentEval}].",http://arxiv.org/pdf/2410.02611v1
Beyond Expected Returns: A Policy Gradient Algorithm for Cumulative Prospect Theoretic Reinforcement Learning,"Olivier Lepel, Anas Barakat",2024-10-03,"The widely used expected utility theory has been shown to be empirically
inconsistent with human preferences in the psychology and behavioral economy
literatures. Cumulative Prospect Theory (CPT) has been developed to fill in
this gap and provide a better model for human-based decision-making supported
by empirical evidence. It allows to express a wide range of attitudes and
perceptions towards risk, gains and losses. A few years ago, CPT has been
combined with Reinforcement Learning (RL) to formulate a CPT policy
optimization problem where the goal of the agent is to search for a policy
generating long-term returns which are aligned with their preferences. In this
work, we revisit this policy optimization problem and provide new insights on
optimal policies and their nature depending on the utility function under
consideration. We further derive a novel policy gradient theorem for the CPT
policy optimization objective generalizing the seminal corresponding result in
standard RL. This result enables us to design a model-free policy gradient
algorithm to solve the CPT-RL problem. We illustrate the performance of our
algorithm in simple examples motivated by traffic control and electricity
management applications. We also demonstrate that our policy gradient algorithm
scales better to larger state spaces compared to the existing zeroth order
algorithm for solving the same problem.",http://arxiv.org/pdf/2410.02605v1
Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks,"Rui Hu, Yifan Zhang, Zhuoran Li, Longbo Huang",2024-10-03,"Generative Flow Networks (GFlowNets) are a novel class of generative models
designed to sample from unnormalized distributions and have found applications
in various important tasks, attracting great research interest in their
training algorithms. In general, GFlowNets are trained by fitting the forward
flow to the backward flow on sampled training objects. Prior work focused on
the choice of training objects, parameterizations, sampling and resampling
strategies, and backward policies, aiming to enhance credit assignment,
exploration, or exploitation of the training process. However, the choice of
regression loss, which can highly influence the exploration and exploitation
behavior of the under-training policy, has been overlooked. Due to the lack of
theoretical understanding for choosing an appropriate regression loss, most
existing algorithms train the flow network by minimizing the squared error of
the forward and backward flows in log-space, i.e., using the quadratic
regression loss. In this work, we rigorously prove that distinct regression
losses correspond to specific divergence measures, enabling us to design and
analyze regression losses according to the desired properties of the
corresponding divergence measures. Specifically, we examine two key properties:
zero-forcing and zero-avoiding, where the former promotes exploitation and
higher rewards, and the latter encourages exploration and enhances diversity.
Based on our theoretical framework, we propose three novel regression losses,
namely, Shifted-Cosh, Linex(1/2), and Linex(1). We evaluate them across three
benchmarks: hyper-grid, bit-sequence generation, and molecule generation. Our
proposed losses are compatible with most existing training algorithms, and
significantly improve the performances of the algorithms concerning convergence
speed, sample diversity, and robustness.",http://arxiv.org/pdf/2410.02596v1
IC3M: In-Car Multimodal Multi-object Monitoring for Abnormal Status of Both Driver and Passengers,"Zihan Fang, Zheng Lin, Senkang Hu, Hangcheng Cao, Yiqin Deng, Xianhao Chen, Yuguang Fang",2024-10-03,"Recently, in-car monitoring has emerged as a promising technology for
detecting early-stage abnormal status of the driver and providing timely alerts
to prevent traffic accidents. Although training models with multimodal data
enhances the reliability of abnormal status detection, the scarcity of labeled
data and the imbalance of class distribution impede the extraction of critical
abnormal state features, significantly deteriorating training performance.
Furthermore, missing modalities due to environment and hardware limitations
further exacerbate the challenge of abnormal status identification. More
importantly, monitoring abnormal health conditions of passengers, particularly
in elderly care, is of paramount importance but remains underexplored. To
address these challenges, we introduce our IC3M, an efficient
camera-rotation-based multimodal framework for monitoring both driver and
passengers in a car. Our IC3M comprises two key modules: an adaptive threshold
pseudo-labeling strategy and a missing modality reconstruction. The former
customizes pseudo-labeling thresholds for different classes based on the class
distribution, generating class-balanced pseudo labels to guide model training
effectively, while the latter leverages crossmodality relationships learned
from limited labels to accurately recover missing modalities by distribution
transferring from available modalities. Extensive experimental results
demonstrate that IC3M outperforms state-of-the-art benchmarks in accuracy,
precision, and recall while exhibiting superior robustness under limited
labeled data and severe missing modality.",http://arxiv.org/pdf/2410.02592v1
Boosting Sample Efficiency and Generalization in Multi-agent Reinforcement Learning via Equivariance,"Joshua McClellan, Naveed Haghani, John Winder, Furong Huang, Pratap Tokekar",2024-10-03,"Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency
and poor generalization [1]. These challenges are partially due to a lack of
structure or inductive bias in the neural networks typically used in learning
the policy. One such form of structure that is commonly observed in multi-agent
scenarios is symmetry. The field of Geometric Deep Learning has developed
Equivariant Graph Neural Networks (EGNN) that are equivariant (or symmetric) to
rotations, translations, and reflections of nodes. Incorporating equivariance
has been shown to improve learning efficiency and decrease error [ 2 ]. In this
paper, we demonstrate that EGNNs improve the sample efficiency and
generalization in MARL. However, we also show that a naive application of EGNNs
to MARL results in poor early exploration due to a bias in the EGNN structure.
To mitigate this bias, we present Exploration-enhanced Equivariant Graph Neural
Networks or E2GN2. We compare E2GN2 to other common function approximators
using common MARL benchmarks MPE and SMACv2. E2GN2 demonstrates a significant
improvement in sample efficiency, greater final reward convergence, and a 2x-5x
gain in over standard GNNs in our generalization tests. These results pave the
way for more reliable and effective solutions in complex multi-agent systems.",http://arxiv.org/pdf/2410.02581v1
ColaCare: Enhancing Electronic Health Record Modeling through Large Language Model-Driven Multi-Agent Collaboration,"Zixiang Wang, Yinghao Zhu, Huiya Zhao, Xiaochen Zheng, Tianlong Wang, Wen Tang, Yasha Wang, Chengwei Pan, Ewen M. Harrison, Junyi Gao, Liantao Ma",2024-10-03,"We introduce ColaCare, a framework that enhances Electronic Health Record
(EHR) modeling through multi-agent collaboration driven by Large Language
Models (LLMs). Our approach seamlessly integrates domain-specific expert models
with LLMs to bridge the gap between structured EHR data and text-based
reasoning. Inspired by clinical consultations, ColaCare employs two types of
agents: DoctorAgent and MetaAgent, which collaboratively analyze patient data.
Expert models process and generate predictions from numerical EHR data, while
LLM agents produce reasoning references and decision-making reports within the
collaborative consultation framework. We additionally incorporate the Merck
Manual of Diagnosis and Therapy (MSD) medical guideline within a
retrieval-augmented generation (RAG) module for authoritative evidence support.
Extensive experiments conducted on four distinct EHR datasets demonstrate
ColaCare's superior performance in mortality prediction tasks, underscoring its
potential to revolutionize clinical decision support systems and advance
personalized precision medicine. The code, complete prompt templates, more case
studies, etc. are publicly available at the anonymous link:
https://colacare.netlify.app.",http://arxiv.org/pdf/2410.02551v1
Personalized Quantum Federated Learning for Privacy Image Classification,"Jinjing Shi, Tian Chen, Shichao Zhang, Xuelong Li",2024-10-03,"Quantum federated learning has brought about the improvement of privacy image
classification, while the lack of personality of the client model may
contribute to the suboptimal of quantum federated learning. A personalized
quantum federated learning algorithm for privacy image classification is
proposed to enhance the personality of the client model in the case of an
imbalanced distribution of images. First, a personalized quantum federated
learning model is constructed, in which a personalized layer is set for the
client model to maintain the personalized parameters. Second, a personalized
quantum federated learning algorithm is introduced to secure the information
exchanged between the client and server.Third, the personalized federated
learning is applied to image classification on the FashionMNIST dataset, and
the experimental results indicate that the personalized quantum federated
learning algorithm can obtain global and local models with excellent
performance, even in situations where local training samples are imbalanced.
The server's accuracy is 100% with 8 clients and a distribution parameter of
100, outperforming the non-personalized model by 7%. The average client
accuracy is 2.9% higher than that of the non-personalized model with 2 clients
and a distribution parameter of 1. Compared to previous quantum federated
learning algorithms, the proposed personalized quantum federated learning
algorithm eliminates the need for additional local training while safeguarding
both model and data privacy.It may facilitate broader adoption and application
of quantum technologies, and pave the way for more secure, scalable, and
efficient quantum distribute machine learning solutions.",http://arxiv.org/pdf/2410.02547v1
Towards Layer-Wise Personalized Federated Learning: Adaptive Layer Disentanglement via Conflicting Gradients,"Minh Duong Nguyen, Khanh Le, Khoi Do, Nguyen H. Tran, Duc Nguyen, Chien Trinh, Zhaohui Yang",2024-10-03,"In personalized Federated Learning (pFL), high data heterogeneity can cause
significant gradient divergence across devices, adversely affecting the
learning process. This divergence, especially when gradients from different
users form an obtuse angle during aggregation, can negate progress, leading to
severe weight and gradient update degradation. To address this issue, we
introduce a new approach to pFL design, namely Federated Learning with
Layer-wise Aggregation via Gradient Analysis (FedLAG), utilizing the concept of
gradient conflict at the layer level. Specifically, when layer-wise gradients
of different clients form acute angles, those gradients align in the same
direction, enabling updates across different clients toward identifying
client-invariant features. Conversely, when layer-wise gradient pairs make
create obtuse angles, the layers tend to focus on client-specific tasks. In
hindsights, FedLAG assigns layers for personalization based on the extent of
layer-wise gradient conflicts. Specifically, layers with gradient conflicts are
excluded from the global aggregation process. The theoretical evaluation
demonstrates that when integrated into other pFL baselines, FedLAG enhances pFL
performance by a certain margin. Therefore, our proposed method achieves
superior convergence behavior compared with other baselines. Extensive
experiments show that our FedLAG outperforms several state-of-the-art methods
and can be easily incorporated with many existing methods to further enhance
performance.",http://arxiv.org/pdf/2410.02845v1
SAFLEX: Self-Adaptive Augmentation via Feature Label Extrapolation,"Mucong Ding, Bang An, Yuancheng Xu, Anirudh Satheesh, Furong Huang",2024-10-03,"Data augmentation, a cornerstone technique in deep learning, is crucial in
enhancing model performance, especially with scarce labeled data. While
traditional techniques are effective, their reliance on hand-crafted methods
limits their applicability across diverse data types and tasks. Although modern
learnable augmentation methods offer increased adaptability, they are
computationally expensive and challenging to incorporate within prevalent
augmentation workflows. In this work, we present a novel, efficient method for
data augmentation, effectively bridging the gap between existing augmentation
strategies and emerging datasets and learning tasks. We introduce SAFLEX
(Self-Adaptive Augmentation via Feature Label EXtrapolation), which learns the
sample weights and soft labels of augmented samples provided by any given
upstream augmentation pipeline, using a specifically designed efficient bilevel
optimization algorithm. Remarkably, SAFLEX effectively reduces the noise and
label errors of the upstream augmentation pipeline with a marginal
computational cost. As a versatile module, SAFLEX excels across diverse
datasets, including natural and medical images and tabular data, showcasing its
prowess in few-shot learning and out-of-distribution generalization. SAFLEX
seamlessly integrates with common augmentation strategies like RandAug, CutMix,
and those from large pre-trained generative models like stable diffusion and is
also compatible with frameworks such as CLIP's fine-tuning. Our findings
highlight the potential to adapt existing augmentation pipelines for new data
types and tasks, signaling a move towards more adaptable and resilient training
frameworks.",http://arxiv.org/pdf/2410.02512v1
CAnDOIT: Causal Discovery with Observational and Interventional Data from Time-Series,"Luca Castri, Sariah Mghames, Marc Hanheide, Nicola Bellotto",2024-10-03,"The study of cause-and-effect is of the utmost importance in many branches of
science, but also for many practical applications of intelligent systems. In
particular, identifying causal relationships in situations that include hidden
factors is a major challenge for methods that rely solely on observational data
for building causal models. This paper proposes CAnDOIT, a causal discovery
method to reconstruct causal models using both observational and interventional
time-series data. The use of interventional data in the causal analysis is
crucial for real-world applications, such as robotics, where the scenario is
highly complex and observational data alone are often insufficient to uncover
the correct causal structure. Validation of the method is performed initially
on randomly generated synthetic models and subsequently on a well-known
benchmark for causal structure learning in a robotic manipulation environment.
The experiments demonstrate that the approach can effectively handle data from
interventions and exploit them to enhance the accuracy of the causal analysis.
A Python implementation of CAnDOIT has also been developed and is publicly
available on GitHub: https://github.com/lcastri/causalflow.",http://arxiv.org/pdf/2410.02844v1
Neural DDEs with Learnable Delays for Partially Observed Dynamical Systems,"Thibault Monsel, Emmanuel Menier, Onofrio Semeraro, Lionel Mathelin, Guillaume Charpiat",2024-10-03,"Many successful methods to learn dynamical systems from data have recently
been introduced. Such methods often rely on the availability of the system's
full state. However, this underlying hypothesis is rather restrictive as it is
typically not confirmed in practice, leaving us with partially observed
systems. Utilizing the Mori-Zwanzig (MZ) formalism from statistical physics, we
demonstrate that Constant Lag Neural Delay Differential Equations (NDDEs)
naturally serve as suitable models for partially observed states. In empirical
evaluation, we show that such models outperform existing methods on both
synthetic and experimental data.",http://arxiv.org/pdf/2410.02843v1
færdXel: An Expert System for Danish Traffic Law,"Luís Cruz-Filipe, Jonas Vistrup",2024-10-04,"We present f{\ae}rdXel, a tool for symbolic reasoning in the domain of Danish
traffic law. f{\ae}rdXel combines techniques from logic programming with a
novel interface that allows users to navigate through its reasoning process,
thereby ensuring the system's trustworthiness. A preliminary empirical
evaluation indicates that this work is seen as very promising, and has the
potential to become a foundation for real-world AI tools supporting
professionals in the Danish legal sector.",http://arxiv.org/pdf/2410.03560v1
Ward: Provable RAG Dataset Inference via LLM Watermarks,"Nikola Jovanović, Robin Staab, Maximilian Baader, Martin Vechev",2024-10-04,"Retrieval-Augmented Generation (RAG) improves LLMs by enabling them to
incorporate external data during generation. This raises concerns for data
owners regarding unauthorized use of their content in RAG systems. Despite its
importance, the challenge of detecting such unauthorized usage remains
underexplored, with existing datasets and methodologies from adjacent fields
being ill-suited for its study. In this work, we take several steps to bridge
this gap. First, we formalize this problem as (black-box) RAG Dataset Inference
(RAG-DI). To facilitate research on this challenge, we further introduce a
novel dataset specifically designed for benchmarking RAG-DI methods under
realistic conditions, and propose a set of baseline approaches. Building on
this foundation, we introduce Ward, a RAG-DI method based on LLM watermarks
that enables data owners to obtain rigorous statistical guarantees regarding
the usage of their dataset in a RAG system. In our experimental evaluation, we
show that Ward consistently outperforms all baselines across many challenging
settings, achieving higher accuracy, superior query efficiency and robustness.
Our work provides a foundation for future studies of RAG-DI and highlights LLM
watermarks as a promising approach to this problem.",http://arxiv.org/pdf/2410.03537v1
Gradient-based Jailbreak Images for Multimodal Fusion Models,"Javier Rando, Hannah Korevaar, Erik Brinkman, Ivan Evtimov, Florian Tramèr",2024-10-04,"Augmenting language models with image inputs may enable more effective
jailbreak attacks through continuous optimization, unlike text inputs that
require discrete optimization. However, new multimodal fusion models tokenize
all input modalities using non-differentiable functions, which hinders
straightforward attacks. In this work, we introduce the notion of a tokenizer
shortcut that approximates tokenization with a continuous function and enables
continuous optimization. We use tokenizer shortcuts to create the first
end-to-end gradient image attacks against multimodal fusion models. We evaluate
our attacks on Chameleon models and obtain jailbreak images that elicit harmful
information for 72.5% of prompts. Jailbreak images outperform text jailbreaks
optimized with the same objective and require 3x lower compute budget to
optimize 50x more input tokens. Finally, we find that representation
engineering defenses, like Circuit Breakers, trained only on text attacks can
effectively transfer to adversarial image inputs.",http://arxiv.org/pdf/2410.03489v1
A Multimodal Framework for Deepfake Detection,"Kashish Gandhi, Prutha Kulkarni, Taran Shah, Piyush Chaudhari, Meera Narvekar, Kranti Ghag",2024-10-04,"The rapid advancement of deepfake technology poses a significant threat to
digital media integrity. Deepfakes, synthetic media created using AI, can
convincingly alter videos and audio to misrepresent reality. This creates risks
of misinformation, fraud, and severe implications for personal privacy and
security. Our research addresses the critical issue of deepfakes through an
innovative multimodal approach, targeting both visual and auditory elements.
This comprehensive strategy recognizes that human perception integrates
multiple sensory inputs, particularly visual and auditory information, to form
a complete understanding of media content. For visual analysis, a model that
employs advanced feature extraction techniques was developed, extracting nine
distinct facial characteristics and then applying various machine learning and
deep learning models. For auditory analysis, our model leverages
mel-spectrogram analysis for feature extraction and then applies various
machine learning and deep learningmodels. To achieve a combined analysis, real
and deepfake audio in the original dataset were swapped for testing purposes
and ensured balanced samples. Using our proposed models for video and audio
classification i.e. Artificial Neural Network and VGG19, the overall sample is
classified as deepfake if either component is identified as such. Our
multimodal framework combines visual and auditory analyses, yielding an
accuracy of 94%.",http://arxiv.org/pdf/2410.03487v1
Group Fairness in Peer Review,"Haris Aziz, Evi Micha, Nisarg Shah",2024-10-04,"Large conferences such as NeurIPS and AAAI serve as crossroads of various AI
fields, since they attract submissions from a vast number of communities.
However, in some cases, this has resulted in a poor reviewing experience for
some communities, whose submissions get assigned to less qualified reviewers
outside of their communities. An often-advocated solution is to break up any
such large conference into smaller conferences, but this can lead to isolation
of communities and harm interdisciplinary research. We tackle this challenge by
introducing a notion of group fairness, called the core, which requires that
every possible community (subset of researchers) to be treated in a way that
prevents them from unilaterally benefiting by withdrawing from a large
conference.
  We study a simple peer review model, prove that it always admits a reviewing
assignment in the core, and design an efficient algorithm to find one such
assignment. We use real data from CVPR and ICLR conferences to compare our
algorithm to existing reviewing assignment algorithms on a number of metrics.",http://arxiv.org/pdf/2410.03474v1
Towards Real-time Intrahepatic Vessel Identification in Intraoperative Ultrasound-Guided Liver Surgery,"Karl-Philippe Beaudet, Alexandros Karargyris, Sidaty El Hadramy, Stéphane Cotin, Jean-Paul Mazellier, Nicolas Padoy, Juan Verde",2024-10-04,"While laparoscopic liver resection is less prone to complications and
maintains patient outcomes compared to traditional open surgery, its complexity
hinders widespread adoption due to challenges in representing the liver's
internal structure. Laparoscopic intraoperative ultrasound offers efficient,
cost-effective and radiation-free guidance. Our objective is to aid physicians
in identifying internal liver structures using laparoscopic intraoperative
ultrasound. We propose a patient-specific approach using preoperative 3D
ultrasound liver volume to train a deep learning model for real-time
identification of portal tree and branch structures. Our personalized AI model,
validated on ex vivo swine livers, achieved superior precision (0.95) and
recall (0.93) compared to surgeons, laying groundwork for precise vessel
identification in ultrasound-based liver resection. Its adaptability and
potential clinical impact promise to advance surgical interventions and improve
patient care.",http://arxiv.org/pdf/2410.03420v1
SoundSignature: What Type of Music Do You Like?,"Brandon James Carone, Pablo Ripollés",2024-10-04,"SoundSignature is a music application that integrates a custom OpenAI
Assistant to analyze users' favorite songs. The system incorporates
state-of-the-art Music Information Retrieval (MIR) Python packages to combine
extracted acoustic/musical features with the assistant's extensive knowledge of
the artists and bands. Capitalizing on this combined knowledge, SoundSignature
leverages semantic audio and principles from the emerging Internet of Sounds
(IoS) ecosystem, integrating MIR with AI to provide users with personalized
insights into the acoustic properties of their music, akin to a musical
preference personality report. Users can then interact with the chatbot to
explore deeper inquiries about the acoustic analyses performed and how they
relate to their musical taste. This interactivity transforms the application,
acting not only as an informative resource about familiar and/or favorite
songs, but also as an educational platform that enables users to deepen their
understanding of musical features, music theory, acoustic properties commonly
used in signal processing, and the artists behind the music. Beyond general
usability, the application also incorporates several well-established
open-source musician-specific tools, such as a chord recognition algorithm
(CREMA), a source separation algorithm (DEMUCS), and an audio-to-MIDI converter
(basic-pitch). These features allow users without coding skills to access
advanced, open-source music processing algorithms simply by interacting with
the chatbot (e.g., can you give me the stems of this song?). In this paper, we
highlight the application's innovative features and educational potential, and
present findings from a pilot user study that evaluates its efficacy and
usability.",http://arxiv.org/pdf/2410.03375v1
AutoPenBench: Benchmarking Generative Agents for Penetration Testing,"Luca Gioacchini, Marco Mellia, Idilio Drago, Alexander Delsanto, Giuseppe Siracusano, Roberto Bifulco",2024-10-04,"Generative AI agents, software systems powered by Large Language Models
(LLMs), are emerging as a promising approach to automate cybersecurity tasks.
Among the others, penetration testing is a challenging field due to the task
complexity and the diverse strategies to simulate cyber-attacks. Despite
growing interest and initial studies in automating penetration testing with
generative agents, there remains a significant gap in the form of a
comprehensive and standard framework for their evaluation and development. This
paper introduces AutoPenBench, an open benchmark for evaluating generative
agents in automated penetration testing. We present a comprehensive framework
that includes 33 tasks, each representing a vulnerable system that the agent
has to attack. Tasks are of increasing difficulty levels, including in-vitro
and real-world scenarios. We assess the agent performance with generic and
specific milestones that allow us to compare results in a standardised manner
and understand the limits of the agent under test. We show the benefits of
AutoPenBench by testing two agent architectures: a fully autonomous and a
semi-autonomous supporting human interaction. We compare their performance and
limitations. For example, the fully autonomous agent performs unsatisfactorily
achieving a 21% Success Rate (SR) across the benchmark, solving 27% of the
simple tasks and only one real-world task. In contrast, the assisted agent
demonstrates substantial improvements, with 64% of SR. AutoPenBench allows us
also to observe how different LLMs like GPT-4o or OpenAI o1 impact the ability
of the agents to complete the tasks. We believe that our benchmark fills the
gap with a standard and flexible framework to compare penetration testing
agents on a common ground. We hope to extend AutoPenBench along with the
research community by making it available under
https://github.com/lucagioacchini/auto-pen-bench.",http://arxiv.org/pdf/2410.03225v1
Remaining Useful Life Prediction: A Study on Multidimensional Industrial Signal Processing and Efficient Transfer Learning Based on Large Language Models,"Yan Chen, Cheng Liu",2024-10-04,"Remaining useful life (RUL) prediction is crucial for maintaining modern
industrial systems, where equipment reliability and operational safety are
paramount. Traditional methods, based on small-scale deep learning or
physical/statistical models, often struggle with complex, multidimensional
sensor data and varying operating conditions, limiting their generalization
capabilities. To address these challenges, this paper introduces an innovative
regression framework utilizing large language models (LLMs) for RUL prediction.
By leveraging the modeling power of LLMs pre-trained on corpus data, the
proposed model can effectively capture complex temporal dependencies and
improve prediction accuracy. Extensive experiments on the Turbofan engine's RUL
prediction task show that the proposed model surpasses state-of-the-art (SOTA)
methods on the challenging FD002 and FD004 subsets and achieves near-SOTA
results on the other subsets. Notably, different from previous research, our
framework uses the same sliding window length and all sensor signals for all
subsets, demonstrating strong consistency and generalization. Moreover,
transfer learning experiments reveal that with minimal target domain data for
fine-tuning, the model outperforms SOTA methods trained on full target domain
data. This research highlights the significant potential of LLMs in industrial
signal processing and RUL prediction, offering a forward-looking solution for
health management in future intelligent industrial systems.",http://arxiv.org/pdf/2410.03134v1
AIME: AI System Optimization via Multiple LLM Evaluators,"Bhrij Patel, Souradip Chakraborty, Wesley A. Suttle, Mengdi Wang, Amrit Singh Bedi, Dinesh Manocha",2024-10-04,"Text-based AI system optimization typically involves a feedback loop scheme
where a single LLM generates an evaluation in natural language of the current
output to improve the next iteration's output. However, in this work, we
empirically demonstrate that for a practical and complex task (code generation)
with multiple criteria to evaluate, utilizing only one LLM evaluator tends to
let errors in generated code go undetected, thus leading to incorrect
evaluations and ultimately suboptimal test case performance. Motivated by this
failure case, we assume there exists an optimal evaluation policy that samples
an evaluation between response and ground truth. We then theoretically prove
that a linear combination of multiple evaluators can approximate this optimal
policy. From this insight, we propose AI system optimization via Multiple LLM
Evaluators (AIME). AIME is an evaluation protocol that utilizes multiple LLMs
that each independently generate an evaluation on separate criteria and then
combine them via concatenation. We provide an extensive empirical study showing
AIME outperforming baseline methods in code generation tasks, with up to $62\%$
higher error detection rate and up to $16\%$ higher success rate than a single
LLM evaluation protocol on LeetCodeHard and HumanEval datasets. We also show
that the selection of the number of evaluators and which criteria to utilize is
non-trivial as it can impact pact success rate by up to $12\%$.",http://arxiv.org/pdf/2410.03131v1
Understanding Decision Subjects' Engagement with and Perceived Fairness of AI Models When Opportunities of Qualification Improvement Exist,"Meric Altug Gemalmaz, Ming Yin",2024-10-04,"We explore how an AI model's decision fairness affects people's engagement
with and perceived fairness of the model if they are subject to its decisions,
but could repeatedly and strategically respond to these decisions. Two types of
strategic responses are considered -- people could determine whether to
continue interacting with the model, and whether to invest in themselves to
improve their chance of future favorable decisions from the model. Via three
human-subject experiments, we found that in decision subjects' strategic,
repeated interactions with an AI model, the model's decision fairness does not
change their willingness to interact with the model or to improve themselves,
even when the model exhibits unfairness on salient protected attributes.
However, decision subjects still perceive the AI model to be less fair when it
systematically biases against their group, especially if the difficulty of
improving one's qualification for the favorable decision is larger for the
lowly-qualified people.",http://arxiv.org/pdf/2410.03126v1
Strategic Insights from Simulation Gaming of AI Race Dynamics,"Ross Gruetzemacher, Shahar Avin, James Fox, Alexander K Saeri",2024-10-04,"We present insights from ""Intelligence Rising"", a scenario exploration
exercise about possible AI futures. Drawing on the experiences of facilitators
who have overseen 43 games over a four-year period, we illuminate recurring
patterns, strategies, and decision-making processes observed during gameplay.
Our analysis reveals key strategic considerations about AI development
trajectories in this simulated environment, including: the destabilising
effects of AI races, the crucial role of international cooperation in
mitigating catastrophic risks, the challenges of aligning corporate and
national interests, and the potential for rapid, transformative change in AI
capabilities. We highlight places where we believe the game has been effective
in exposing participants to the complexities and uncertainties inherent in AI
governance. Key recurring gameplay themes include the emergence of
international agreements, challenges to the robustness of such agreements, the
critical role of cybersecurity in AI development, and the potential for
unexpected crises to dramatically alter AI trajectories. By documenting these
insights, we aim to provide valuable foresight for policymakers, industry
leaders, and researchers navigating the complex landscape of AI development and
governance.",http://arxiv.org/pdf/2410.03092v1
Integrating Natural Language Prompting Tasks in Introductory Programming Courses,"Chris Kerslake, Paul Denny, David H Smith IV, James Prather, Juho Leinonen, Andrew Luxton-Reilly, Stephen MacNeil",2024-10-04,"Introductory programming courses often emphasize mastering syntax and basic
constructs before progressing to more complex and interesting programs. This
bottom-up approach can be frustrating for novices, shifting the focus away from
problem solving and potentially making computing less appealing to a broad
range of students. The rise of generative AI for code production could
partially address these issues by fostering new skills via interaction with AI
models, including constructing high-level prompts and evaluating code that is
automatically generated. In this experience report, we explore the inclusion of
two prompt-focused activities in an introductory course, implemented across
four labs in a six-week module. The first requires students to solve
computational problems by writing natural language prompts, emphasizing
problem-solving over syntax. The second involves students crafting prompts to
generate code equivalent to provided fragments, to foster an understanding of
the relationship between prompts and code. Most of the students in the course
had reported finding programming difficult to learn, often citing frustrations
with syntax and debugging. We found that self-reported difficulty with learning
programming had a strong inverse relationship with performance on traditional
programming assessments such as tests and projects, as expected. However,
performance on the natural language tasks was less strongly related to
self-reported difficulty, suggesting they may target different skills. Learning
how to communicate with AI coding models is becoming an important skill, and
natural language prompting tasks may appeal to a broad range of students.",http://arxiv.org/pdf/2410.03063v1
Permissive Information-Flow Analysis for Large Language Models,"Shoaib Ahmed Siddiqui, Radhika Gaonkar, Boris Köpf, David Krueger, Andrew Paverd, Ahmed Salem, Shruti Tople, Lukas Wutschitz, Menglin Xia, Santiago Zanella-Béguelin",2024-10-04,"Large Language Models (LLMs) are rapidly becoming commodity components of
larger software systems. This poses natural security and privacy problems:
poisoned data retrieved from one component can change the model's behavior and
compromise the entire system, including coercing the model to spread
confidential data to untrusted components. One promising approach is to tackle
this problem at the system level via dynamic information flow (aka taint)
tracking. Unfortunately, the traditional approach of propagating the most
restrictive input label to the output is too conservative for applications
where LLMs operate on inputs retrieved from diverse sources. In this paper, we
propose a novel, more permissive approach to propagate information flow labels
through LLM queries. The key idea behind our approach is to propagate only the
labels of the samples that were influential in generating the model output and
to eliminate the labels of unnecessary input. We implement and investigate the
effectiveness of two variations of this approach, based on (i) prompt-based
retrieval augmentation, and (ii) a $k$-nearest-neighbors language model. We
compare these with the baseline of an introspection-based influence estimator
that directly asks the language model to predict the output label. The results
obtained highlight the superiority of our prompt-based label propagator, which
improves the label in more than 85% of the cases in an LLM agent setting. These
findings underscore the practicality of permissive label propagation for
retrieval augmentation.",http://arxiv.org/pdf/2410.03055v1
SPINE: Online Semantic Planning for Missions with Incomplete Natural Language Specifications in Unstructured Environments,"Zachary Ravichandran, Varun Murali, Mariliza Tzes, George J. Pappas, Vijay Kumar",2024-10-03,"As robots become increasingly capable, users will want to describe high-level
missions and have robots fill in the gaps. In many realistic settings,
pre-built maps are difficult to obtain, so execution requires exploration and
mapping that are necessary and specific to the mission. Consider an emergency
response scenario where a user commands a robot, ""triage impacted regions."" The
robot must infer relevant semantics (victims, etc.) and exploration targets
(damaged regions) based on priors or other context, then explore and refine its
plan online. These missions are incompletely specified, meaning they imply
subtasks and semantics. While many semantic planning methods operate online,
they are typically designed for well specified tasks such as object search or
exploration. Recently, Large Language Models (LLMs) have demonstrated powerful
contextual reasoning over a range of robotic tasks described in natural
language. However, existing LLM planners typically do not consider online
planning or complex missions; rather, relevant subtasks are provided by a
pre-built map or a user. We address these limitations via SPINE (online
Semantic Planner for missions with Incomplete Natural language specifications
in unstructured Environments). SPINE uses an LLM to reason about subtasks
implied by the mission then realizes these subtasks in a receding horizon
framework. Tasks are automatically validated for safety and refined online with
new observations. We evaluate SPINE in simulation and real-world settings.
Evaluation missions require multiple steps of semantic reasoning and
exploration in cluttered outdoor environments of over 20,000m$^2$ area. We
evaluate SPINE against competitive baselines in single-agent and air-ground
teaming applications. Please find videos and software on our project page:
https://zacravichandran.github.io/SPINE",http://arxiv.org/pdf/2410.03035v1
CounterQuill: Investigating the Potential of Human-AI Collaboration in Online Counterspeech Writing,"Xiaohan Ding, Kaike Ping, Uma Sushmitha Gunturi, Buse Carik, Sophia Stil, Lance T Wilhelm, Taufiq Daryanto, James Hawdon, Sang Won Lee, Eugenia H Rho",2024-10-03,"Online hate speech has become increasingly prevalent on social media
platforms, causing harm to individuals and society. While efforts have been
made to combat this issue through content moderation, the potential of
user-driven counterspeech as an alternative solution remains underexplored.
Existing counterspeech methods often face challenges such as fear of
retaliation and skill-related barriers. To address these challenges, we
introduce CounterQuill, an AI-mediated system that assists users in composing
effective and empathetic counterspeech. CounterQuill provides a three-step
process: (1) a learning session to help users understand hate speech and
counterspeech; (2) a brainstorming session that guides users in identifying key
elements of hate speech and exploring counterspeech strategies; and (3) a
co-writing session that enables users to draft and refine their counterspeech
with CounterQuill. We conducted a within-subjects user study with 20
participants to evaluate CounterQuill in comparison to ChatGPT. Results show
that CounterQuill's guidance and collaborative writing process provided users a
stronger sense of ownership over their co-authored counterspeech. Users
perceived CounterQuill as a writing partner and thus were more willing to post
the co-written counterspeech online compared to the one written with ChatGPT.",http://arxiv.org/pdf/2410.03032v1
Is Your Paper Being Reviewed by an LLM? Investigating AI Text Detectability in Peer Review,"Sungduk Yu, Man Luo, Avinash Madasu, Vasudev Lal, Phillip Howard",2024-10-03,"Peer review is a critical process for ensuring the integrity of published
scientific research. Confidence in this process is predicated on the assumption
that experts in the relevant domain give careful consideration to the merits of
manuscripts which are submitted for publication. With the recent rapid
advancements in the linguistic capabilities of large language models (LLMs), a
new potential risk to the peer review process is that negligent reviewers will
rely on LLMs to perform the often time consuming process of reviewing a paper.
In this study, we investigate the ability of existing AI text detection
algorithms to distinguish between peer reviews written by humans and different
state-of-the-art LLMs. Our analysis shows that existing approaches fail to
identify many GPT-4o written reviews without also producing a high number of
false positive classifications. To address this deficiency, we propose a new
detection approach which surpasses existing methods in the identification of
GPT-4o written peer reviews at low levels of false positive classifications.
Our work reveals the difficulty of accurately identifying AI-generated text at
the individual review level, highlighting the urgent need for new tools and
methods to detect this type of unethical application of generative AI.",http://arxiv.org/pdf/2410.03019v1
"Transforming Teachers' Roles and Agencies in the Era of Generative AI: Perceptions, Acceptance, Knowledge, and Practices",Xiaoming Zhai,2024-10-03,"This paper explores the transformative impact of Generative Artificial
Intelligence (GenAI) on teachers' roles and agencies in education, presenting a
comprehensive framework that addresses teachers' perceptions, knowledge,
acceptance, and practices of GenAI. As GenAI technologies, such as ChatGPT,
become increasingly integrated into educational settings, teachers are required
to adapt to evolving classroom dynamics, where AI plays a significant role in
content creation, personalized learning, and student engagement. However,
existing literature often treats these factors in isolation, overlooking how
they collectively influence teachers' ability to effectively integrate GenAI
into their pedagogical practices. This paper fills this gap by proposing a
framework that categorizes teachers into four roles -- Observer, Adopter,
Collaborator, and Innovator -- each representing different levels of GenAI
engagement, outlining teachers' agencies in GenAI classrooms. By highlighting
the need for continuous professional development and institutional support, we
demonstrate how teachers can evolve from basic GenAI users to co-creators of
knowledge alongside GenAI systems. The findings emphasize that for GenAI to
reach its full educational potential, teachers must not only accept and
understand its capabilities but also integrate it deeply into their teaching
strategies. This study contributes to the growing literature on GenAI in
education, offering practical implications for supporting teachers in
navigating the complexities of GenAI adoption.",http://arxiv.org/pdf/2410.03018v1
F-Fidelity: A Robust Framework for Faithfulness Evaluation of Explainable AI,"Xu Zheng, Farhad Shirani, Zhuomin Chen, Chaohao Lin, Wei Cheng, Wenbo Guo, Dongsheng Luo",2024-10-03,"Recent research has developed a number of eXplainable AI (XAI) techniques.
Although extracting meaningful insights from deep learning models, how to
properly evaluate these XAI methods remains an open problem. The most widely
used approach is to perturb or even remove what the XAI method considers to be
the most important features in an input and observe the changes in the output
prediction. This approach although efficient suffers the Out-of-Distribution
(OOD) problem as the perturbed samples may no longer follow the original data
distribution. A recent method RemOve And Retrain (ROAR) solves the OOD issue by
retraining the model with perturbed samples guided by explanations. However,
the training may not always converge given the distribution difference.
Furthermore, using the model retrained based on XAI methods to evaluate these
explainers may cause information leakage and thus lead to unfair comparisons.
We propose Fine-tuned Fidelity F-Fidelity, a robust evaluation framework for
XAI, which utilizes i) an explanation-agnostic fine-tuning strategy, thus
mitigating the information leakage issue and ii) a random masking operation
that ensures that the removal step does not generate an OOD input. We designed
controlled experiments with state-of-the-art (SOTA) explainers and their
degraded version to verify the correctness of our framework. We conducted
experiments on multiple data structures, such as images, time series, and
natural language. The results demonstrate that F-Fidelity significantly
improves upon prior evaluation metrics in recovering the ground-truth ranking
of the explainers. Furthermore, we show both theoretically and empirically
that, given a faithful explainer, F-Fidelity metric can be used to compute the
sparsity of influential input components, i.e., to extract the true explanation
size.",http://arxiv.org/pdf/2410.02970v1
Label-Free Subjective Player Experience Modelling via Let's Play Videos,"Dave Goel, Athar Mahmoudi-Nejad, Matthew Guzdial",2024-10-03,"Player Experience Modelling (PEM) is the study of AI techniques applied to
modelling a player's experience within a video game. PEM development can be
labour-intensive, requiring expert hand-authoring or specialized data
collection. In this work, we propose a novel PEM development approach,
approximating player experience from gameplay video. We evaluate this approach
predicting affect in the game Angry Birds via a human subject study. We
validate that our PEM can strongly correlate with self-reported and sensor
measures of affect, demonstrating the potential of this approach.",http://arxiv.org/pdf/2410.02967v1
AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML,"Patara Trirat, Wonyong Jeong, Sung Ju Hwang",2024-10-03,"Automated machine learning (AutoML) accelerates AI development by automating
tasks in the development pipeline, such as optimal model search and
hyperparameter tuning. Existing AutoML systems often require technical
expertise to set up complex tools, which is in general time-consuming and
requires a large amount of human effort. Therefore, recent works have started
exploiting large language models (LLM) to lessen such burden and increase the
usability of AutoML frameworks via a natural language interface, allowing
non-expert users to build their data-driven solutions. These methods, however,
are usually designed only for a particular process in the AI development
pipeline and do not efficiently use the inherent capacity of the LLMs. This
paper proposes AutoML-Agent, a novel multi-agent framework tailored for
full-pipeline AutoML, i.e., from data retrieval to model deployment.
AutoML-Agent takes user's task descriptions, facilitates collaboration between
specialized LLM agents, and delivers deployment-ready models. Unlike existing
work, instead of devising a single plan, we introduce a retrieval-augmented
planning strategy to enhance exploration to search for more optimal plans. We
also decompose each plan into sub-tasks (e.g., data preprocessing and neural
network design) each of which is solved by a specialized agent we build via
prompting executing in parallel, making the search process more efficient.
Moreover, we propose a multi-stage verification to verify executed results and
guide the code generation LLM in implementing successful solutions. Extensive
experiments on seven downstream tasks using fourteen datasets show that
AutoML-Agent achieves a higher success rate in automating the full AutoML
process, yielding systems with good performance throughout the diverse domains.",http://arxiv.org/pdf/2410.02958v1
"AiBAT: Artificial Intelligence/Instructions for Build, Assembly, and Test","Benjamin Nuernberger, Anny Liu, Heather Stefanini, Richard Otis, Amanda Towler, R. Peter Dillon",2024-10-03,"Instructions for Build, Assembly, and Test (IBAT) refers to the process used
whenever any operation is conducted on hardware, including tests, assembly, and
maintenance. Currently, the generation of IBAT documents is time-intensive, as
users must manually reference and transfer information from engineering
diagrams and parts lists into IBAT instructions. With advances in machine
learning and computer vision, however, it is possible to have an artificial
intelligence (AI) model perform the partial filling of the IBAT template,
freeing up engineer time for more highly skilled tasks. AiBAT is a novel system
for assisting users in authoring IBATs. It works by first analyzing assembly
drawing documents, extracting information and parsing it, and then filling in
IBAT templates with the extracted information. Such assisted authoring has
potential to save time and reduce cost. This paper presents an overview of the
AiBAT system, including promising preliminary results and discussion on future
work.",http://arxiv.org/pdf/2410.02955v1
Safeguard is a Double-edged Sword: Denial-of-service Attack on Large Language Models,"Qingzhao Zhang, Ziyang Xiong, Z. Morley Mao",2024-10-03,"Safety is a paramount concern of large language models (LLMs) in their open
deployment. To this end, safeguard methods aim to enforce the ethical and
responsible use of LLMs through safety alignment or guardrail mechanisms.
However, we found that the malicious attackers could exploit false positives of
safeguards, i.e., fooling the safeguard model to block safe content mistakenly,
leading to a new denial-of-service (DoS) attack on LLMs. Specifically, by
software or phishing attacks on user client software, attackers insert a short,
seemingly innocuous adversarial prompt into to user prompt templates in
configuration files; thus, this prompt appears in final user requests without
visibility in the user interface and is not trivial to identify. By designing
an optimization process that utilizes gradient and attention information, our
attack can automatically generate seemingly safe adversarial prompts,
approximately only 30 characters long, that universally block over 97\% of user
requests on Llama Guard 3. The attack presents a new dimension of evaluating
LLM safeguards focusing on false positives, fundamentally different from the
classic jailbreak.",http://arxiv.org/pdf/2410.02916v1
Fine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation,"Xianzhi Li, Ran Zmigrod, Zhiqiang Ma, Xiaomo Liu, Xiaodan Zhu",2024-10-03,"Language models are capable of memorizing detailed patterns and information,
leading to a double-edged effect: they achieve impressive modeling performance
on downstream tasks with the stored knowledge but also raise significant
privacy concerns. Traditional differential privacy based training approaches
offer robust safeguards by employing a uniform noise distribution across all
parameters. However, this overlooks the distinct sensitivities and
contributions of individual parameters in privacy protection and often results
in suboptimal models. To address these limitations, we propose ANADP, a novel
algorithm that adaptively allocates additive noise based on the importance of
model parameters. We demonstrate that ANADP narrows the performance gap between
regular fine-tuning and traditional DP fine-tuning on a series of datasets
while maintaining the required privacy constraints.",http://arxiv.org/pdf/2410.02912v1
FakeShield: Explainable Image Forgery Detection and Localization via Multi-modal Large Language Models,"Zhipei Xu, Xuanyu Zhang, Runyi Li, Zecheng Tang, Qing Huang, Jian Zhang",2024-10-03,"The rapid development of generative AI is a double-edged sword, which not
only facilitates content creation but also makes image manipulation easier and
more difficult to detect. Although current image forgery detection and
localization (IFDL) methods are generally effective, they tend to face two
challenges: \textbf{1)} black-box nature with unknown detection principle,
\textbf{2)} limited generalization across diverse tampering methods (e.g.,
Photoshop, DeepFake, AIGC-Editing). To address these issues, we propose the
explainable IFDL task and design FakeShield, a multi-modal framework capable of
evaluating image authenticity, generating tampered region masks, and providing
a judgment basis based on pixel-level and image-level tampering clues.
Additionally, we leverage GPT-4o to enhance existing IFDL datasets, creating
the Multi-Modal Tamper Description dataSet (MMTD-Set) for training FakeShield's
tampering analysis capabilities. Meanwhile, we incorporate a Domain Tag-guided
Explainable Forgery Detection Module (DTE-FDM) and a Multi-modal Forgery
Localization Module (MFLM) to address various types of tamper detection
interpretation and achieve forgery localization guided by detailed textual
descriptions. Extensive experiments demonstrate that FakeShield effectively
detects and localizes various tampering techniques, offering an explainable and
superior solution compared to previous IFDL methods.",http://arxiv.org/pdf/2410.02761v1
Custom Non-Linear Model Predictive Control for Obstacle Avoidance in Indoor and Outdoor Environments,"Lara Laban, Mariusz Wzorek, Piotr Rudol, Tommy Persson",2024-10-03,"Navigating complex environments requires Unmanned Aerial Vehicles (UAVs) and
autonomous systems to perform trajectory tracking and obstacle avoidance in
real-time. While many control strategies have effectively utilized linear
approximations, addressing the non-linear dynamics of UAV, especially in
obstacle-dense environments, remains a key challenge that requires further
research. This paper introduces a Non-linear Model Predictive Control (NMPC)
framework for the DJI Matrice 100, addressing these challenges by using a
dynamic model and B-spline interpolation for smooth reference trajectories,
ensuring minimal deviation while respecting safety constraints. The framework
supports various trajectory types and employs a penalty-based cost function for
control accuracy in tight maneuvers. The framework utilizes CasADi for
efficient real-time optimization, enabling the UAV to maintain robust operation
even under tight computational constraints. Simulation and real-world indoor
and outdoor experiments demonstrated the NMPC ability to adapt to disturbances,
resulting in smooth, collision-free navigation.",http://arxiv.org/pdf/2410.02732v1
SteerDiff: Steering towards Safe Text-to-Image Diffusion Models,"Hongxiang Zhang, Yifeng He, Hao Chen",2024-10-03,"Text-to-image (T2I) diffusion models have drawn attention for their ability
to generate high-quality images with precise text alignment. However, these
models can also be misused to produce inappropriate content. Existing safety
measures, which typically rely on text classifiers or ControlNet-like
approaches, are often insufficient. Traditional text classifiers rely on
large-scale labeled datasets and can be easily bypassed by rephrasing. As
diffusion models continue to scale, fine-tuning these safeguards becomes
increasingly challenging and lacks flexibility. Recent red-teaming attack
researches further underscore the need for a new paradigm to prevent the
generation of inappropriate content. In this paper, we introduce SteerDiff, a
lightweight adaptor module designed to act as an intermediary between user
input and the diffusion model, ensuring that generated images adhere to ethical
and safety standards with little to no impact on usability. SteerDiff
identifies and manipulates inappropriate concepts within the text embedding
space to guide the model away from harmful outputs. We conduct extensive
experiments across various concept unlearning tasks to evaluate the
effectiveness of our approach. Furthermore, we benchmark SteerDiff against
multiple red-teaming strategies to assess its robustness. Finally, we explore
the potential of SteerDiff for concept forgetting tasks, demonstrating its
versatility in text-conditioned image generation.",http://arxiv.org/pdf/2410.02710v1
Discovering Clues of Spoofed LM Watermarks,"Thibaud Gloaguen, Nikola Jovanović, Robin Staab, Martin Vechev",2024-10-03,"LLM watermarks stand out as a promising way to attribute ownership of
LLM-generated text. One threat to watermark credibility comes from spoofing
attacks, where an unauthorized third party forges the watermark, enabling it to
falsely attribute arbitrary texts to a particular LLM. While recent works have
demonstrated that state-of-the-art schemes are in fact vulnerable to spoofing,
they lack deeper qualitative analysis of the texts produced by spoofing
methods. In this work, we for the first time reveal that there are observable
differences between genuine and spoofed watermark texts. Namely, we show that
regardless of their underlying approach, all current spoofing methods
consistently leave observable artifacts in spoofed texts, indicative of
watermark forgery. We build upon these findings to propose rigorous statistical
tests that reliably reveal the presence of such artifacts, effectively
discovering that a watermark was spoofed. Our experimental evaluation shows
high test power across all current spoofing methods, providing insights into
their fundamental limitations, and suggesting a way to mitigate this threat.",http://arxiv.org/pdf/2410.02693v1
DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life,"Yu Ying Chiu, Liwei Jiang, Yejin Choi",2024-10-03,"As we increasingly seek guidance from LLMs for decision-making in daily life,
many of these decisions are not clear-cut and depend significantly on the
personal values and ethical standards of the users. We present DailyDilemmas, a
dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma
includes two possible actions and with each action, the affected parties and
human values invoked. Based on these dilemmas, we consolidated a set of human
values across everyday topics e.g., interpersonal relationships, workplace, and
environmental issues. We evaluated LLMs on these dilemmas to determine what
action they will take and the values represented by these actions. Then, we
analyzed these values through the lens of five popular theories inspired by
sociology, psychology and philosophy. These theories are: World Value Survey,
Moral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and
Plutchik Wheel of Emotion. We find that LLMs are most aligned with the
self-expression over survival values in terms of World Value Survey, care over
loyalty in Moral Foundation Theory. Interestingly, we find large preferences
differences in models for some core values such as truthfulness e.g.,
Mixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to
select it by 9.4%. We also study the recent guidance released by OpenAI
(ModelSpec), and Anthropic (Constitutional AI) to understand how their released
principles reflect their actual value prioritization when facing nuanced moral
reasoning in daily-life settings. We find that end users cannot effectively
steer such prioritization using system prompts.",http://arxiv.org/pdf/2410.02683v1
Undesirable Memorization in Large Language Models: A Survey,"Ali Satvaty, Suzan Verberne, Fatih Turkmen",2024-10-03,"While recent research increasingly showcases the remarkable capabilities of
Large Language Models (LLMs), it's vital to confront their hidden pitfalls.
Among these challenges, the issue of memorization stands out, posing
significant ethical and legal risks. In this paper, we presents a
Systematization of Knowledge (SoK) on the topic of memorization in LLMs.
Memorization is the effect that a model tends to store and reproduce phrases or
passages from the training data and has been shown to be the fundamental issue
to various privacy and security attacks against LLMs.
  We begin by providing an overview of the literature on the memorization,
exploring it across five key dimensions: intentionality, degree,
retrievability, abstraction, and transparency. Next, we discuss the metrics and
methods used to measure memorization, followed by an analysis of the factors
that contribute to memorization phenomenon. We then examine how memorization
manifests itself in specific model architectures and explore strategies for
mitigating these effects. We conclude our overview by identifying potential
research topics for the near future: to develop methods for balancing
performance and privacy in LLMs, and the analysis of memorization in specific
contexts, including conversational agents, retrieval-augmented generation,
multilingual language models, and diffusion language models.",http://arxiv.org/pdf/2410.02650v1
Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents,"Hanrong Zhang, Jingyuan Huang, Kai Mei, Yifei Yao, Zhenting Wang, Chenlu Zhan, Hongwei Wang, Yongfeng Zhang",2024-10-03,"Although LLM-based agents, powered by Large Language Models (LLMs), can use
external tools and memory mechanisms to solve complex real-world tasks, they
may also introduce critical security vulnerabilities. However, the existing
literature does not comprehensively evaluate attacks and defenses against
LLM-based agents. To address this, we introduce Agent Security Bench (ASB), a
comprehensive framework designed to formalize, benchmark, and evaluate the
attacks and defenses of LLM-based agents, including 10 scenarios (e.g.,
e-commerce, autonomous driving, finance), 10 agents targeting the scenarios,
over 400 tools, 23 different types of attack/defense methods, and 8 evaluation
metrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory
poisoning attack, a novel Plan-of-Thought backdoor attack, a mixed attack, and
10 corresponding defenses across 13 LLM backbones with nearly 90,000 testing
cases in total. Our benchmark results reveal critical vulnerabilities in
different stages of agent operation, including system prompt, user prompt
handling, tool usage, and memory retrieval, with the highest average attack
success rate of 84.30\%, but limited effectiveness shown in current defenses,
unveiling important works to be done in terms of agent security for the
community. Our code can be found at https://github.com/agiresearch/ASB.",http://arxiv.org/pdf/2410.02644v1
Personalized Quantum Federated Learning for Privacy Image Classification,"Jinjing Shi, Tian Chen, Shichao Zhang, Xuelong Li",2024-10-03,"Quantum federated learning has brought about the improvement of privacy image
classification, while the lack of personality of the client model may
contribute to the suboptimal of quantum federated learning. A personalized
quantum federated learning algorithm for privacy image classification is
proposed to enhance the personality of the client model in the case of an
imbalanced distribution of images. First, a personalized quantum federated
learning model is constructed, in which a personalized layer is set for the
client model to maintain the personalized parameters. Second, a personalized
quantum federated learning algorithm is introduced to secure the information
exchanged between the client and server.Third, the personalized federated
learning is applied to image classification on the FashionMNIST dataset, and
the experimental results indicate that the personalized quantum federated
learning algorithm can obtain global and local models with excellent
performance, even in situations where local training samples are imbalanced.
The server's accuracy is 100% with 8 clients and a distribution parameter of
100, outperforming the non-personalized model by 7%. The average client
accuracy is 2.9% higher than that of the non-personalized model with 2 clients
and a distribution parameter of 1. Compared to previous quantum federated
learning algorithms, the proposed personalized quantum federated learning
algorithm eliminates the need for additional local training while safeguarding
both model and data privacy.It may facilitate broader adoption and application
of quantum technologies, and pave the way for more secure, scalable, and
efficient quantum distribute machine learning solutions.",http://arxiv.org/pdf/2410.02547v1
Strong Preferences Affect the Robustness of Value Alignment,"Ziwei Xu, Mohan Kankanhalli",2024-10-03,"Value alignment, which aims to ensure that large language models (LLMs) and
other AI agents behave in accordance with human values, is critical for
ensuring safety and trustworthiness of these systems. A key component of value
alignment is the modeling of human preferences as a representation of human
values. In this paper, we investigate the robustness of value alignment by
examining the sensitivity of preference models. Specifically, we ask: how do
changes in the probabilities of some preferences affect the predictions of
these models for other preferences? To answer this question, we theoretically
analyze the robustness of widely used preference models by examining their
sensitivities to minor changes in preferences they model. Our findings reveal
that, in the Bradley-Terry and the Placket-Luce model, the probability of a
preference can change significantly as other preferences change, especially
when these preferences are dominant (i.e., with probabilities near 0 or 1). We
identify specific conditions where this sensitivity becomes significant for
these models and discuss the practical implications for the robustness and
safety of value alignment in AI systems.",http://arxiv.org/pdf/2410.02451v1
Clinnova Federated Learning Proof of Concept: Key Takeaways from a Cross-border Collaboration,"Julia Alekseenko, Bram Stieltjes, Michael Bach, Melanie Boerries, Oliver Opitz, Alexandros Karargyris, Nicolas Padoy",2024-10-03,"Clinnova, a collaborative initiative involving France, Germany, Switzerland,
and Luxembourg, is dedicated to unlocking the power of precision medicine
through data federation, standardization, and interoperability. This European
Greater Region initiative seeks to create an interoperable European standard
using artificial intelligence (AI) and data science to enhance healthcare
outcomes and efficiency. Key components include multidisciplinary research
centers, a federated biobanking strategy, a digital health innovation platform,
and a federated AI strategy. It targets inflammatory bowel disease, rheumatoid
diseases, and multiple sclerosis (MS), emphasizing data quality to develop AI
algorithms for personalized treatment and translational research.
  The IHU Strasbourg (Institute of Minimal-invasive Surgery) has the lead in
this initiative to develop the federated learning (FL) proof of concept (POC)
that will serve as a foundation for advancing AI in healthcare. At its core,
Clinnova-MS aims to enhance MS patient care by using FL to develop more
accurate models that detect disease progression, guide interventions, and
validate digital biomarkers across multiple sites. This technical report
presents insights and key takeaways from the first cross-border federated POC
on MS segmentation of MRI images within the Clinnova framework. While our work
marks a significant milestone in advancing MS segmentation through cross-border
collaboration, it also underscores the importance of addressing technical,
logistical, and ethical considerations to realize the full potential of FL in
healthcare settings.",http://arxiv.org/pdf/2410.02443v1
Optimizing Adaptive Attacks against Content Watermarks for Language Models,"Abdulrahman Diaa, Toluwani Aremu, Nils Lukas",2024-10-03,"Large Language Models (LLMs) can be \emph{misused} to spread online spam and
misinformation. Content watermarking deters misuse by hiding a message in
model-generated outputs, enabling their detection using a secret watermarking
key. Robustness is a core security property, stating that evading detection
requires (significant) degradation of the content's quality. Many LLM
watermarking methods have been proposed, but robustness is tested only against
\emph{non-adaptive} attackers who lack knowledge of the watermarking method and
can find only suboptimal attacks. We formulate the robustness of LLM
watermarking as an objective function and propose preference-based optimization
to tune \emph{adaptive} attacks against the specific watermarking method. Our
evaluation shows that (i) adaptive attacks substantially outperform
non-adaptive baselines. (ii) Even in a non-adaptive setting, adaptive attacks
optimized against a few known watermarks remain highly effective when tested
against other unseen watermarks, and (iii) optimization-based attacks are
practical and require less than seven GPU hours. Our findings underscore the
need to test robustness against adaptive attackers.",http://arxiv.org/pdf/2410.02440v1
PFGuard: A Generative Framework with Privacy and Fairness Safeguards,"Soyeon Kim, Yuji Roh, Geon Heo, Steven Euijong Whang",2024-10-03,"Generative models must ensure both privacy and fairness for Trustworthy AI.
While these goals have been pursued separately, recent studies propose to
combine existing privacy and fairness techniques to achieve both goals.
However, naively combining these techniques can be insufficient due to
privacy-fairness conflicts, where a sample in a minority group may be amplified
for fairness, only to be suppressed for privacy. We demonstrate how these
conflicts lead to adverse effects, such as privacy violations and unexpected
fairness-utility tradeoffs. To mitigate these risks, we propose PFGuard, a
generative framework with privacy and fairness safeguards, which simultaneously
addresses privacy, fairness, and utility. By using an ensemble of multiple
teacher models, PFGuard balances privacy-fairness conflicts between fair and
private training stages and achieves high utility based on ensemble learning.
Extensive experiments show that PFGuard successfully generates synthetic data
on high-dimensional data while providing both fairness convergence and strict
DP guarantees - the first of its kind to our knowledge.",http://arxiv.org/pdf/2410.02246v1
Buckle Up: Robustifying LLMs at Every Customization Stage via Data Curation,"Xiaoqun Liu, Jiacheng Liang, Luoxi Tang, Chenyu You, Muchao Ye, Zhaohan Xi",2024-10-03,"Large language models (LLMs) are extensively adapted for downstream
applications through a process known as ""customization,"" with fine-tuning being
a common method for integrating domain-specific expertise. However, recent
studies have revealed a vulnerability that tuning LLMs with malicious samples
can compromise their robustness and amplify harmful content, an attack known as
""jailbreaking."" To mitigate such attack, we propose an effective defensive
framework utilizing data curation to revise commonsense texts and enhance their
safety implication from the perspective of LLMs. The curated texts can mitigate
jailbreaking attacks at every stage of the customization process: before
customization to immunize LLMs against future jailbreak attempts, during
customization to neutralize jailbreaking risks, or after customization to
restore the compromised models. Since the curated data strengthens LLMs through
the standard fine-tuning workflow, we do not introduce additional modules
during LLM inference, thereby preserving the original customization process.
Experimental results demonstrate a substantial reduction in jailbreaking
effects, with up to a 100% success in generating responsible responses.
Notably, our method is effective even with commonsense texts, which are often
more readily available than safety-relevant data. With the every-stage
defensive framework and supporting experimental performance, this work
represents a significant advancement in mitigating jailbreaking risks and
ensuring the secure customization of LLMs.",http://arxiv.org/pdf/2410.02220v2
G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models,"Zhaoning Yu, Xiangyang Xu, Hongyang Gao",2024-10-03,"We introduce G2T-LLM, a novel approach for molecule generation that uses
graph-to-tree text encoding to transform graph-based molecular structures into
a hierarchical text format optimized for large language models (LLMs). This
encoding converts complex molecular graphs into tree-structured formats, such
as JSON and XML, which LLMs are particularly adept at processing due to their
extensive pre-training on these types of data. By leveraging the flexibility of
LLMs, our approach allows for intuitive interaction using natural language
prompts, providing a more accessible interface for molecular design. Through
supervised fine-tuning, G2T-LLM generates valid and coherent chemical
structures, addressing common challenges like invalid outputs seen in
traditional graph-based methods. While LLMs are computationally intensive, they
offer superior generalization and adaptability, enabling the generation of
diverse molecular structures with minimal task-specific customization. The
proposed approach achieved comparable performances with state-of-the-art
methods on various benchmark molecular generation datasets, demonstrating its
potential as a flexible and innovative tool for AI-driven molecular design.",http://arxiv.org/pdf/2410.02198v1
BACKTIME: Backdoor Attacks on Multivariate Time Series Forecasting,"Xiao Lin, Zhining Liu, Dongqi Fu, Ruizhong Qiu, Hanghang Tong",2024-10-03,"Multivariate Time Series (MTS) forecasting is a fundamental task with
numerous real-world applications, such as transportation, climate, and
epidemiology. While a myriad of powerful deep learning models have been
developed for this task, few works have explored the robustness of MTS
forecasting models to malicious attacks, which is crucial for their trustworthy
employment in high-stake scenarios. To address this gap, we dive deep into the
backdoor attacks on MTS forecasting models and propose an effective attack
method named BackTime.By subtly injecting a few stealthy triggers into the MTS
data, BackTime can alter the predictions of the forecasting model according to
the attacker's intent. Specifically, BackTime first identifies vulnerable
timestamps in the data for poisoning, and then adaptively synthesizes stealthy
and effective triggers by solving a bi-level optimization problem with a
GNN-based trigger generator. Extensive experiments across multiple datasets and
state-of-the-art MTS forecasting models demonstrate the effectiveness,
versatility, and stealthiness of \method{} attacks. The code is available at
\url{https://github.com/xiaolin-cs/BackTime}.",http://arxiv.org/pdf/2410.02195v1
"A Survey on Point-of-Interest Recommendation: Models, Architectures, and Security","Qianru Zhang, Peng Yang, Junliang Yu, Haixin Wang, Xingwei He, Siu-Ming Yiu, Hongzhi Yin",2024-10-03,"The widespread adoption of smartphones and Location-Based Social Networks has
led to a massive influx of spatio-temporal data, creating unparalleled
opportunities for enhancing Point-of-Interest (POI) recommendation systems.
These advanced POI systems are crucial for enriching user experiences, enabling
personalized interactions, and optimizing decision-making processes in the
digital landscape. However, existing surveys tend to focus on traditional
approaches and few of them delve into cutting-edge developments, emerging
architectures, as well as security considerations in POI recommendations. To
address this gap, our survey stands out by offering a comprehensive, up-to-date
review of POI recommendation systems, covering advancements in models,
architectures, and security aspects. We systematically examine the transition
from traditional models to advanced techniques such as large language models.
Additionally, we explore the architectural evolution from centralized to
decentralized and federated learning systems, highlighting the improvements in
scalability and privacy. Furthermore, we address the increasing importance of
security, examining potential vulnerabilities and privacy-preserving
approaches. Our taxonomy provides a structured overview of the current state of
POI recommendation, while we also identify promising directions for future
research in this rapidly advancing field.",http://arxiv.org/pdf/2410.02191v1
Planning in Strawberry Fields: Evaluating and Improving the Planning and Scheduling Capabilities of LRM o1,"Karthik Valmeekam, Kaya Stechly, Atharva Gundawar, Subbarao Kambhampati",2024-10-03,"The ability to plan a course of action that achieves a desired state of
affairs has long been considered a core competence of intelligent agents and
has been an integral part of AI research since its inception. With the advent
of large language models (LLMs), there has been considerable interest in the
question of whether or not they possess such planning abilities, but -- despite
the slew of new private and open source LLMs since GPT3 -- progress has
remained slow. OpenAI claims that their recent o1 (Strawberry) model has been
specifically constructed and trained to escape the normal limitations of
autoregressive LLMs -- making it a new kind of model: a Large Reasoning Model
(LRM). In this paper, we evaluate the planning capabilities of two LRMs
(o1-preview and o1-mini) on both planning and scheduling benchmarks. We see
that while o1 does seem to offer significant improvements over autoregressive
LLMs, this comes at a steep inference cost, while still failing to provide any
guarantees over what it generates. We also show that combining o1 models with
external verifiers -- in a so-called LRM-Modulo system -- guarantees the
correctness of the combined system's output while further improving
performance.",http://arxiv.org/pdf/2410.02162v1
RiskSEA : A Scalable Graph Embedding for Detecting On-chain Fraudulent Activities on the Ethereum Blockchain,"Ayush Agarwal, Lv Lu, Arjun Maheswaran, Varsha Mahadevan, Bhaskar Krishnamachari",2024-10-03,"Like any other useful technology, cryptocurrencies are sometimes used for
criminal activities. While transactions are recorded on the blockchain, there
exists a need for a more rapid and scalable method to detect addresses
associated with fraudulent activities. We present RiskSEA, a scalable risk
scoring system capable of effectively handling the dynamic nature of
large-scale blockchain transaction graphs. The risk scoring system, which we
implement for Ethereum, consists of 1. a scalable approach to generating
node2vec embedding for entire set of addresses to capture the graph topology 2.
transaction-based features to capture the transactional behavioral pattern of
an address 3. a classifier model to generate risk score for addresses that
combines the node2vec embedding and behavioral features. Efficiently generating
node2vec embedding for large scale and dynamically evolving blockchain
transaction graphs is challenging, we present two novel approaches for
generating node2vec embeddings and effectively scaling it to the entire set of
blockchain addresses: 1. node2vec embedding propagation and 2. dynamic node2vec
embedding. We present a comprehensive analysis of the proposed approaches. Our
experiments show that combining both behavioral and node2vec features boosts
the classification performance significantly, and that the dynamic node2vec
embeddings perform better than the node2vec propagated embeddings.",http://arxiv.org/pdf/2410.02160v1
"The why, what, and how of AI-based coding in scientific research","Tonghe Zhuang, Zhicheng Lin",2024-10-03,"Computer programming (coding) is indispensable for researchers across
disciplines, yet it remains challenging to learn and time-consuming to carry
out. Generative AI, particularly large language models (LLMs), has the
potential to transform coding into intuitive conversations, but best practices
and effective workflows are only emerging. We dissect AI-based coding through
three key lenses: the nature and role of LLMs in coding (why), six types of
coding assistance they provide (what), and a five-step workflow in action with
practical implementation strategies (how). Additionally, we address the
limitations and future outlook of AI in coding. By offering actionable
insights, this framework helps to guide researchers in effectively leveraging
AI to enhance coding practices and education, accelerating scientific progress.",http://arxiv.org/pdf/2410.02156v1
The Impact of Generative AI on Collaborative Open-Source Software Development: Evidence from GitHub Copilot,"Fangchen Song, Ashish Agarwal, Wen Wen",2024-10-02,"Generative artificial intelligence (AI) has opened the possibility of
automated content production, including coding in software development, which
can significantly influence the participation and performance of software
developers. To explore this impact, we investigate the role of GitHub Copilot,
a generative AI pair programmer, on software development in open-source
community, where multiple developers voluntarily collaborate on software
projects. Using GitHub's dataset for open-source repositories and a generalized
synthetic control method, we find that Copilot significantly enhances
project-level productivity by 6.5%. Delving deeper, we dissect the key
mechanisms driving this improvement. Our findings reveal a 5.5% increase in
individual productivity and a 5.4% increase in participation. However, this is
accompanied with a 41.6% increase in integration time, potentially due to
higher coordination costs. Interestingly, we also observe the differential
effects among developers. We discover that core developers achieve greater
project-level productivity gains from using Copilot, benefiting more in terms
of individual productivity and participation compared to peripheral developers,
plausibly due to their deeper familiarity with software projects. We also find
that the increase in project-level productivity is accompanied with no change
in code quality. We conclude that AI pair programmers bring benefits to
developers to automate and augment their code, but human developers' knowledge
of software projects can enhance the benefits. In summary, our research
underscores the role of AI pair programmers in impacting project-level
productivity within the open-source community and suggests potential
implications for the structure of open-source software projects.",http://arxiv.org/pdf/2410.02091v1
EAB-FL: Exacerbating Algorithmic Bias through Model Poisoning Attacks in Federated Learning,"Syed Irfan Ali Meerza, Jian Liu",2024-10-02,"Federated Learning (FL) is a technique that allows multiple parties to train
a shared model collaboratively without disclosing their private data. It has
become increasingly popular due to its distinct privacy advantages. However, FL
models can suffer from biases against certain demographic groups (e.g., racial
and gender groups) due to the heterogeneity of data and party selection.
Researchers have proposed various strategies for characterizing the group
fairness of FL algorithms to address this issue. However, the effectiveness of
these strategies in the face of deliberate adversarial attacks has not been
fully explored. Although existing studies have revealed various threats (e.g.,
model poisoning attacks) against FL systems caused by malicious participants,
their primary aim is to decrease model accuracy, while the potential of
leveraging poisonous model updates to exacerbate model unfairness remains
unexplored. In this paper, we propose a new type of model poisoning attack,
EAB-FL, with a focus on exacerbating group unfairness while maintaining a good
level of model utility. Extensive experiments on three datasets demonstrate the
effectiveness and efficiency of our attack, even with state-of-the-art fairness
optimization algorithms and secure aggregation rules employed.",http://arxiv.org/pdf/2410.02042v1
Zodiac: A Cardiologist-Level LLM Framework for Multi-Agent Diagnostics,"Yuan Zhou, Peng Zhang, Mengya Song, Alice Zheng, Yiwen Lu, Zhiheng Liu, Yong Chen, Zhaohan Xi",2024-10-02,"Large language models (LLMs) have demonstrated remarkable progress in
healthcare. However, a significant gap remains regarding LLMs' professionalism
in domain-specific clinical practices, limiting their application in real-world
diagnostics. In this work, we introduce ZODIAC, an LLM-powered framework with
cardiologist-level professionalism designed to engage LLMs in cardiological
diagnostics. ZODIAC assists cardiologists by extracting clinically relevant
characteristics from patient data, detecting significant arrhythmias, and
generating preliminary reports for the review and refinement by cardiologists.
To achieve cardiologist-level professionalism, ZODIAC is built on a multi-agent
collaboration framework, enabling the processing of patient data across
multiple modalities. Each LLM agent is fine-tuned using real-world patient data
adjudicated by cardiologists, reinforcing the model's professionalism. ZODIAC
undergoes rigorous clinical validation with independent cardiologists,
evaluated across eight metrics that measure clinical effectiveness and address
security concerns. Results show that ZODIAC outperforms industry-leading
models, including OpenAI's GPT-4o, Meta's Llama-3.1-405B, and Google's
Gemini-pro, as well as medical-specialist LLMs like Microsoft's BioGPT. ZODIAC
demonstrates the transformative potential of specialized LLMs in healthcare by
delivering domain-specific solutions that meet the stringent demands of medical
practice. Notably, ZODIAC has been successfully integrated into
electrocardiography (ECG) devices, exemplifying the growing trend of embedding
LLMs into Software-as-Medical-Device (SaMD).",http://arxiv.org/pdf/2410.02026v1
Risk Alignment in Agentic AI Systems,"Hayley Clatterbuck, Clinton Castro, Arvo Muñoz Morán",2024-10-02,"Agentic AIs $-$ AIs that are capable and permitted to undertake complex
actions with little supervision $-$ mark a new frontier in AI capabilities and
raise new questions about how to safely create and align such systems with
users, developers, and society. Because agents' actions are influenced by their
attitudes toward risk, one key aspect of alignment concerns the risk profiles
of agentic AIs. Risk alignment will matter for user satisfaction and trust, but
it will also have important ramifications for society more broadly, especially
as agentic AIs become more autonomous and are allowed to control key aspects of
our lives. AIs with reckless attitudes toward risk (either because they are
calibrated to reckless human users or are poorly designed) may pose significant
threats. They might also open 'responsibility gaps' in which there is no agent
who can be held accountable for harmful actions. What risk attitudes should
guide an agentic AI's decision-making? How might we design AI systems that are
calibrated to the risk attitudes of their users? What guardrails, if any,
should be placed on the range of permissible risk attitudes? What are the
ethical considerations involved when designing systems that make risky
decisions on behalf of others? We present three papers that bear on key
normative and technical aspects of these questions.",http://arxiv.org/pdf/2410.01927v1
Social Media Authentication and Combating Deepfakes using Semi-fragile Invisible Image Watermarking,"Aakash Varma Nadimpalli, Ajita Rattani",2024-10-02,"With the significant advances in deep generative models for image and video
synthesis, Deepfakes and manipulated media have raised severe societal
concerns. Conventional machine learning classifiers for deepfake detection
often fail to cope with evolving deepfake generation technology and are
susceptible to adversarial attacks. Alternatively, invisible image watermarking
is being researched as a proactive defense technique that allows media
authentication by verifying an invisible secret message embedded in the image
pixels. A handful of invisible image watermarking techniques introduced for
media authentication have proven vulnerable to basic image processing
operations and watermark removal attacks. In response, we have proposed a
semi-fragile image watermarking technique that embeds an invisible secret
message into real images for media authentication. Our proposed watermarking
framework is designed to be fragile to facial manipulations or tampering while
being robust to benign image-processing operations and watermark removal
attacks. This is facilitated through a unique architecture of our proposed
technique consisting of critic and adversarial networks that enforce high image
quality and resiliency to watermark removal efforts, respectively, along with
the backbone encoder-decoder and the discriminator networks. Thorough
experimental investigations on SOTA facial Deepfake datasets demonstrate that
our proposed model can embed a $64$-bit secret as an imperceptible image
watermark that can be recovered with a high-bit recovery accuracy when benign
image processing operations are applied while being non-recoverable when unseen
Deepfake manipulations are applied. In addition, our proposed watermarking
technique demonstrates high resilience to several white-box and black-box
watermark removal attacks. Thus, obtaining state-of-the-art performance.",http://arxiv.org/pdf/2410.01906v1
The potential of LLM-generated reports in DevSecOps,"Nikolaos Lykousas, Vasileios Argyropoulos, Fran Casino",2024-10-02,"Alert fatigue is a common issue faced by software teams using the DevSecOps
paradigm. The overwhelming number of warnings and alerts generated by security
and code scanning tools, particularly in smaller teams where resources are
limited, leads to desensitization and diminished responsiveness to security
warnings, potentially exposing systems to vulnerabilities. This paper explores
the potential of LLMs in generating actionable security reports that emphasize
the financial impact and consequences of detected security issues, such as
credential leaks, if they remain unaddressed. A survey conducted among
developers indicates that LLM-generated reports significantly enhance the
likelihood of immediate action on security issues by providing clear,
comprehensive, and motivating insights. Integrating these reports into
DevSecOps workflows can mitigate attention saturation and alert fatigue,
ensuring that critical security warnings are addressed effectively.",http://arxiv.org/pdf/2410.01899v1
Auction-Based Regulation for Artificial Intelligence,"Marco Bornstein, Zora Che, Suhas Julapalli, Abdirisak Mohamed, Amrit Singh Bedi, Furong Huang",2024-10-02,"In an era of ""moving fast and breaking things"", regulators have moved slowly
to pick up the safety, bias, and legal pieces left in the wake of broken
Artificial Intelligence (AI) deployment. Since AI models, such as large
language models, are able to push misinformation and stoke division within our
society, it is imperative for regulators to employ a framework that mitigates
these dangers and ensures user safety. While there is much-warranted discussion
about how to address the safety, bias, and legal woes of state-of-the-art AI
models, the number of rigorous and realistic mathematical frameworks to
regulate AI safety is lacking. We take on this challenge, proposing an
auction-based regulatory mechanism that provably incentivizes model-building
agents (i) to deploy safer models and (ii) to participate in the regulation
process. We provably guarantee, via derived Nash Equilibria, that each
participating agent's best strategy is to submit a model safer than a
prescribed minimum-safety threshold. Empirical results show that our regulatory
auction boosts safety and participation rates by 20% and 15% respectively,
outperforming simple regulatory frameworks that merely enforce minimum safety
standards.",http://arxiv.org/pdf/2410.01871v1
DreamGarden: A Designer Assistant for Growing Games from a Single Prompt,"Sam Earle, Samyak Parajuli, Andrzej Banburski-Fahey",2024-10-02,"Coding assistants are increasingly leveraged in game design, both generating
code and making high-level plans. To what degree can these tools align with
developer workflows, and what new modes of human-computer interaction can
emerge from their use? We present DreamGarden, an AI system capable of
assisting with the development of diverse game environments in Unreal Engine.
At the core of our method is an LLM-driven planner, capable of breaking down a
single, high-level prompt -- a dream, memory, or imagined scenario provided by
a human user -- into a hierarchical action plan, which is then distributed
across specialized submodules facilitating concrete implementation. This system
is presented to the user as a garden of plans and actions, both growing
independently and responding to user intervention via seed prompts, pruning,
and feedback. Through a user study, we explore design implications of this
system, charting courses for future work in semi-autonomous assistants and
open-ended simulation design.",http://arxiv.org/pdf/2410.01791v1
From Prohibition to Adoption: How Hong Kong Universities Are Navigating ChatGPT in Academic Workflows,"Junjun Huang, Jifan Wu, Qing Wang, Kemeng Yuan, Jiefeng Li, Di Lu",2024-10-02,"This paper aims at comparing the time when Hong Kong universities used to ban
ChatGPT to the current periods where it has become integrated in the academic
processes. Bolted by concerns of integrity and ethical issues in technologies,
institutions have adapted by moving towards the center adopting AI literacy and
responsibility policies. This study examines new paradigms which have been
developed to help implement these positives while preventing negative effects
on academia. Keywords: ChatGPT, Academic Integrity, AI Literacy, Ethical AI
Use, Generative AI in Education, University Policy, AI Integration in Academia,
Higher Education and Technology",http://arxiv.org/pdf/2410.01695v1
Why context matters in VQA and Reasoning: Semantic interventions for VLM input modalities,"Kenza Amara, Lukas Klein, Carsten Lüth, Paul Jäger, Hendrik Strobelt, Mennatallah El-Assady",2024-10-02,"The various limitations of Generative AI, such as hallucinations and model
failures, have made it crucial to understand the role of different modalities
in Visual Language Model (VLM) predictions. Our work investigates how the
integration of information from image and text modalities influences the
performance and behavior of VLMs in visual question answering (VQA) and
reasoning tasks. We measure this effect through answer accuracy, reasoning
quality, model uncertainty, and modality relevance. We study the interplay
between text and image modalities in different configurations where visual
content is essential for solving the VQA task. Our contributions include (1)
the Semantic Interventions (SI)-VQA dataset, (2) a benchmark study of various
VLM architectures under different modality configurations, and (3) the
Interactive Semantic Interventions (ISI) tool. The SI-VQA dataset serves as the
foundation for the benchmark, while the ISI tool provides an interface to test
and apply semantic interventions in image and text inputs, enabling more
fine-grained analysis. Our results show that complementary information between
modalities improves answer and reasoning quality, while contradictory
information harms model performance and confidence. Image text annotations have
minimal impact on accuracy and uncertainty, slightly increasing image
relevance. Attention analysis confirms the dominant role of image inputs over
text in VQA tasks. In this study, we evaluate state-of-the-art VLMs that allow
us to extract attention coefficients for each modality. A key finding is
PaliGemma's harmful overconfidence, which poses a higher risk of silent
failures compared to the LLaVA models. This work sets the foundation for
rigorous analysis of modality integration, supported by datasets specifically
designed for this purpose.",http://arxiv.org/pdf/2410.01690v1
Trying to be human: Linguistic traces of stochastic empathy in language models,"Bennett Kleinberg, Jari Zegers, Jonas Festor, Stefana Vida, Julian Präsent, Riccardo Loconte, Sanne Peereboom",2024-10-02,"Differentiating between generated and human-written content is important for
navigating the modern world. Large language models (LLMs) are crucial drivers
behind the increased quality of computer-generated content. Reportedly, humans
find it increasingly difficult to identify whether an AI model generated a
piece of text. Our work tests how two important factors contribute to the human
vs AI race: empathy and an incentive to appear human. We address both aspects
in two experiments: human participants and a state-of-the-art LLM wrote
relationship advice (Study 1, n=530) or mere descriptions (Study 2, n=610),
either instructed to be as human as possible or not. New samples of humans
(n=428 and n=408) then judged the texts' source. Our findings show that when
empathy is required, humans excel. Contrary to expectations, instructions to
appear human were only effective for the LLM, so the human advantage
diminished. Computational text analysis revealed that LLMs become more human
because they may have an implicit representation of what makes a text human and
effortlessly apply these heuristics. The model resorts to a conversational,
self-referential, informal tone with a simpler vocabulary to mimic stochastic
empathy. We discuss these findings in light of recent claims on the on-par
performance of LLMs.",http://arxiv.org/pdf/2410.01675v1
Conformal Generative Modeling with Improved Sample Efficiency through Sequential Greedy Filtering,"Klaus-Rudolf Kladny, Bernhard Schölkopf, Michael Muehlebach",2024-10-02,"Generative models lack rigorous statistical guarantees for their outputs and
are therefore unreliable in safety-critical applications. In this work, we
propose Sequential Conformal Prediction for Generative Models (SCOPE-Gen), a
sequential conformal prediction method producing prediction sets that satisfy a
rigorous statistical guarantee called conformal admissibility control. This
guarantee states that with high probability, the prediction sets contain at
least one admissible (or valid) example. To this end, our method first samples
an initial set of i.i.d. examples from a black box generative model. Then, this
set is iteratively pruned via so-called greedy filters. As a consequence of the
iterative generation procedure, admissibility of the final prediction set
factorizes as a Markov chain. This factorization is crucial, because it allows
to control each factor separately, using conformal prediction. In comparison to
prior work, our method demonstrates a large reduction in the number of
admissibility evaluations during calibration. This reduction is important in
safety-critical applications, where these evaluations must be conducted
manually by domain experts and are therefore costly and time consuming. We
highlight the advantages of our method in terms of admissibility evaluations
and cardinality of the prediction sets through experiments in natural language
generation and molecular graph extension tasks.",http://arxiv.org/pdf/2410.01660v1
Entropy-Based Uncertainty Modeling for Trajectory Prediction in Autonomous Driving,"Aron Distelzweig, Andreas Look, Eitan Kosman, Faris Janjoš, Jörg Wagner, Abhinav Valadaa",2024-10-02,"In autonomous driving, accurate motion prediction is essential for safe and
efficient motion planning. To ensure safety, planners must rely on reliable
uncertainty information about the predicted future behavior of surrounding
agents, yet this aspect has received limited attention. This paper addresses
the so-far neglected problem of uncertainty modeling in trajectory prediction.
We adopt a holistic approach that focuses on uncertainty quantification,
decomposition, and the influence of model composition. Our method is based on a
theoretically grounded information-theoretic approach to measure uncertainty,
allowing us to decompose total uncertainty into its aleatoric and epistemic
components. We conduct extensive experiments on the nuScenes dataset to assess
how different model architectures and configurations affect uncertainty
quantification and model robustness.",http://arxiv.org/pdf/2410.01628v1
Automated Red Teaming with GOAT: the Generative Offensive Agent Tester,"Maya Pavlova, Erik Brinkman, Krithika Iyer, Vitor Albiero, Joanna Bitton, Hailey Nguyen, Joe Li, Cristian Canton Ferrer, Ivan Evtimov, Aaron Grattafiori",2024-10-02,"Red teaming assesses how large language models (LLMs) can produce content
that violates norms, policies, and rules set during their safety training.
However, most existing automated methods in the literature are not
representative of the way humans tend to interact with AI models. Common users
of AI models may not have advanced knowledge of adversarial machine learning
methods or access to model internals, and they do not spend a lot of time
crafting a single highly effective adversarial prompt. Instead, they are likely
to make use of techniques commonly shared online and exploit the multiturn
conversational nature of LLMs. While manual testing addresses this gap, it is
an inefficient and often expensive process. To address these limitations, we
introduce the Generative Offensive Agent Tester (GOAT), an automated agentic
red teaming system that simulates plain language adversarial conversations
while leveraging multiple adversarial prompting techniques to identify
vulnerabilities in LLMs. We instantiate GOAT with 7 red teaming attacks by
prompting a general-purpose model in a way that encourages reasoning through
the choices of methods available, the current target model's response, and the
next steps. Our approach is designed to be extensible and efficient, allowing
human testers to focus on exploring new areas of risk while automation covers
the scaled adversarial stress-testing of known risk territory. We present the
design and evaluation of GOAT, demonstrating its effectiveness in identifying
vulnerabilities in state-of-the-art LLMs, with an ASR@10 of 97% against Llama
3.1 and 88% against GPT-4 on the JailbreakBench dataset.",http://arxiv.org/pdf/2410.01606v1
OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data,"Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, Igor Gitman",2024-10-02,"Mathematical reasoning continues to be a critical challenge in large language
model (LLM) development with significant interest. However, most of the
cutting-edge progress in mathematical reasoning with LLMs has become
\emph{closed-source} due to lack of access to training data. This lack of data
access limits researchers from understanding the impact of different choices
for synthesizing and utilizing the data. With the goal of creating a
high-quality finetuning (SFT) dataset for math reasoning, we conduct careful
ablation experiments on data synthesis using the recently released
\texttt{Llama3.1} family of models. Our experiments show that: (a) solution
format matters, with excessively verbose solutions proving detrimental to SFT
performance, (b) data generated by a strong teacher outperforms
\emph{on-policy} data generated by a weak student model, (c) SFT is robust to
low-quality solutions, allowing for imprecise data filtering, and (d) question
diversity is crucial for achieving data scaling gains. Based on these insights,
we create the OpenMathInstruct-2 dataset, which consists of 14M
question-solution pairs ($\approx$ 600K unique questions), making it nearly
eight times larger than the previous largest open-source math reasoning
dataset. Finetuning the \texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2
outperforms \texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\% (51.9\%
$\rightarrow$ 67.8\%). Finally, to accelerate the open-source efforts, we
release the code, the finetuned models, and the OpenMathInstruct-2 dataset
under a commercially permissive license.",http://arxiv.org/pdf/2410.01560v1
MedQA-CS: Benchmarking Large Language Models Clinical Skills Using an AI-SCE Framework,"Zonghai Yao, Zihao Zhang, Chaolong Tang, Xingyu Bian, Youxia Zhao, Zhichao Yang, Junda Wang, Huixue Zhou, Won Seok Jang, Feiyun Ouyang, Hong Yu",2024-10-02,"Artificial intelligence (AI) and large language models (LLMs) in healthcare
require advanced clinical skills (CS), yet current benchmarks fail to evaluate
these comprehensively. We introduce MedQA-CS, an AI-SCE framework inspired by
medical education's Objective Structured Clinical Examinations (OSCEs), to
address this gap. MedQA-CS evaluates LLMs through two instruction-following
tasks, LLM-as-medical-student and LLM-as-CS-examiner, designed to reflect real
clinical scenarios. Our contributions include developing MedQA-CS, a
comprehensive evaluation framework with publicly available data and expert
annotations, and providing the quantitative and qualitative assessment of LLMs
as reliable judges in CS evaluation. Our experiments show that MedQA-CS is a
more challenging benchmark for evaluating clinical skills than traditional
multiple-choice QA benchmarks (e.g., MedQA). Combined with existing benchmarks,
MedQA-CS enables a more comprehensive evaluation of LLMs' clinical capabilities
for both open- and closed-source LLMs.",http://arxiv.org/pdf/2410.01553v1
Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models,"Angela Lopez-Cardona, Carlos Segura, Alexandros Karatzoglou, Sergi Abadal, Ioannis Arapakis",2024-10-02,"Advancements in Natural Language Processing (NLP), have led to the emergence
of Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which
excel across a range of tasks but require extensive fine-tuning to align their
outputs with human expectations. A widely used method for achieving this
alignment is Reinforcement Learning from Human Feedback (RLHF), which, despite
its success, faces challenges in accurately modelling human preferences. In
this paper, we introduce GazeReward, a novel framework that integrates implicit
feedback -- and specifically eye-tracking (ET) data -- into the Reward Model
(RM). In addition, we explore how ET-based features can provide insights into
user preferences. Through ablation studies we test our framework with different
integration methods, LLMs, and ET generator models, demonstrating that our
approach significantly improves the accuracy of the RM on established human
preference datasets. This work advances the ongoing discussion on optimizing AI
alignment with human values, exploring the potential of cognitive data for
shaping future NLP research.",http://arxiv.org/pdf/2410.01532v1
One Wave to Explain Them All: A Unifying Perspective on Post-hoc Explainability,"Gabriel Kasmi, Amandine Brunetto, Thomas Fel, Jayneel Parekh",2024-10-02,"Despite the growing use of deep neural networks in safety-critical
decision-making, their inherent black-box nature hinders transparency and
interpretability. Explainable AI (XAI) methods have thus emerged to understand
a model's internal workings, and notably attribution methods also called
saliency maps. Conventional attribution methods typically identify the
locations -- the where -- of significant regions within an input. However,
because they overlook the inherent structure of the input data, these methods
often fail to interpret what these regions represent in terms of structural
components (e.g., textures in images or transients in sounds). Furthermore,
existing methods are usually tailored to a single data modality, limiting their
generalizability. In this paper, we propose leveraging the wavelet domain as a
robust mathematical foundation for attribution. Our approach, the Wavelet
Attribution Method (WAM) extends the existing gradient-based feature
attributions into the wavelet domain, providing a unified framework for
explaining classifiers across images, audio, and 3D shapes. Empirical
evaluations demonstrate that WAM matches or surpasses state-of-the-art methods
across faithfulness metrics and models in image, audio, and 3D explainability.
Finally, we show how our method explains not only the where -- the important
parts of the input -- but also the what -- the relevant patterns in terms of
structural components.",http://arxiv.org/pdf/2410.01482v1
SonicSim: A customizable simulation platform for speech processing in moving sound source scenarios,"Kai Li, Wendi Sang, Chang Zeng, Runxuan Yang, Guo Chen, Xiaolin Hu",2024-10-02,"The systematic evaluation of speech separation and enhancement models under
moving sound source conditions typically requires extensive data comprising
diverse scenarios. However, real-world datasets often contain insufficient data
to meet the training and evaluation requirements of models. Although synthetic
datasets offer a larger volume of data, their acoustic simulations lack
realism. Consequently, neither real-world nor synthetic datasets effectively
fulfill practical needs. To address these issues, we introduce SonicSim, a
synthetic toolkit de-designed to generate highly customizable data for moving
sound sources. SonicSim is developed based on the embodied AI simulation
platform, Habitat-sim, supporting multi-level adjustments, including
scene-level, microphone-level, and source-level, thereby generating more
diverse synthetic data. Leveraging SonicSim, we constructed a moving sound
source benchmark dataset, SonicSet, using the Librispeech, the Freesound
Dataset 50k (FSD50K) and Free Music Archive (FMA), and 90 scenes from the
Matterport3D to evaluate speech separation and enhancement models.
Additionally, to validate the differences between synthetic data and real-world
data, we randomly selected 5 hours of raw data without reverberation from the
SonicSet validation set to record a real-world speech separation dataset, which
was then compared with the corresponding synthetic datasets. Similarly, we
utilized the real-world speech enhancement dataset RealMAN to validate the
acoustic gap between other synthetic datasets and the SonicSet dataset for
speech enhancement. The results indicate that the synthetic data generated by
SonicSim can effectively generalize to real-world scenarios. Demo and code are
publicly available at https://cslikai.cn/SonicSim/.",http://arxiv.org/pdf/2410.01481v1
FlipAttack: Jailbreak LLMs via Flipping,"Yue Liu, Xiaoxin He, Miao Xiong, Jinlan Fu, Shumin Deng, Bryan Hooi",2024-10-02,"This paper proposes a simple yet effective jailbreak attack named FlipAttack
against black-box LLMs. First, from the autoregressive nature, we reveal that
LLMs tend to understand the text from left to right and find that they struggle
to comprehend the text when noise is added to the left side. Motivated by these
insights, we propose to disguise the harmful prompt by constructing left-side
noise merely based on the prompt itself, then generalize this idea to 4
flipping modes. Second, we verify the strong ability of LLMs to perform the
text-flipping task, and then develop 4 variants to guide LLMs to denoise,
understand, and execute harmful behaviors accurately. These designs keep
FlipAttack universal, stealthy, and simple, allowing it to jailbreak black-box
LLMs within only 1 query. Experiments on 8 LLMs demonstrate the superiority of
FlipAttack. Remarkably, it achieves $\sim$98\% attack success rate on GPT-4o,
and $\sim$98\% bypass rate against 5 guardrail models on average. The codes are
available at GitHub\footnote{https://github.com/yueliu1999/FlipAttack}.",http://arxiv.org/pdf/2410.02832v1
Fair Class-Incremental Learning using Sample Weighting,"Jaeyoung Park, Minsu Kim, Steven Euijong Whang",2024-10-02,"Model fairness is becoming important in class-incremental learning for
Trustworthy AI. While accuracy has been a central focus in class-incremental
learning, fairness has been relatively understudied. However, naively using all
the samples of the current task for training results in unfair catastrophic
forgetting for certain sensitive groups including classes. We theoretically
analyze that forgetting occurs if the average gradient vector of the current
task data is in an ""opposite direction"" compared to the average gradient vector
of a sensitive group, which means their inner products are negative. We then
propose a fair class-incremental learning framework that adjusts the training
weights of current task samples to change the direction of the average gradient
vector and thus reduce the forgetting of underperforming groups and achieve
fairness. For various group fairness measures, we formulate optimization
problems to minimize the overall losses of sensitive groups while minimizing
the disparities among them. We also show the problems can be solved with linear
programming and propose an efficient Fairness-aware Sample Weighting (FSW)
algorithm. Experiments show that FSW achieves better accuracy-fairness tradeoff
results than state-of-the-art approaches on real datasets.",http://arxiv.org/pdf/2410.01324v1
FanCric : Multi-Agentic Framework for Crafting Fantasy 11 Cricket Teams,Mohit Bhatnagar,2024-10-02,"Cricket, with its intricate strategies and deep history, increasingly
captivates a global audience. The Indian Premier League (IPL), epitomizing
Twenty20 cricket, showcases talent in a format that lasts just a few hours as
opposed to the longer forms of the game. Renowned for its fusion of technology
and fan engagement, the IPL stands as the world's most popular cricket league.
This study concentrates on Dream11, India's leading fantasy cricket league for
IPL, where participants craft virtual teams based on real player performances
to compete internationally. Building a winning fantasy team requires navigating
various complex factors including player form and match conditions.
Traditionally, this has been approached through operations research and machine
learning. This research introduces the FanCric framework, an advanced
multi-agent system leveraging Large Language Models (LLMs) and a robust
orchestration framework to enhance fantasy team selection in cricket. FanCric
employs both structured and unstructured data to surpass traditional methods by
incorporating sophisticated AI technologies. The analysis involved scrutinizing
approximately 12.7 million unique entries from a Dream11 contest, evaluating
FanCric's efficacy against the collective wisdom of crowds and a simpler Prompt
Engineering approach. Ablation studies further assessed the impact of
generating varying numbers of teams. The exploratory findings are promising,
indicating that further investigation into FanCric's capabilities is warranted
to fully realize its potential in enhancing strategic decision-making using
LLMs in fantasy sports and business in general.",http://arxiv.org/pdf/2410.01307v1
Deep Unlearn: Benchmarking Machine Unlearning,"Xavier F. Cadet, Anastasia Borovykh, Mohammad Malekzadeh, Sara Ahmadi-Abhari, Hamed Haddadi",2024-10-02,"Machine unlearning (MU) aims to remove the influence of particular data
points from the learnable parameters of a trained machine learning model. This
is a crucial capability in light of data privacy requirements, trustworthiness,
and safety in deployed models. MU is particularly challenging for deep neural
networks (DNNs), such as convolutional nets or vision transformers, as such
DNNs tend to memorize a notable portion of their training dataset.
Nevertheless, the community lacks a rigorous and multifaceted study that looks
into the success of MU methods for DNNs. In this paper, we investigate 18
state-of-the-art MU methods across various benchmark datasets and models, with
each evaluation conducted over 10 different initializations, a comprehensive
evaluation involving MU over 100K models. We show that, with the proper
hyperparameters, Masked Small Gradients (MSG) and Convolution Transpose (CT),
consistently perform better in terms of model accuracy and run-time efficiency
across different models, datasets, and initializations, assessed by
population-based membership inference attacks (MIA) and per-sample unlearning
likelihood ratio attacks (U-LiRA). Furthermore, our benchmark highlights the
fact that comparing a MU method only with commonly used baselines, such as
Gradient Ascent (GA) or Successive Random Relabeling (SRL), is inadequate, and
we need better baselines like Negative Gradient Plus (NG+) with proper
hyperparameter selection.",http://arxiv.org/pdf/2410.01276v1
A versatile machine learning workflow for high-throughput analysis of supported metal catalyst particles,"Arda Genc, Justin Marlowe, Anika Jalil, Libor Kovarik, Phillip Christopher",2024-10-02,"Accurate and efficient characterization of nanoparticles (NPs), particularly
regarding particle size distribution, is essential for advancing our
understanding of their structure-property relationships and facilitating their
design for various applications. In this study, we introduce a novel two-stage
artificial intelligence (AI)-driven workflow for NP analysis that leverages
prompt engineering techniques from state-of-the-art single-stage object
detection and large-scale vision transformer (ViT) architectures. This
methodology was applied to transmission electron microscopy (TEM) and scanning
TEM (STEM) images of heterogeneous catalysts, enabling high-resolution,
high-throughput analysis of particle size distributions for supported metal
catalysts. The model's performance in detecting and segmenting NPs was
validated across diverse heterogeneous catalyst systems, including various
metals (Cu, Ru, Pt, and PtCo), supports (silica ($\text{SiO}_2$),
$\gamma$-alumina ($\gamma$-$\text{Al}_2\text{O}_3$), and carbon black), and
particle diameter size distributions with means and standard deviations of 2.9
$\pm$ 1.1 nm, 1.6 $\pm$ 0.2 nm, 9.7 $\pm$ 4.6 nm, and 4 $\pm$ 1.0 nm.
Additionally, the proposed machine learning (ML) approach successfully detects
and segments overlapping NPs anchored on non-uniform catalytic support
materials, providing critical insights into their spatial arrangements and
interactions. Our AI-assisted NP analysis workflow demonstrates robust
generalization across diverse datasets and can be readily applied to similar NP
segmentation tasks without requiring costly model retraining.",http://arxiv.org/pdf/2410.01213v1
Generative Diffusion-based Contract Design for Efficient AI Twins Migration in Vehicular Embodied AI Networks,"Yue Zhong, Jiawen Kang, Jinbo Wen, Dongdong Ye, Jiangtian Nie, Dusit Niyato, Xiaozheng Gao, Shengli Xie",2024-10-02,"Embodied AI is a rapidly advancing field that bridges the gap between
cyberspace and physical space, enabling a wide range of applications. This
evolution has led to the development of the Vehicular Embodied AI NETwork
(VEANET), where advanced AI capabilities are integrated into vehicular systems
to enhance autonomous operations and decision-making. Embodied agents, such as
Autonomous Vehicles (AVs), are autonomous entities that can perceive their
environment and take actions to achieve specific goals, actively interacting
with the physical world. Embodied twins are digital models of these embodied
agents, with various embodied AI twins for intelligent applications in
cyberspace. In VEANET, embodied AI twins act as in-vehicle AI assistants to
perform diverse tasks supporting autonomous driving using generative AI models.
Due to limited computational resources of AVs, these AVs often offload
computationally intensive tasks, such as constructing and updating embodied AI
twins, to nearby RSUs. However, since the rapid mobility of AVs and the limited
provision coverage of a single RSU, embodied AI twins require dynamic
migrations from current RSU to other RSUs in real-time, resulting in the
challenge of selecting suitable RSUs for efficient embodied AI twins
migrations. Given information asymmetry, AVs cannot know the detailed
information of RSUs. To this end, in this paper, we construct a
multi-dimensional contract theoretical model between AVs and alternative RSUs.
Considering that AVs may exhibit irrational behavior, we utilize prospect
theory instead of expected utility theory to model the actual utilities of AVs.
Finally, we employ a generative diffusion model-based algorithm to identify the
optimal contract designs. Compared with traditional deep reinforcement learning
algorithms, numerical results demonstrate the effectiveness of the proposed
scheme.",http://arxiv.org/pdf/2410.01176v1
Towards Inference-time Category-wise Safety Steering for Large Language Models,"Amrita Bhattacharjee, Shaona Ghosh, Traian Rebedea, Christopher Parisien",2024-10-02,"While large language models (LLMs) have seen unprecedented advancements in
capabilities and applications across a variety of use-cases, safety alignment
of these models is still an area of active research. The fragile nature of
LLMs, even models that have undergone extensive alignment and safety training
regimes, warrants additional safety steering steps via training-free,
inference-time methods. While recent work in the area of mechanistic
interpretability has investigated how activations in latent representation
spaces may encode concepts, and thereafter performed representation engineering
to induce such concepts in LLM outputs, the applicability of such for safety is
relatively under-explored. Unlike recent inference-time safety steering works,
in this paper we explore safety steering of LLM outputs using: (i)
category-specific steering vectors, thereby enabling fine-grained control over
the steering, and (ii) sophisticated methods for extracting informative
steering vectors for more effective safety steering while retaining quality of
the generated text. We demonstrate our exploration on multiple LLMs and
datasets, and showcase the effectiveness of the proposed steering method, along
with a discussion on the implications and best practices.",http://arxiv.org/pdf/2410.01174v1
Explainable Diagnosis Prediction through Neuro-Symbolic Integration,"Qiuhao Lu, Rui Li, Elham Sagheb, Andrew Wen, Jinlian Wang, Liwei Wang, Jungwei W. Fan, Hongfang Liu",2024-10-01,"Diagnosis prediction is a critical task in healthcare, where timely and
accurate identification of medical conditions can significantly impact patient
outcomes. Traditional machine learning and deep learning models have achieved
notable success in this domain but often lack interpretability which is a
crucial requirement in clinical settings. In this study, we explore the use of
neuro-symbolic methods, specifically Logical Neural Networks (LNNs), to develop
explainable models for diagnosis prediction. Essentially, we design and
implement LNN-based models that integrate domain-specific knowledge through
logical rules with learnable thresholds. Our models, particularly
$M_{\text{multi-pathway}}$ and $M_{\text{comprehensive}}$, demonstrate superior
performance over traditional models such as Logistic Regression, SVM, and
Random Forest, achieving higher accuracy (up to 80.52\%) and AUROC scores (up
to 0.8457) in the case study of diabetes prediction. The learned weights and
thresholds within the LNN models provide direct insights into feature
contributions, enhancing interpretability without compromising predictive
power. These findings highlight the potential of neuro-symbolic approaches in
bridging the gap between accuracy and explainability in healthcare AI
applications. By offering transparent and adaptable diagnostic models, our work
contributes to the advancement of precision medicine and supports the
development of equitable healthcare solutions. Future research will focus on
extending these methods to larger and more diverse datasets to further validate
their applicability across different medical conditions and populations.",http://arxiv.org/pdf/2410.01855v1
softmax is not enough (for sharp out-of-distribution),"Petar Veličković, Christos Perivolaropoulos, Federico Barbero, Razvan Pascanu",2024-10-01,"A key property of reasoning systems is the ability to make sharp decisions on
their input data. For contemporary AI systems, a key carrier of sharp behaviour
is the softmax function, with its capability to perform differentiable
query-key lookups. It is a common belief that the predictive power of networks
leveraging softmax arises from ""circuits"" which sharply perform certain kinds
of computations consistently across many diverse inputs. However, for these
circuits to be robust, they would need to generalise well to arbitrary valid
inputs. In this paper, we dispel this myth: even for tasks as simple as finding
the maximum key, any learned circuitry must disperse as the number of items
grows at test time. We attribute this to a fundamental limitation of the
softmax function to robustly approximate sharp functions, prove this phenomenon
theoretically, and propose adaptive temperature as an ad-hoc technique for
improving the sharpness of softmax at inference time.",http://arxiv.org/pdf/2410.01104v1
Generative AI Application for Building Industry,"Hanlong Wan, Jian Zhang, Yan Chen, Weili Xu, Fan Feng",2024-10-01,"This paper investigates the transformative potential of generative AI
technologies, particularly large language models (LLMs), within the building
industry. By leveraging these advanced AI tools, the study explores their
application across key areas such as energy code compliance, building design
optimization, and workforce training. The research highlights how LLMs can
automate labor-intensive processes, significantly improving efficiency,
accuracy, and safety in building practices. The paper also addresses the
challenges associated with interpreting complex visual and textual data in
architectural plans and regulatory codes, proposing innovative solutions to
enhance AI-driven compliance checking and design processes. Additionally, the
study considers the broader implications of AI integration, including the
development of AI-powered tools for comprehensive code compliance across
various regulatory domains and the potential for AI to revolutionize workforce
training through realistic simulations. This paper provides a comprehensive
analysis of the current capabilities of generative AI in the building industry
while outlining future directions for research and development, aiming to pave
the way for smarter, more sustainable, and responsive construction practices.",http://arxiv.org/pdf/2410.01098v1
PyRIT: A Framework for Security Risk Identification and Red Teaming in Generative AI System,"Gary D. Lopez Munoz, Amanda J. Minnich, Roman Lutz, Richard Lundeen, Raja Sekhar Rao Dheekonda, Nina Chikanov, Bolor-Erdene Jagdagdorj, Martin Pouliot, Shiven Chawla, Whitney Maxwell, Blake Bullwinkel, Katherine Pratt, Joris de Gruyter, Charlotte Siska, Pete Bryan, Tori Westerhoff, Chang Kawaguchi, Christian Seifert, Ram Shankar Siva Kumar, Yonatan Zunger",2024-10-01,"Generative Artificial Intelligence (GenAI) is becoming ubiquitous in our
daily lives. The increase in computational power and data availability has led
to a proliferation of both single- and multi-modal models. As the GenAI
ecosystem matures, the need for extensible and model-agnostic risk
identification frameworks is growing. To meet this need, we introduce the
Python Risk Identification Toolkit (PyRIT), an open-source framework designed
to enhance red teaming efforts in GenAI systems. PyRIT is a model- and
platform-agnostic tool that enables red teamers to probe for and identify novel
harms, risks, and jailbreaks in multimodal generative AI models. Its composable
architecture facilitates the reuse of core building blocks and allows for
extensibility to future models and modalities. This paper details the
challenges specific to red teaming generative AI systems, the development and
features of PyRIT, and its practical applications in real-world scenarios.",http://arxiv.org/pdf/2410.02828v1
Explainable Multi-Stakeholder Job Recommender Systems,Roan Schellingerhout,2024-10-01,"Public opinion on recommender systems has become increasingly wary in recent
years. In line with this trend, lawmakers have also started to become more
critical of such systems, resulting in the introduction of new laws focusing on
aspects such as privacy, fairness, and explainability for recommender systems
and AI at large. These concepts are especially crucial in high-risk domains
such as recruitment. In recruitment specifically, decisions carry substantial
weight, as the outcomes can significantly impact individuals' careers and
companies' success. Additionally, there is a need for a multi-stakeholder
approach, as these systems are used by job seekers, recruiters, and companies
simultaneously, each with its own requirements and expectations. In this paper,
I summarize my current research on the topic of explainable, multi-stakeholder
job recommender systems and set out a number of future research directions.",http://arxiv.org/pdf/2410.00654v1
Human-Robot Collaborative Minimum Time Search through Sub-priors in Ant Colony Optimization,"Oscar Gil Viyuela, Alberto Sanfeliu",2024-10-01,"Human-Robot Collaboration (HRC) has evolved into a highly promising issue
owing to the latest breakthroughs in Artificial Intelligence (AI) and
Human-Robot Interaction (HRI), among other reasons. This emerging growth
increases the need to design multi-agent algorithms that can manage also human
preferences. This paper presents an extension of the Ant Colony Optimization
(ACO) meta-heuristic to solve the Minimum Time Search (MTS) task, in the case
where humans and robots perform an object searching task together. The proposed
model consists of two main blocks. The first one is a convolutional neural
network (CNN) that provides the prior probabilities about where an object may
be from a segmented image. The second one is the Sub-prior MTS-ACO algorithm
(SP-MTS-ACO), which takes as inputs the prior probabilities and the particular
search preferences of the agents in different sub-priors to generate search
plans for all agents. The model has been tested in real experiments for the
joint search of an object through a Vizanti web-based visualization in a tablet
computer. The designed interface allows the communication between a human and
our humanoid robot named IVO. The obtained results show an improvement in the
search perception of the users without loss of efficiency.",http://arxiv.org/pdf/2410.00517v1
Probabilistic Analysis of Copyright Disputes and Generative AI Safety,Hiroaki Chiba-Okabe,2024-10-01,"This paper presents a probabilistic approach to analyzing copyright
infringement disputes by formalizing relevant judicial principles within a
coherent framework based on the random-worlds method. The approach provides a
structured analysis of key evidentiary principles, with particular emphasis on
the ""inverse ratio rule""--a controversial doctrine adopted by some courts.
Although this rule has faced significant criticism, a formal proof demonstrates
its validity, provided it is properly defined. Additionally, the paper examines
the heightened copyright risks posed by generative AI, highlighting how
extensive access to copyrighted material by generative models increases the
risk of infringement. Utilizing the probabilistic approach, the Near
Access-Free (NAF) condition, previously proposed as a potential mitigation
strategy, is evaluated. The analysis reveals that while the NAF condition
mitigates some infringement risks, its justifiability and efficacy are
questionable in certain contexts. These findings demonstrate how a rigorous
probabilistic approach can advance our understanding of copyright jurisprudence
and its interaction with emerging technologies.",http://arxiv.org/pdf/2410.00475v2
Adversarial Suffixes May Be Features Too!,"Wei Zhao, Zhe Li, Yige Li, Jun Sun",2024-10-01,"Despite significant ongoing efforts in safety alignment, large language
models (LLMs) such as GPT-4 and LLaMA 3 remain vulnerable to jailbreak attacks
that can induce harmful behaviors, including those triggered by adversarial
suffixes. Building on prior research, we hypothesize that these adversarial
suffixes are not mere bugs but may represent features that can dominate the
LLM's behavior. To evaluate this hypothesis, we conduct several experiments.
First, we demonstrate that benign features can be effectively made to function
as adversarial suffixes, i.e., we develop a feature extraction method to
extract sample-agnostic features from benign dataset in the form of suffixes
and show that these suffixes may effectively compromise safety alignment.
Second, we show that adversarial suffixes generated from jailbreak attacks may
contain meaningful features, i.e., appending the same suffix to different
prompts results in responses exhibiting specific characteristics. Third, we
show that such benign-yet-safety-compromising features can be easily introduced
through fine-tuning using only benign datasets, i.e., even in the absence of
harmful content. This highlights the critical risk posed by dominating benign
features in the training data and calls for further research to reinforce LLM
safety alignment. Our code and data is available at
\url{https://github.com/anonymous}.",http://arxiv.org/pdf/2410.00451v1
ReXplain: Translating Radiology into Patient-Friendly Video Reports,"Luyang Luo, Jenanan Vairavamurthy, Xiaoman Zhang, Abhinav Kumar, Ramon R. Ter-Oganesyan, Stuart T. Schroff, Dan Shilo, Rydhwana Hossain, Mike Moritz, Pranav Rajpurkar",2024-10-01,"Radiology reports often remain incomprehensible to patients, undermining
patient-centered care. We present ReXplain (Radiology eXplanation), an
innovative AI-driven system that generates patient-friendly video reports for
radiology findings. ReXplain uniquely integrates a large language model for
text simplification, an image segmentation model for anatomical region
identification, and an avatar generation tool, producing comprehensive
explanations with plain language, highlighted imagery, and 3D organ renderings.
Our proof-of-concept study with five board-certified radiologists indicates
that ReXplain could accurately deliver radiological information and effectively
simulate one-on-one consultations. This work demonstrates a new paradigm in
AI-assisted medical communication, potentially improving patient engagement and
satisfaction in radiology care, and opens new avenues for research in
multimodal medical communication.",http://arxiv.org/pdf/2410.00441v1
ManiSkill3: GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI,"Stone Tao, Fanbo Xiang, Arth Shukla, Yuzhe Qin, Xander Hinrichsen, Xiaodi Yuan, Chen Bao, Xinsong Lin, Yulin Liu, Tse-kai Chan, Yuan Gao, Xuanlin Li, Tongzhou Mu, Nan Xiao, Arnav Gurha, Zhiao Huang, Roberto Calandra, Rui Chen, Shan Luo, Hao Su",2024-10-01,"Simulation has enabled unprecedented compute-scalable approaches to robot
learning. However, many existing simulation frameworks typically support a
narrow range of scenes/tasks and lack features critical for scaling
generalizable robotics and sim2real. We introduce and open source ManiSkill3,
the fastest state-visual GPU parallelized robotics simulator with contact-rich
physics targeting generalizable manipulation. ManiSkill3 supports GPU
parallelization of many aspects including simulation+rendering, heterogeneous
simulation, pointclouds/voxels visual input, and more. Simulation with
rendering on ManiSkill3 can run 10-1000x faster with 2-3x less GPU memory usage
than other platforms, achieving up to 30,000+ FPS in benchmarked environments
due to minimal python/pytorch overhead in the system, simulation on the GPU,
and the use of the SAPIEN parallel rendering system. Tasks that used to take
hours to train can now take minutes. We further provide the most comprehensive
range of GPU parallelized environments/tasks spanning 12 distinct domains
including but not limited to mobile manipulation for tasks such as drawing,
humanoids, and dextrous manipulation in realistic scenes designed by artists or
real-world digital twins. In addition, millions of demonstration frames are
provided from motion planning, RL, and teleoperation. ManiSkill3 also provides
a comprehensive set of baselines that span popular RL and
learning-from-demonstrations algorithms.",http://arxiv.org/pdf/2410.00425v1
LinkThief: Combining Generalized Structure Knowledge with Node Similarity for Link Stealing Attack against GNN,"Yuxing Zhang, Siyuan Meng, Chunchun Chen, Mengyao Peng, Hongyan Gu, Xinli Huang",2024-10-01,"Graph neural networks(GNNs) have a wide range of applications in
multimedia.Recent studies have shown that Graph neural networks(GNNs) are
vulnerable to link stealing attacks,which infers the existence of edges in the
target GNN's training graph.Existing attacks are usually based on the
assumption that links exist between two nodes that share similar
posteriors;however,they fail to focus on links that do not hold under this
assumption.To this end,we propose LinkThief,an improved link stealing attack
that combines generalized structure knowledge with node similarity,in a
scenario where the attackers' background knowledge contains partially leaked
target graph and shadow graph.Specifically,to equip the attack model with
insights into the link structure spanning both the shadow graph and the target
graph,we introduce the idea of creating a Shadow-Target Bridge Graph and
extracting edge subgraph structure features from it.Through theoretical
analysis from the perspective of privacy theft,we first explore how to
implement the aforementioned ideas.Building upon the findings,we design the
Bridge Graph Generator to construct the Shadow-Target Bridge Graph.Then,the
subgraph around the link is sampled by the Edge Subgraph Preparation
Module.Finally,the Edge Structure Feature Extractor is designed to obtain
generalized structure knowledge,which is combined with node similarity to form
the features provided to the attack model.Extensive experiments validate the
correctness of theoretical analysis and demonstrate that LinkThief still
effectively steals links without extra assumptions.",http://arxiv.org/pdf/2410.02826v1
Easydiagnos: a framework for accurate feature selection for automatic diagnosis in smart healthcare,"Prasenjit Maji, Amit Kumar Mondal, Hemanta Kumar Mondal, Saraju P. Mohanty",2024-10-01,"The rapid advancements in artificial intelligence (AI) have revolutionized
smart healthcare, driving innovations in wearable technologies, continuous
monitoring devices, and intelligent diagnostic systems. However, security,
explainability, robustness, and performance optimization challenges remain
critical barriers to widespread adoption in clinical environments. This
research presents an innovative algorithmic method using the Adaptive Feature
Evaluator (AFE) algorithm to improve feature selection in healthcare datasets
and overcome problems. AFE integrating Genetic Algorithms (GA), Explainable
Artificial Intelligence (XAI), and Permutation Combination Techniques (PCT),
the algorithm optimizes Clinical Decision Support Systems (CDSS), thereby
enhancing predictive accuracy and interpretability. The proposed method is
validated across three diverse healthcare datasets using six distinct machine
learning algorithms, demonstrating its robustness and superiority over
conventional feature selection techniques. The results underscore the
transformative potential of AFE in smart healthcare, enabling personalized and
transparent patient care. Notably, the AFE algorithm, when combined with a
Multi-layer Perceptron (MLP), achieved an accuracy of up to 98.5%, highlighting
its capability to improve clinical decision-making processes in real-world
healthcare applications.",http://arxiv.org/pdf/2410.00366v1
Social Conjuring: Multi-User Runtime Collaboration with AI in Building Virtual 3D Worlds,"Amina Kobenova, Cyan DeVeaux, Samyak Parajuli, Andrzej Banburski-Fahey, Judith Amores Fernandez, Jaron Lanier",2024-09-30,"Generative artificial intelligence has shown promise in prompting virtual
worlds into existence, yet little attention has been given to understanding how
this process unfolds as social interaction. We present Social Conjurer, a
framework for AI-augmented dynamic 3D scene co-creation, where multiple users
collaboratively build and modify virtual worlds in real-time. Through an
expanded set of interactions, including social and tool-based engagements as
well as spatial reasoning, our framework facilitates the creation of rich,
diverse virtual environments. Findings from a preliminary user study (N=12)
provide insight into the user experience of this approach, how social contexts
shape the prompting of spatial environments, and perspective on social
applications of prompt-based 3D co-creation. In addition to highlighting the
potential of AI-supported multi-user world creation and offering new pathways
for AI-augmented creative processes in VR, this article presents a set of
implications for designing human-centered interfaces that incorporate AI models
into 3D content generation.",http://arxiv.org/pdf/2410.00274v2
Possible principles for aligned structure learning agents,"Lancelot Da Costa, Tomáš Gavenčiak, David Hyland, Mandana Samiei, Cristian Dragos-Manta, Candice Pattisapu, Adeel Razi, Karl Friston",2024-09-30,"This paper offers a roadmap for the development of scalable aligned
artificial intelligence (AI) from first principle descriptions of natural
intelligence. In brief, a possible path toward scalable aligned AI rests upon
enabling artificial agents to learn a good model of the world that includes a
good model of our preferences. For this, the main objective is creating agents
that learn to represent the world and other agents' world models; a problem
that falls under structure learning (a.k.a. causal representation learning). We
expose the structure learning and alignment problems with this goal in mind, as
well as principles to guide us forward, synthesizing various ideas across
mathematics, statistics, and cognitive science. 1) We discuss the essential
role of core knowledge, information geometry and model reduction in structure
learning, and suggest core structural modules to learn a wide range of
naturalistic worlds. 2) We outline a way toward aligned agents through
structure learning and theory of mind. As an illustrative example, we
mathematically sketch Asimov's Laws of Robotics, which prescribe agents to act
cautiously to minimize the ill-being of other agents. We supplement this
example by proposing refined approaches to alignment. These observations may
guide the development of artificial intelligence in helping to scale existing
-- or design new -- aligned structure learning systems.",http://arxiv.org/pdf/2410.00258v1
Demonstrating the Continual Learning Capabilities and Practical Application of Discrete-Time Active Inference,Rithvik Prakki,2024-09-30,"Active inference is a mathematical framework for understanding how agents
(biological or artificial) interact with their environments, enabling continual
adaptation and decision-making. It combines Bayesian inference and free energy
minimization to model perception, action, and learning in uncertain and dynamic
contexts. Unlike reinforcement learning, active inference integrates
exploration and exploitation seamlessly by minimizing expected free energy. In
this paper, we present a continual learning framework for agents operating in
discrete time environments, using active inference as the foundation. We derive
the mathematical formulations of variational and expected free energy and apply
them to the design of a self-learning research agent. This agent updates its
beliefs and adapts its actions based on new data without manual intervention.
Through experiments in changing environments, we demonstrate the agent's
ability to relearn and refine its models efficiently, making it suitable for
complex domains like finance and healthcare. The paper concludes by discussing
how the proposed framework generalizes to other systems, positioning active
inference as a flexible approach for adaptive AI.",http://arxiv.org/pdf/2410.00240v1
Adapting LLMs for the Medical Domain in Portuguese: A Study on Fine-Tuning and Model Evaluation,"Pedro Henrique Paiola, Gabriel Lino Garcia, João Renato Ribeiro Manesco, Mateus Roder, Douglas Rodrigues, João Paulo Papa",2024-09-30,"This study evaluates the performance of large language models (LLMs) as
medical agents in Portuguese, aiming to develop a reliable and relevant virtual
assistant for healthcare professionals. The HealthCareMagic-100k-en and MedQuAD
datasets, translated from English using GPT-3.5, were used to fine-tune the
ChatBode-7B model using the PEFT-QLoRA method. The InternLM2 model, with
initial training on medical data, presented the best overall performance, with
high precision and adequacy in metrics such as accuracy, completeness and
safety. However, DrBode models, derived from ChatBode, exhibited a phenomenon
of catastrophic forgetting of acquired medical knowledge. Despite this, these
models performed frequently or even better in aspects such as grammaticality
and coherence. A significant challenge was low inter-rater agreement,
highlighting the need for more robust assessment protocols. This work paves the
way for future research, such as evaluating multilingual models specific to the
medical field, improving the quality of training data, and developing more
consistent evaluation methodologies for the medical field.",http://arxiv.org/pdf/2410.00163v1
Semantic-Driven Topic Modeling Using Transformer-Based Embeddings and Clustering Algorithms,"Melkamu Abay Mersha, Mesay Gemeda yigezu, Jugal Kalita",2024-09-30,"Topic modeling is a powerful technique to discover hidden topics and patterns
within a collection of documents without prior knowledge. Traditional topic
modeling and clustering-based techniques encounter challenges in capturing
contextual semantic information. This study introduces an innovative end-to-end
semantic-driven topic modeling technique for the topic extraction process,
utilizing advanced word and document embeddings combined with a powerful
clustering algorithm. This semantic-driven approach represents a significant
advancement in topic modeling methodologies. It leverages contextual semantic
information to extract coherent and meaningful topics. Specifically, our model
generates document embeddings using pre-trained transformer-based language
models, reduces the dimensions of the embeddings, clusters the embeddings based
on semantic similarity, and generates coherent topics for each cluster.
Compared to ChatGPT and traditional topic modeling algorithms, our model
provides more coherent and meaningful topics.",http://arxiv.org/pdf/2410.00134v1
Maia-2: A Unified Model for Human-AI Alignment in Chess,"Zhenwei Tang, Difan Jiao, Reid McIlroy-Young, Jon Kleinberg, Siddhartha Sen, Ashton Anderson",2024-09-30,"There are an increasing number of domains in which artificial intelligence
(AI) systems both surpass human ability and accurately model human behavior.
This introduces the possibility of algorithmically-informed teaching in these
domains through more relatable AI partners and deeper insights into human
decision-making. Critical to achieving this goal, however, is coherently
modeling human behavior at various skill levels. Chess is an ideal model system
for conducting research into this kind of human-AI alignment, with its rich
history as a pivotal testbed for AI research, mature superhuman AI systems like
AlphaZero, and precise measurements of skill via chess rating systems. Previous
work in modeling human decision-making in chess uses completely independent
models to capture human style at different skill levels, meaning they lack
coherence in their ability to adapt to the full spectrum of human improvement
and are ultimately limited in their effectiveness as AI partners and teaching
tools. In this work, we propose a unified modeling approach for human-AI
alignment in chess that coherently captures human style across different skill
levels and directly captures how people improve. Recognizing the complex,
non-linear nature of human learning, we introduce a skill-aware attention
mechanism to dynamically integrate players' strengths with encoded chess
positions, enabling our model to be sensitive to evolving player skill. Our
experimental results demonstrate that this unified framework significantly
enhances the alignment between AI and human players across a diverse range of
expertise levels, paving the way for deeper insights into human decision-making
and AI-guided teaching tools.",http://arxiv.org/pdf/2409.20553v1
From homeostasis to resource sharing: Biologically and economically compatible multi-objective multi-agent AI safety benchmarks,"Roland Pihlakas, Joel Pyykkö",2024-09-30,"Developing safe agentic AI systems benefits from automated empirical testing
that conforms with human values, a subfield that is largely underdeveloped at
the moment. To contribute towards this topic, present work focuses on
introducing biologically and economically motivated themes that have been
neglected in the safety aspects of modern reinforcement learning literature,
namely homeostasis, balancing multiple objectives, bounded objectives,
diminishing returns, sustainability, and multi-agent resource sharing. We
implemented eight main benchmark environments on the above themes, for
illustrating the potential shortcomings of current mainstream discussions on AI
safety.",http://arxiv.org/pdf/2410.00081v1
SMLE: Safe Machine Learning via Embedded Overapproximation,"Matteo Francobaldi, Michele Lombardi",2024-09-30,"Despite the extent of recent advances in Machine Learning (ML) and Neural
Networks, providing formal guarantees on the behavior of these systems is still
an open problem, and a crucial requirement for their adoption in regulated or
safety-critical scenarios. We consider the task of training differentiable ML
models guaranteed to satisfy designer-chosen properties, stated as input-output
implications. This is very challenging, due to the computational complexity of
rigorously verifying and enforcing compliance in modern neural models. We
provide an innovative approach based on three components: 1) a general, simple
architecture enabling efficient verification with a conservative semantic; 2) a
rigorous training algorithm based on the Projected Gradient Method; 3) a
formulation of the problem of searching for strong counterexamples. The
proposed framework, being only marginally affected by model complexity, scales
well to practical applications, and produces models that provide full property
satisfaction guarantees. We evaluate our approach on properties defined by
linear inequalities in regression, and on mutually exclusive classes in
multilabel classification. Our approach is competitive with a baseline that
includes property enforcement during preprocessing, i.e. on the training data,
as well as during postprocessing, i.e. on the model predictions. Finally, our
contributions establish a framework that opens up multiple research directions
and potential improvements.",http://arxiv.org/pdf/2409.20517v1
Interactive Speculative Planning: Enhance Agent Efficiency through Co-design of System and User Interface,"Wenyue Hua, Mengting Wan, Shashank Vadrevu, Ryan Nadel, Yongfeng Zhang, Chi Wang",2024-09-30,"Agents, as user-centric tools, are increasingly deployed for human task
delegation, assisting with a broad spectrum of requests by generating thoughts,
engaging with user proxies, and producing action plans. However, agents based
on large language models (LLMs) often face substantial planning latency due to
two primary factors: the efficiency limitations of the underlying LLMs due to
their large size and high demand, and the structural complexity of the agents
due to the extensive generation of intermediate thoughts to produce the final
output. Given that inefficiency in service provision can undermine the value of
automation for users, this paper presents a human-centered efficient agent
planning method -- Interactive Speculative Planning -- aiming at enhancing the
efficiency of agent planning through both system design and human-AI
interaction. Our approach advocates for the co-design of the agent system and
user interface, underscoring the importance of an agent system that can fluidly
manage user interactions and interruptions. By integrating human interruptions
as a fundamental component of the system, we not only make it more user-centric
but also expedite the entire process by leveraging human-in-the-loop
interactions to provide accurate intermediate steps. Code and data will be
released.",http://arxiv.org/pdf/2410.00079v1
Enhancing GANs with Contrastive Learning-Based Multistage Progressive Finetuning SNN and RL-Based External Optimization,Osama Mustafa,2024-09-30,"The application of deep learning in cancer research, particularly in early
diagnosis, case understanding, and treatment strategy design, emphasizes the
need for high-quality data. Generative AI, especially Generative Adversarial
Networks (GANs), has emerged as a leading solution to challenges like class
imbalance, robust learning, and model training, while addressing issues
stemming from patient privacy and the scarcity of real data. Despite their
promise, GANs face several challenges, both inherent and specific to
histopathology data. Inherent issues include training imbalance, mode collapse,
linear learning from insufficient discriminator feedback, and hard boundary
convergence due to stringent feedback. Histopathology data presents a unique
challenge with its complex representation, high spatial resolution, and
multiscale features. To address these challenges, we propose a framework
consisting of two components. First, we introduce a contrastive learning-based
Multistage Progressive Finetuning Siamese Neural Network (MFT-SNN) for
assessing the similarity between histopathology patches. Second, we implement a
Reinforcement Learning-based External Optimizer (RL-EO) within the GAN training
loop, serving as a reward signal generator. The modified discriminator loss
function incorporates a weighted reward, guiding the GAN to maximize this
reward while minimizing loss. This approach offers an external optimization
guide to the discriminator, preventing generator overfitting and ensuring
smooth convergence. Our proposed solution has been benchmarked against
state-of-the-art (SOTA) GANs and a Denoising Diffusion Probabilistic model,
outperforming previous SOTA across various metrics, including FID score, KID
score, Perceptual Path Length, and downstream classification tasks.",http://arxiv.org/pdf/2409.20340v2
Learning to Ground Existentially Quantified Goals,"Martin Funkquist, Simon Ståhlberg, Hector Geffner",2024-09-30,"Goal instructions for autonomous AI agents cannot assume that objects have
unique names. Instead, objects in goals must be referred to by providing
suitable descriptions. However, this raises problems in both classical planning
and generalized planning. The standard approach to handling existentially
quantified goals in classical planning involves compiling them into a DNF
formula that encodes all possible variable bindings and adding dummy actions to
map each DNF term into the new, dummy goal. This preprocessing is exponential
in the number of variables. In generalized planning, the problem is different:
even if general policies can deal with any initial situation and goal,
executing a general policy requires the goal to be grounded to define a value
for the policy features. The problem of grounding goals, namely finding the
objects to bind the goal variables, is subtle: it is a generalization of
classical planning, which is a special case when there are no goal variables to
bind, and constraint reasoning, which is a special case when there are no
actions. In this work, we address the goal grounding problem with a novel
supervised learning approach. A GNN architecture, trained to predict the cost
of partially quantified goals over small domain instances is tested on larger
instances involving more objects and different quantified goals. The proposed
architecture is evaluated experimentally over several planning domains where
generalization is tested along several dimensions including the number of goal
variables and objects that can bind such variables. The scope of the approach
is also discussed in light of the known relationship between GNNs and C2
logics.",http://arxiv.org/pdf/2409.20259v1
Mitigating Backdoor Threats to Large Language Models: Advancement and Challenges,"Qin Liu, Wenjie Mo, Terry Tong, Jiashu Xu, Fei Wang, Chaowei Xiao, Muhao Chen",2024-09-30,"The advancement of Large Language Models (LLMs) has significantly impacted
various domains, including Web search, healthcare, and software development.
However, as these models scale, they become more vulnerable to cybersecurity
risks, particularly backdoor attacks. By exploiting the potent memorization
capacity of LLMs, adversaries can easily inject backdoors into LLMs by
manipulating a small portion of training data, leading to malicious behaviors
in downstream applications whenever the hidden backdoor is activated by the
pre-defined triggers. Moreover, emerging learning paradigms like instruction
tuning and reinforcement learning from human feedback (RLHF) exacerbate these
risks as they rely heavily on crowdsourced data and human feedback, which are
not fully controlled. In this paper, we present a comprehensive survey of
emerging backdoor threats to LLMs that appear during LLM development or
inference, and cover recent advancement in both defense and detection
strategies for mitigating backdoor threats to LLMs. We also outline key
challenges in addressing these threats, highlighting areas for future research.",http://arxiv.org/pdf/2409.19993v1
Positive-Sum Fairness: Leveraging Demographic Attributes to Achieve Fair AI Outcomes Without Sacrificing Group Gains,"Samia Belhadj, Sanguk Park, Ambika Seth, Hesham Dar, Thijs Kooi",2024-09-30,"Fairness in medical AI is increasingly recognized as a crucial aspect of
healthcare delivery. While most of the prior work done on fairness emphasizes
the importance of equal performance, we argue that decreases in fairness can be
either harmful or non-harmful, depending on the type of change and how
sensitive attributes are used. To this end, we introduce the notion of
positive-sum fairness, which states that an increase in performance that
results in a larger group disparity is acceptable as long as it does not come
at the cost of individual subgroup performance. This allows sensitive
attributes correlated with the disease to be used to increase performance
without compromising on fairness.
  We illustrate this idea by comparing four CNN models that make different use
of the race attribute in the training phase. The results show that removing all
demographic encodings from the images helps close the gap in performance
between the different subgroups, whereas leveraging the race attribute as a
model's input increases the overall performance while widening the disparities
between subgroups. These larger gaps are then put in perspective of the
collective benefit through our notion of positive-sum fairness to distinguish
harmful from non harmful disparities.",http://arxiv.org/pdf/2409.19940v1
"UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional Summarization Evaluation for LLMs","Yuho Lee, Taewon Yun, Jason Cai, Hang Su, Hwanjun Song",2024-09-30,"Existing benchmarks for summarization quality evaluation often lack diverse
input scenarios, focus on narrowly defined dimensions (e.g., faithfulness), and
struggle with subjective and coarse-grained annotation schemes. To address
these shortcomings, we create UniSumEval benchmark, which extends the range of
input context (e.g., domain, length) and provides fine-grained,
multi-dimensional annotations. We use AI assistance in data creation,
identifying potentially hallucinogenic input texts, and also helping human
annotators reduce the difficulty of fine-grained annotation tasks. With
UniSumEval, we benchmark nine latest language models as summarizers, offering
insights into their performance across varying input contexts and evaluation
dimensions. Furthermore, we conduct a thorough comparison of SOTA automated
summary evaluators. Our benchmark data will be available at
https://github.com/DISL-Lab/UniSumEval-v1.0.",http://arxiv.org/pdf/2409.19898v2
ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities,"Ezra Karger, Houtan Bastani, Chen Yueh-Han, Zachary Jacobs, Danny Halawi, Fred Zhang, Philip E. Tetlock",2024-09-30,"Forecasts of future events are essential inputs into informed
decision-making. Machine learning (ML) systems have the potential to deliver
forecasts at scale, but there is no framework for evaluating the accuracy of ML
systems on a standardized set of forecasting questions. To address this gap, we
introduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML
systems on an automatically generated and regularly updated set of 1,000
forecasting questions. To avoid any possibility of data leakage, ForecastBench
is comprised solely of questions about future events that have no known answer
at the time of submission. We quantify the ability of current ML systems by
collecting forecasts from expert (human) forecasters, the general public, and
LLMs on a random subset of questions from the benchmark (N = 200). While LLMs
have achieved super-human performance on many benchmarks, they perform less
well here: expert forecasters outperform the top-performing LLM (p-values <=
0.01). We display system and human scores in a public leaderboard at
www.forecastbench.org.",http://arxiv.org/pdf/2409.19839v1
Can Models Learn Skill Composition from Examples?,"Haoyu Zhao, Simran Kaur, Dingli Yu, Anirudh Goyal, Sanjeev Arora",2024-09-29,"As large language models (LLMs) become increasingly advanced, their ability
to exhibit compositional generalization -- the capacity to combine learned
skills in novel ways not encountered during training -- has garnered
significant attention. This type of generalization, particularly in scenarios
beyond training data, is also of great interest in the study of AI safety and
alignment. A recent study introduced the SKILL-MIX evaluation, where models are
tasked with composing a short paragraph demonstrating the use of a specified
$k$-tuple of language skills. While small models struggled with composing even
with $k=3$, larger models like GPT-4 performed reasonably well with $k=5$ and
$6$.
  In this paper, we employ a setup akin to SKILL-MIX to evaluate the capacity
of smaller models to learn compositional generalization from examples.
Utilizing a diverse set of language skills -- including rhetorical, literary,
reasoning, theory of mind, and common sense -- GPT-4 was used to generate text
samples that exhibit random subsets of $k$ skills. Subsequent fine-tuning of 7B
and 13B parameter models on these combined skill texts, for increasing values
of $k$, revealed the following findings: (1) Training on combinations of $k=2$
and $3$ skills results in noticeable improvements in the ability to compose
texts with $k=4$ and $5$ skills, despite models never having seen such examples
during training. (2) When skill categories are split into training and held-out
groups, models significantly improve at composing texts with held-out skills
during testing despite having only seen training skills during fine-tuning,
illustrating the efficacy of the training approach even with previously unseen
skills. This study also suggests that incorporating skill-rich (potentially
synthetic) text into training can substantially enhance the compositional
capabilities of models.",http://arxiv.org/pdf/2409.19808v1
Constrained Reinforcement Learning for Safe Heat Pump Control,"Baohe Zhang, Lilli Frison, Thomas Brox, Joschka Bödecker",2024-09-29,"Constrained Reinforcement Learning (RL) has emerged as a significant research
area within RL, where integrating constraints with rewards is crucial for
enhancing safety and performance across diverse control tasks. In the context
of heating systems in the buildings, optimizing the energy efficiency while
maintaining the residents' thermal comfort can be intuitively formulated as a
constrained optimization problem. However, to solve it with RL may require
large amount of data. Therefore, an accurate and versatile simulator is
favored. In this paper, we propose a novel building simulator I4B which
provides interfaces for different usages and apply a model-free constrained RL
algorithm named constrained Soft Actor-Critic with Linear Smoothed Log Barrier
function (CSAC-LB) to the heating optimization problem. Benchmarking against
baseline algorithms demonstrates CSAC-LB's efficiency in data exploration,
constraint satisfaction and performance.",http://arxiv.org/pdf/2409.19716v1
Fine-Tuning Hybrid Physics-Informed Neural Networks for Vehicle Dynamics Model Estimation,"Shiming Fang, Kaiyan Yu",2024-09-29,"Accurate dynamic modeling is critical for autonomous racing vehicles,
especially during high-speed and agile maneuvers where precise motion
prediction is essential for safety. Traditional parameter estimation methods
face limitations such as reliance on initial guesses, labor-intensive fitting
procedures, and complex testing setups. On the other hand, purely data-driven
machine learning methods struggle to capture inherent physical constraints and
typically require large datasets for optimal performance. To address these
challenges, this paper introduces the Fine-Tuning Hybrid Dynamics (FTHD)
method, which integrates supervised and unsupervised Physics-Informed Neural
Networks (PINNs), combining physics-based modeling with data-driven techniques.
FTHD fine-tunes a pre-trained Deep Dynamics Model (DDM) using a smaller
training dataset, delivering superior performance compared to state-of-the-art
methods such as the Deep Pacejka Model (DPM) and outperforming the original
DDM. Furthermore, an Extended Kalman Filter (EKF) is embedded within FTHD
(EKF-FTHD) to effectively manage noisy real-world data, ensuring accurate
denoising while preserving the vehicle's essential physical characteristics.
The proposed FTHD framework is validated through scaled simulations using the
BayesRace Physics-based Simulator and full-scale real-world experiments from
the Indy Autonomous Challenge. Results demonstrate that the hybrid approach
significantly improves parameter estimation accuracy, even with reduced data,
and outperforms existing models. EKF-FTHD enhances robustness by denoising
real-world data while maintaining physical insights, representing a notable
advancement in vehicle dynamics modeling for high-speed autonomous racing.",http://arxiv.org/pdf/2409.19647v1
BadHMP: Backdoor Attack against Human Motion Prediction,"Chaohui Xu, Si Wang, Chip-Hong Chang",2024-09-29,"Precise future human motion prediction over subsecond horizons from past
observations is crucial for various safety-critical applications. To date, only
one study has examined the vulnerability of human motion prediction to evasion
attacks. In this paper, we propose BadHMP, the first backdoor attack that
targets specifically human motion prediction. Our approach involves generating
poisoned training samples by embedding a localized backdoor trigger in one arm
of the skeleton, causing selected joints to remain relatively still or follow
predefined motion in historical time steps. Subsequently, the future sequences
are globally modified to the target sequences, and the entire training dataset
is traversed to select the most suitable samples for poisoning. Our carefully
designed backdoor triggers and targets guarantee the smoothness and naturalness
of the poisoned samples, making them stealthy enough to evade detection by the
model trainer while keeping the poisoned model unobtrusive in terms of
prediction fidelity to untainted sequences. The target sequences can be
successfully activated by the designed input sequences even with a low poisoned
sample injection ratio. Experimental results on two datasets (Human3.6M and
CMU-Mocap) and two network architectures (LTD and HRI) demonstrate the
high-fidelity, effectiveness, and stealthiness of BadHMP. Robustness of our
attack against fine-tuning defense is also verified.",http://arxiv.org/pdf/2409.19638v1
Estimating Body and Hand Motion in an Ego-sensed World,"Brent Yi, Vickie Ye, Maya Zheng, Lea Müller, Georgios Pavlakos, Yi Ma, Jitendra Malik, Angjoo Kanazawa",2024-10-04,"We present EgoAllo, a system for human motion estimation from a head-mounted
device. Using only egocentric SLAM poses and images, EgoAllo guides sampling
from a conditional diffusion model to estimate 3D body pose, height, and hand
parameters that capture the wearer's actions in the allocentric coordinate
frame of the scene. To achieve this, our key insight is in representation: we
propose spatial and temporal invariance criteria for improving model
performance, from which we derive a head motion conditioning parameterization
that improves estimation by up to 18%. We also show how the bodies estimated by
our system can improve the hands: the resulting kinematic and temporal
constraints result in over 40% lower hand estimation errors compared to noisy
monocular estimates. Project page: https://egoallo.github.io/",http://arxiv.org/pdf/2410.03665v1
Enhance Reasoning by Learning from Mistakes: Peer-Review Knowledge Distillation from Multiple Large Language Models,"Zhuochun Li, Yuelyu Ji, Rui Meng, Daqing He",2024-10-04,"Large language models (LLMs) have exhibited complex reasoning abilities by
generating question rationales and demonstrated exceptional performance in
natural language processing (NLP) tasks. However, these reasoning capabilities
generally emerge in models with tens of billions of parameters, creating
significant computational challenges for real-world deployment. Recent research
has concentrated on improving open-source smaller models through knowledge
distillation (KD) from commercial LLMs. Nevertheless, most of these studies
rely solely on the responses from one single LLM as the gold rationale for
training. In this paper, we introduce a novel Mistake-Aware Peer-Review
Distillation (MAPD) approach: 1) Instead of merely obtaining gold rationales
from teachers, our method asks teachers to identify and explain the student's
mistakes, providing customized instruction learning data. 2) We design a
simulated peer-review process between teacher LLMs, which selects only the
generated rationales above the acceptance threshold. This reduces the chance of
teachers guessing correctly with flawed rationale, improving instructional data
quality. Comprehensive experiments and analysis on mathematical, commonsense,
and logical reasoning tasks demonstrate the effectiveness of our method.",http://arxiv.org/pdf/2410.03663v1
System 2 reasoning capabilities are nigh,Scott C. Lowe,2024-10-04,"In recent years, machine learning models have made strides towards human-like
reasoning capabilities from several directions. In this work, we review the
current state of the literature and describe the remaining steps to achieve a
neural model which can perform System 2 reasoning analogous to a human. We
argue that if current models are insufficient to be classed as performing
reasoning, there remains very little additional progress needed to attain that
goal.",http://arxiv.org/pdf/2410.03662v1
Geometric Representation Condition Improves Equivariant Molecule Generation,"Zian Li, Cai Zhou, Xiyuan Wang, Xingang Peng, Muhan Zhang",2024-10-04,"Recent advancements in molecular generative models have demonstrated
substantial potential in accelerating scientific discovery, particularly in
drug design. However, these models often face challenges in generating
high-quality molecules, especially in conditional scenarios where specific
molecular properties must be satisfied. In this work, we introduce GeoRCG, a
general framework to enhance the performance of molecular generative models by
integrating geometric representation conditions. We decompose the molecule
generation process into two stages: first, generating an informative geometric
representation; second, generating a molecule conditioned on the
representation. Compared to directly generating a molecule, the relatively
easy-to-generate representation in the first-stage guides the second-stage
generation to reach a high-quality molecule in a more goal-oriented and much
faster way. Leveraging EDM as the base generator, we observe significant
quality improvements in unconditional molecule generation on the widely-used
QM9 and GEOM-DRUG datasets. More notably, in the challenging conditional
molecular generation task, our framework achieves an average 31\% performance
improvement over state-of-the-art approaches, highlighting the superiority of
conditioning on semantically rich geometric representations over conditioning
on individual property values as in previous approaches. Furthermore, we show
that, with such representation guidance, the number of diffusion steps can be
reduced to as small as 100 while maintaining superior generation quality than
that achieved with 1,000 steps, thereby significantly accelerating the
generation process.",http://arxiv.org/pdf/2410.03655v1
Aligning LLMs with Individual Preferences via Interaction,"Shujin Wu, May Fung, Cheng Qian, Jeonghwan Kim, Dilek Hakkani-Tur, Heng Ji",2024-10-04,"As large language models (LLMs) demonstrate increasingly advanced
capabilities, aligning their behaviors with human values and preferences
becomes crucial for their wide adoption. While previous research focuses on
general alignment to principles such as helpfulness, harmlessness, and honesty,
the need to account for individual and diverse preferences has been largely
overlooked, potentially undermining customized human experiences. To address
this gap, we train LLMs that can ''interact to align'', essentially cultivating
the meta-skill of LLMs to implicitly infer the unspoken personalized
preferences of the current user through multi-turn conversations, and then
dynamically align their following behaviors and responses to these inferred
preferences. Our approach involves establishing a diverse pool of 3,310
distinct user personas by initially creating seed examples, which are then
expanded through iterative self-generation and filtering. Guided by distinct
user personas, we leverage multi-LLM collaboration to develop a multi-turn
preference dataset containing 3K+ multi-turn conversations in tree structures.
Finally, we apply supervised fine-tuning and reinforcement learning to enhance
LLMs using this dataset. For evaluation, we establish the ALOE (ALign With
CustOmized PrEferences) benchmark, consisting of 100 carefully selected
examples and well-designed metrics to measure the customized alignment
performance during conversations. Experimental results demonstrate the
effectiveness of our method in enabling dynamic, personalized alignment via
interaction.",http://arxiv.org/pdf/2410.03642v1
What Matters for Model Merging at Scale?,"Prateek Yadav, Tu Vu, Jonathan Lai, Alexandra Chronopoulou, Manaal Faruqui, Mohit Bansal, Tsendsuren Munkhdalai",2024-10-04,"Model merging aims to combine multiple expert models into a more capable
single model, offering benefits such as reduced storage and serving costs,
improved generalization, and support for decentralized model development.
Despite its promise, previous studies have primarily focused on merging a few
small models. This leaves many unanswered questions about the effect of scaling
model size and how it interplays with other key factors -- like the base model
quality and number of expert models -- , to affect the merged model's
performance. This work systematically evaluates the utility of model merging at
scale, examining the impact of these different factors. We experiment with
merging fully fine-tuned models using 4 popular merging methods -- Averaging,
Task~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B
parameters and merging up to 8 different expert models. We evaluate the merged
models on both held-in tasks, i.e., the expert's training tasks, and zero-shot
generalization to unseen held-out tasks. Our experiments provide several new
insights about model merging at scale and the interplay between different
factors. First, we find that merging is more effective when experts are created
from strong base models, i.e., models with good zero-shot performance. Second,
larger models facilitate easier merging. Third merging consistently improves
generalization capabilities. Notably, when merging 8 large expert models, the
merged models often generalize better compared to the multitask trained models.
Fourth, we can better merge more expert models when working with larger models.
Fifth, different merging methods behave very similarly at larger scales.
Overall, our findings shed light on some interesting properties of model
merging while also highlighting some limitations. We hope that this study will
serve as a reference point on large-scale merging for upcoming research.",http://arxiv.org/pdf/2410.03617v1
TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation,"Jonathan Cook, Tim Rocktäschel, Jakob Foerster, Dennis Aumiller, Alex Wang",2024-10-04,"Given the widespread adoption and usage of Large Language Models (LLMs), it
is crucial to have flexible and interpretable evaluations of their
instruction-following ability. Preference judgments between model outputs have
become the de facto evaluation standard, despite distilling complex,
multi-faceted preferences into a single ranking. Furthermore, as human
annotation is slow and costly, LLMs are increasingly used to make these
judgments, at the expense of reliability and interpretability. In this work, we
propose TICK (Targeted Instruct-evaluation with ChecKlists), a fully automated,
interpretable evaluation protocol that structures evaluations with
LLM-generated, instruction-specific checklists. We first show that, given an
instruction, LLMs can reliably produce high-quality, tailored evaluation
checklists that decompose the instruction into a series of YES/NO questions.
Each question asks whether a candidate response meets a specific requirement of
the instruction. We demonstrate that using TICK leads to a significant increase
(46.4% $\to$ 52.2%) in the frequency of exact agreements between LLM judgements
and human preferences, as compared to having an LLM directly score an output.
We then show that STICK (Self-TICK) can be used to improve generation quality
across multiple benchmarks via self-refinement and Best-of-N selection. STICK
self-refinement on LiveBench reasoning tasks leads to an absolute gain of
$+$7.8%, whilst Best-of-N selection with STICK attains $+$6.3% absolute
improvement on the real-world instruction dataset, WildBench. In light of this,
structured, multi-faceted self-improvement is shown to be a promising way to
further advance LLM capabilities. Finally, by providing LLM-generated
checklists to human evaluators tasked with directly scoring LLM responses to
WildBench instructions, we notably increase inter-annotator agreement (0.194
$\to$ 0.256).",http://arxiv.org/pdf/2410.03608v1
SiMilarity-Enhanced Homophily for Multi-View Heterophilous Graph Clustering,"Jianpeng Chen, Yawen Ling, Yazhou Ren, Zichen Wen, Tianyi Wu, Shufei Zhang, Lifang He",2024-10-04,"With the increasing prevalence of graph-structured data, multi-view graph
clustering has been widely used in various downstream applications. Existing
approaches primarily rely on a unified message passing mechanism, which
significantly enhances clustering performance. Nevertheless, this mechanism
limits its applicability to heterophilous situations, as it is fundamentally
predicated on the assumption of homophily, i.e., the connected nodes often
belong to the same class. In reality, this assumption does not always hold; a
moderately or even mildly homophilous graph is more common than a fully
homophilous one due to inevitable heterophilous information in the graph. To
address this issue, in this paper, we propose a novel SiMilarity-enhanced
Homophily for Multi-view Heterophilous Graph Clustering (SMHGC) approach. By
analyzing the relationship between similarity and graph homophily, we propose
to enhance the homophily by introducing three similarity terms, i.e., neighbor
pattern similarity, node feature similarity, and multi-view global similarity,
in a label-free manner. Then, a consensus-based inter- and intra-view fusion
paradigm is proposed to fuse the improved homophilous graph from different
views and utilize them for clustering. The state-of-the-art experimental
results on both multi-view heterophilous and homophilous datasets collectively
demonstrate the strong capacity of similarity for unsupervised multi-view
heterophilous graph learning. Additionally, the consistent performance across
semi-synthetic datasets with varying levels of homophily serves as further
evidence of SMHGC's resilience to heterophily.",http://arxiv.org/pdf/2410.03596v1
Understanding Reasoning in Chain-of-Thought from the Hopfieldian View,"Lijie Hu, Liang Liu, Shu Yang, Xin Chen, Zhen Tan, Muhammad Asif Ali, Mengdi Li, Di Wang",2024-10-04,"Large Language Models have demonstrated remarkable abilities across various
tasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to
enhance reasoning capabilities. However, existing research primarily focuses on
improving performance, lacking a comprehensive framework to explain and
understand the fundamental factors behind CoT's success. To bridge this gap, we
introduce a novel perspective grounded in the Hopfieldian view of cognition in
cognitive neuroscience. We establish a connection between CoT reasoning and key
cognitive elements such as stimuli, actions, neural populations, and
representation spaces. From our view, we can understand the reasoning process
as the movement between these representation spaces. Building on this insight,
we develop a method for localizing reasoning errors in the response of CoTs.
Moreover, we propose the Representation-of-Thought (RoT) framework, which
leverages the robustness of low-dimensional representation spaces to enhance
the robustness of the reasoning process in CoTs. Experimental results
demonstrate that RoT improves the robustness and interpretability of CoT
reasoning while offering fine-grained control over the reasoning process.",http://arxiv.org/pdf/2410.03595v1
Variational Bayes Gaussian Splatting,"Toon Van de Maele, Ozan Catal, Alexander Tschantz, Christopher L. Buckley, Tim Verbelen",2024-10-04,"Recently, 3D Gaussian Splatting has emerged as a promising approach for
modeling 3D scenes using mixtures of Gaussians. The predominant optimization
method for these models relies on backpropagating gradients through a
differentiable rendering pipeline, which struggles with catastrophic forgetting
when dealing with continuous streams of data. To address this limitation, we
propose Variational Bayes Gaussian Splatting (VBGS), a novel approach that
frames training a Gaussian splat as variational inference over model
parameters. By leveraging the conjugacy properties of multivariate Gaussians,
we derive a closed-form variational update rule, allowing efficient updates
from partial, sequential observations without the need for replay buffers. Our
experiments show that VBGS not only matches state-of-the-art performance on
static datasets, but also enables continual learning from sequentially streamed
2D and 3D data, drastically improving performance in this setting.",http://arxiv.org/pdf/2410.03592v1
A Multi-model Approach for Video Data Retrieval in Autonomous Vehicle Development,"Jesper Knapp, Klas Moberg, Yuchuan Jin, Simin Sun, Miroslaw Staron",2024-10-04,"Autonomous driving software generates enormous amounts of data every second,
which software development organizations save for future analysis and testing
in the form of logs. However, given the vast size of this data, locating
specific scenarios within a collection of vehicle logs can be challenging.
Writing the correct SQL queries to find these scenarios requires engineers to
have a strong background in SQL and the specific databases in question, further
complicating the search process. This paper presents and evaluates a pipeline
that allows searching for specific scenarios in log collections using natural
language descriptions instead of SQL. The generated descriptions were evaluated
by engineers working with vehicle logs at the Zenseact on a scale from 1 to 5.
Our approach achieved a mean score of 3.3, demonstrating the potential of using
a multi-model architecture to improve the software development workflow. We
also present an interface that can visualize the query process and visualize
the results.",http://arxiv.org/pdf/2410.03580v1
Training on more Reachable Tasks for Generalisation in Reinforcement Learning,"Max Weltevrede, Caroline Horsch, Matthijs T. J. Spaan, Wendelin Böhmer",2024-10-04,"In multi-task reinforcement learning, agents train on a fixed set of tasks
and have to generalise to new ones. Recent work has shown that increased
exploration improves this generalisation, but it remains unclear why exactly
that is. In this paper, we introduce the concept of reachability in multi-task
reinforcement learning and show that an initial exploration phase increases the
number of reachable tasks the agent is trained on. This, and not the increased
exploration, is responsible for the improved generalisation, even to
unreachable tasks. Inspired by this, we propose a novel method Explore-Go that
implements such an exploration phase at the beginning of each episode.
Explore-Go only modifies the way experience is collected and can be used with
most existing on-policy or off-policy reinforcement learning algorithms. We
demonstrate the effectiveness of our method when combined with some popular
algorithms and show an increase in generalisation performance across several
environments.",http://arxiv.org/pdf/2410.03565v1
færdXel: An Expert System for Danish Traffic Law,"Luís Cruz-Filipe, Jonas Vistrup",2024-10-04,"We present f{\ae}rdXel, a tool for symbolic reasoning in the domain of Danish
traffic law. f{\ae}rdXel combines techniques from logic programming with a
novel interface that allows users to navigate through its reasoning process,
thereby ensuring the system's trustworthiness. A preliminary empirical
evaluation indicates that this work is seen as very promising, and has the
potential to become a foundation for real-world AI tools supporting
professionals in the Danish legal sector.",http://arxiv.org/pdf/2410.03560v1
Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features,"Benyuan Meng, Qianqian Xu, Zitai Wang, Xiaochun Cao, Qingming Huang",2024-10-04,"Diffusion models are initially designed for image generation. Recent research
shows that the internal signals within their backbones, named activations, can
also serve as dense features for various discriminative tasks such as semantic
segmentation. Given numerous activations, selecting a small yet effective
subset poses a fundamental problem. To this end, the early study of this field
performs a large-scale quantitative comparison of the discriminative ability of
the activations. However, we find that many potential activations have not been
evaluated, such as the queries and keys used to compute attention scores.
Moreover, recent advancements in diffusion architectures bring many new
activations, such as those within embedded ViT modules. Both combined,
activation selection remains unresolved but overlooked. To tackle this issue,
this paper takes a further step with a much broader range of activations
evaluated. Considering the significant increase in activations, a full-scale
quantitative comparison is no longer operational. Instead, we seek to
understand the properties of these activations, such that the activations that
are clearly inferior can be filtered out in advance via simple qualitative
evaluation. After careful analysis, we discover three properties universal
among diffusion models, enabling this study to go beyond specific models. On
top of this, we present effective feature selection solutions for several
popular diffusion models. Finally, the experiments across multiple
discriminative tasks validate the superiority of our method over the SOTA
competitors. Our code is available at
https://github.com/Darkbblue/generic-diffusion-feature.",http://arxiv.org/pdf/2410.03558v1
Ward: Provable RAG Dataset Inference via LLM Watermarks,"Nikola Jovanović, Robin Staab, Maximilian Baader, Martin Vechev",2024-10-04,"Retrieval-Augmented Generation (RAG) improves LLMs by enabling them to
incorporate external data during generation. This raises concerns for data
owners regarding unauthorized use of their content in RAG systems. Despite its
importance, the challenge of detecting such unauthorized usage remains
underexplored, with existing datasets and methodologies from adjacent fields
being ill-suited for its study. In this work, we take several steps to bridge
this gap. First, we formalize this problem as (black-box) RAG Dataset Inference
(RAG-DI). To facilitate research on this challenge, we further introduce a
novel dataset specifically designed for benchmarking RAG-DI methods under
realistic conditions, and propose a set of baseline approaches. Building on
this foundation, we introduce Ward, a RAG-DI method based on LLM watermarks
that enables data owners to obtain rigorous statistical guarantees regarding
the usage of their dataset in a RAG system. In our experimental evaluation, we
show that Ward consistently outperforms all baselines across many challenging
settings, achieving higher accuracy, superior query efficiency and robustness.
Our work provides a foundation for future studies of RAG-DI and highlights LLM
watermarks as a promising approach to this problem.",http://arxiv.org/pdf/2410.03537v1
MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction,"Han Jiang, Junwen Duan, Zhe Qu, Jianxin Wang",2024-10-04,"Unsupervised rationale extraction aims to extract text snippets to support
model predictions without explicit rationale annotation. Researchers have made
many efforts to solve this task. Previous works often encode each aspect
independently, which may limit their ability to capture meaningful internal
correlations between aspects. While there has been significant work on
mitigating spurious correlations, our approach focuses on leveraging the
beneficial internal correlations to improve multi-aspect rationale extraction.
In this paper, we propose a Multi-Aspect Rationale Extractor (MARE) to explain
and predict multiple aspects simultaneously. Concretely, we propose a
Multi-Aspect Multi-Head Attention (MAMHA) mechanism based on hard deletion to
encode multiple text chunks simultaneously. Furthermore, multiple special
tokens are prepended in front of the text with each corresponding to one
certain aspect. Finally, multi-task training is deployed to reduce the training
overhead. Experimental results on two unsupervised rationale extraction
benchmarks show that MARE achieves state-of-the-art performance. Ablation
studies further demonstrate the effectiveness of our method. Our codes have
been available at https://github.com/CSU-NLP-Group/MARE.",http://arxiv.org/pdf/2410.03531v1
A Probabilistic Perspective on Unlearning and Alignment for Large Language Models,"Yan Scholten, Stephan Günnemann, Leo Schwinn",2024-10-04,"Comprehensive evaluation of Large Language Models (LLMs) is an open research
problem. Existing evaluations rely on deterministic point estimates generated
via greedy decoding. However, we find that deterministic evaluations fail to
capture the whole output distribution of a model, yielding inaccurate
estimations of model capabilities. This is particularly problematic in critical
contexts such as unlearning and alignment, where precise model evaluations are
crucial. To remedy this, we introduce the first formal probabilistic evaluation
framework in LLMs. Namely, we derive novel metrics with high-probability
guarantees concerning the output distribution of a model. Our metrics are
application-independent and allow practitioners to make more reliable estimates
about model capabilities before deployment. Through a case study focused on
unlearning, we reveal that deterministic evaluations falsely indicate
successful unlearning, whereas our probabilistic evaluations demonstrate that
most if not all of the supposedly unlearned information remains accessible in
these models. Additionally, we propose a novel unlearning loss based on entropy
optimization and adaptive temperature scaling, which significantly improves
unlearning in probabilistic settings on recent benchmarks. Our proposed shift
from point estimates to probabilistic evaluations of output distributions
represents an important step toward comprehensive evaluations of LLMs.
https://github.com/yascho/probabilistic-unlearning",http://arxiv.org/pdf/2410.03523v1
FedStein: Enhancing Multi-Domain Federated Learning Through James-Stein Estimator,"Sunny Gupta, Nikita Jangid, Amit Sethi",2024-10-04,"Federated Learning (FL) facilitates data privacy by enabling collaborative
in-situ training across decentralized clients. Despite its inherent advantages,
FL faces significant challenges of performance and convergence when dealing
with data that is not independently and identically distributed (non-i.i.d.).
While previous research has primarily addressed the issue of skewed label
distribution across clients, this study focuses on the less explored challenge
of multi-domain FL, where client data originates from distinct domains with
varying feature distributions. We introduce a novel method designed to address
these challenges FedStein: Enhancing Multi-Domain Federated Learning Through
the James-Stein Estimator. FedStein uniquely shares only the James-Stein (JS)
estimates of batch normalization (BN) statistics across clients, while
maintaining local BN parameters. The non-BN layer parameters are exchanged via
standard FL techniques. Extensive experiments conducted across three datasets
and multiple models demonstrate that FedStein surpasses existing methods such
as FedAvg and FedBN, with accuracy improvements exceeding 14% in certain
domains leading to enhanced domain generalization. The code is available at
https://github.com/sunnyinAI/FedStein",http://arxiv.org/pdf/2410.03499v1
Generative Artificial Intelligence for Navigating Synthesizable Chemical Space,"Wenhao Gao, Shitong Luo, Connor W. Coley",2024-10-04,"We introduce SynFormer, a generative modeling framework designed to
efficiently explore and navigate synthesizable chemical space. Unlike
traditional molecular generation approaches, we generate synthetic pathways for
molecules to ensure that designs are synthetically tractable. By incorporating
a scalable transformer architecture and a diffusion module for building block
selection, SynFormer surpasses existing models in synthesizable molecular
design. We demonstrate SynFormer's effectiveness in two key applications: (1)
local chemical space exploration, where the model generates synthesizable
analogs of a reference molecule, and (2) global chemical space exploration,
where the model aims to identify optimal molecules according to a black-box
property prediction oracle. Additionally, we demonstrate the scalability of our
approach via the improvement in performance as more computational resources
become available. With our code and trained models openly available, we hope
that SynFormer will find use across applications in drug discovery and
materials science.",http://arxiv.org/pdf/2410.03494v1
Gradient-based Jailbreak Images for Multimodal Fusion Models,"Javier Rando, Hannah Korevaar, Erik Brinkman, Ivan Evtimov, Florian Tramèr",2024-10-04,"Augmenting language models with image inputs may enable more effective
jailbreak attacks through continuous optimization, unlike text inputs that
require discrete optimization. However, new multimodal fusion models tokenize
all input modalities using non-differentiable functions, which hinders
straightforward attacks. In this work, we introduce the notion of a tokenizer
shortcut that approximates tokenization with a continuous function and enables
continuous optimization. We use tokenizer shortcuts to create the first
end-to-end gradient image attacks against multimodal fusion models. We evaluate
our attacks on Chameleon models and obtain jailbreak images that elicit harmful
information for 72.5% of prompts. Jailbreak images outperform text jailbreaks
optimized with the same objective and require 3x lower compute budget to
optimize 50x more input tokens. Finally, we find that representation
engineering defenses, like Circuit Breakers, trained only on text attacks can
effectively transfer to adversarial image inputs.",http://arxiv.org/pdf/2410.03489v1
A Multimodal Framework for Deepfake Detection,"Kashish Gandhi, Prutha Kulkarni, Taran Shah, Piyush Chaudhari, Meera Narvekar, Kranti Ghag",2024-10-04,"The rapid advancement of deepfake technology poses a significant threat to
digital media integrity. Deepfakes, synthetic media created using AI, can
convincingly alter videos and audio to misrepresent reality. This creates risks
of misinformation, fraud, and severe implications for personal privacy and
security. Our research addresses the critical issue of deepfakes through an
innovative multimodal approach, targeting both visual and auditory elements.
This comprehensive strategy recognizes that human perception integrates
multiple sensory inputs, particularly visual and auditory information, to form
a complete understanding of media content. For visual analysis, a model that
employs advanced feature extraction techniques was developed, extracting nine
distinct facial characteristics and then applying various machine learning and
deep learning models. For auditory analysis, our model leverages
mel-spectrogram analysis for feature extraction and then applies various
machine learning and deep learningmodels. To achieve a combined analysis, real
and deepfake audio in the original dataset were swapped for testing purposes
and ensured balanced samples. Using our proposed models for video and audio
classification i.e. Artificial Neural Network and VGG19, the overall sample is
classified as deepfake if either component is identified as such. Our
multimodal framework combines visual and auditory analyses, yielding an
accuracy of 94%.",http://arxiv.org/pdf/2410.03487v1
Group Fairness in Peer Review,"Haris Aziz, Evi Micha, Nisarg Shah",2024-10-04,"Large conferences such as NeurIPS and AAAI serve as crossroads of various AI
fields, since they attract submissions from a vast number of communities.
However, in some cases, this has resulted in a poor reviewing experience for
some communities, whose submissions get assigned to less qualified reviewers
outside of their communities. An often-advocated solution is to break up any
such large conference into smaller conferences, but this can lead to isolation
of communities and harm interdisciplinary research. We tackle this challenge by
introducing a notion of group fairness, called the core, which requires that
every possible community (subset of researchers) to be treated in a way that
prevents them from unilaterally benefiting by withdrawing from a large
conference.
  We study a simple peer review model, prove that it always admits a reviewing
assignment in the core, and design an efficient algorithm to find one such
assignment. We use real data from CVPR and ICLR conferences to compare our
algorithm to existing reviewing assignment algorithms on a number of metrics.",http://arxiv.org/pdf/2410.03474v1
Vulnerability Detection via Topological Analysis of Attention Maps,"Pavel Snopov, Andrey Nikolaevich Golubinskiy",2024-10-04,"Recently, deep learning (DL) approaches to vulnerability detection have
gained significant traction. These methods demonstrate promising results, often
surpassing traditional static code analysis tools in effectiveness.
  In this study, we explore a novel approach to vulnerability detection
utilizing the tools from topological data analysis (TDA) on the attention
matrices of the BERT model. Our findings reveal that traditional machine
learning (ML) techniques, when trained on the topological features extracted
from these attention matrices, can perform competitively with pre-trained
language models (LLMs) such as CodeBERTa. This suggests that TDA tools,
including persistent homology, are capable of effectively capturing semantic
information critical for identifying vulnerabilities.",http://arxiv.org/pdf/2410.03470v1
Diffusion State-Guided Projected Gradient for Inverse Problems,"Rayhan Zirvi, Bahareh Tolooshams, Anima Anandkumar",2024-10-04,"Recent advancements in diffusion models have been effective in learning data
priors for solving inverse problems. They leverage diffusion sampling steps for
inducing a data prior while using a measurement guidance gradient at each step
to impose data consistency. For general inverse problems, approximations are
needed when an unconditionally trained diffusion model is used since the
measurement likelihood is intractable, leading to inaccurate posterior
sampling. In other words, due to their approximations, these methods fail to
preserve the generation process on the data manifold defined by the diffusion
prior, leading to artifacts in applications such as image restoration. To
enhance the performance and robustness of diffusion models in solving inverse
problems, we propose Diffusion State-Guided Projected Gradient (DiffStateGrad),
which projects the measurement gradient onto a subspace that is a low-rank
approximation of an intermediate state of the diffusion process. DiffStateGrad,
as a module, can be added to a wide range of diffusion-based inverse solvers to
improve the preservation of the diffusion process on the prior manifold and
filter out artifact-inducing components. We highlight that DiffStateGrad
improves the robustness of diffusion models in terms of the choice of
measurement guidance step size and noise while improving the worst-case
performance. Finally, we demonstrate that DiffStateGrad improves upon the
state-of-the-art on linear and nonlinear image restoration inverse problems.",http://arxiv.org/pdf/2410.03463v1
How Toxicity Classifiers and Large Language Models Respond to Ableism,"Mahika Phutane, Ananya Seelam, Aditya Vashistha",2024-10-04,"People with disabilities (PwD) regularly encounter ableist hate and
microaggressions online. While online platforms use machine learning models to
moderate online harm, there is little research investigating how these models
interact with ableism. In this paper, we curated a dataset of 100 social media
comments targeted towards PwD, and recruited 160 participants to rate and
explain how toxic and ableist these comments were. We then prompted
state-of-the art toxicity classifiers (TCs) and large language models (LLMs) to
rate and explain the harm. Our analysis revealed that TCs and LLMs rated
toxicity significantly lower than PwD, but LLMs rated ableism generally on par
with PwD. However, ableism explanations by LLMs overlooked emotional harm, and
lacked specificity and acknowledgement of context, important facets of PwD
explanations. Going forward, we discuss challenges in designing
disability-aware toxicity classifiers, and advocate for the shift from ableism
detection to ableism interpretation and explanation.",http://arxiv.org/pdf/2410.03448v1
On Uncertainty In Natural Language Processing,Dennis Ulmer,2024-10-04,"The last decade in deep learning has brought on increasingly capable systems
that are deployed on a wide variety of applications. In natural language
processing, the field has been transformed by a number of breakthroughs
including large language models, which are used in increasingly many
user-facing applications. In order to reap the benefits of this technology and
reduce potential harms, it is important to quantify the reliability of model
predictions and the uncertainties that shroud their development.
  This thesis studies how uncertainty in natural language processing can be
characterized from a linguistic, statistical and neural perspective, and how it
can be reduced and quantified through the design of the experimental pipeline.
We further explore uncertainty quantification in modeling by theoretically and
empirically investigating the effect of inductive model biases in text
classification tasks. The corresponding experiments include data for three
different languages (Danish, English and Finnish) and tasks as well as a large
set of different uncertainty quantification approaches. Additionally, we
propose a method for calibrated sampling in natural language generation based
on non-exchangeable conformal prediction, which provides tighter token sets
with better coverage of the actual continuation. Lastly, we develop an approach
to quantify confidence in large black-box language models using auxiliary
predictors, where the confidence is predicted from the input to and generated
output text of the target model alone.",http://arxiv.org/pdf/2410.03446v1
Exploring the Benefit of Activation Sparsity in Pre-training,"Zhengyan Zhang, Chaojun Xiao, Qiujieli Qin, Yankai Lin, Zhiyuan Zeng, Xu Han, Zhiyuan Liu, Ruobing Xie, Maosong Sun, Jie Zhou",2024-10-04,"Pre-trained Transformers inherently possess the characteristic of sparse
activation, where only a small fraction of the neurons are activated for each
token. While sparse activation has been explored through post-training methods,
its potential in pre-training remains untapped. In this work, we first study
how activation properties change during pre-training. Our examination reveals
that Transformers exhibit sparse activation throughout the majority of the
pre-training process while the activation correlation keeps evolving as
training progresses. Leveraging this observation, we propose Switchable
Sparse-Dense Learning (SSD). SSD adaptively switches between the
Mixtures-of-Experts (MoE) based sparse training and the conventional dense
training during the pre-training process, leveraging the efficiency of sparse
training and avoiding the static activation correlation of sparse training.
Compared to dense training, SSD achieves comparable performance with identical
model size and reduces pre-training costs. Moreover, the models trained with
SSD can be directly used as MoE models for sparse inference and achieve the
same performance as dense models with up to $2\times$ faster inference speed.
Codes are available at https://github.com/thunlp/moefication.",http://arxiv.org/pdf/2410.03440v1
A General Framework for Producing Interpretable Semantic Text Embeddings,"Yiqun Sun, Qiang Huang, Yixuan Tang, Anthony K. H. Tung, Jun Yu",2024-10-04,"Semantic text embedding is essential to many tasks in Natural Language
Processing (NLP). While black-box models are capable of generating high-quality
embeddings, their lack of interpretability limits their use in tasks that
demand transparency. Recent approaches have improved interpretability by
leveraging domain-expert-crafted or LLM-generated questions, but these methods
rely heavily on expert input or well-prompt design, which restricts their
generalizability and ability to generate discriminative questions across a wide
range of tasks. To address these challenges, we introduce \algo{CQG-MBQA}
(Contrastive Question Generation - Multi-task Binary Question Answering), a
general framework for producing interpretable semantic text embeddings across
diverse tasks. Our framework systematically generates highly discriminative,
low cognitive load yes/no questions through the \algo{CQG} method and answers
them efficiently with the \algo{MBQA} model, resulting in interpretable
embeddings in a cost-effective manner. We validate the effectiveness and
interpretability of \algo{CQG-MBQA} through extensive experiments and ablation
studies, demonstrating that it delivers embedding quality comparable to many
advanced black-box models while maintaining inherently interpretability.
Additionally, \algo{CQG-MBQA} outperforms other interpretable text embedding
methods across various downstream tasks.",http://arxiv.org/pdf/2410.03435v1
Self-supervised Spatio-Temporal Graph Mask-Passing Attention Network for Perceptual Importance Prediction of Multi-point Tactility,"Dazhong He, Qian Liu",2024-10-04,"While visual and auditory information are prevalent in modern multimedia
systems, haptic interaction, e.g., tactile and kinesthetic interaction,
provides a unique form of human perception. However, multimedia technology for
contact interaction is less mature than non-contact multimedia technologies and
requires further development. Specialized haptic media technologies, requiring
low latency and bitrates, are essential to enable haptic interaction,
necessitating haptic information compression. Existing vibrotactile signal
compression methods, based on the perceptual model, do not consider the
characteristics of fused tactile perception at multiple spatially distributed
interaction points. In fact, differences in tactile perceptual importance are
not limited to conventional frequency and time domains, but also encompass
differences in the spatial locations on the skin unique to tactile perception.
For the most frequently used tactile information, vibrotactile texture
perception, we have developed a model to predict its perceptual importance at
multiple points, based on self-supervised learning and Spatio-Temporal Graph
Neural Network. Current experimental results indicate that this model can
effectively predict the perceptual importance of various points in multi-point
tactile perception scenarios.",http://arxiv.org/pdf/2410.03434v1
EB-NeRD: A Large-Scale Dataset for News Recommendation,"Johannes Kruse, Kasper Lindskow, Saikishore Kalloori, Marco Polignano, Claudio Pomo, Abhishek Srivastava, Anshuk Uppal, Michael Riis Andersen, Jes Frellsen",2024-10-04,"Personalized content recommendations have been pivotal to the content
experience in digital media from video streaming to social networks. However,
several domain specific challenges have held back adoption of recommender
systems in news publishing. To address these challenges, we introduce the
Ekstra Bladet News Recommendation Dataset (EB-NeRD). The dataset encompasses
data from over a million unique users and more than 37 million impression logs
from Ekstra Bladet. It also includes a collection of over 125,000 Danish news
articles, complete with titles, abstracts, bodies, and metadata, such as
categories. EB-NeRD served as the benchmark dataset for the RecSys '24
Challenge, where it was demonstrated how the dataset can be used to address
both technical and normative challenges in designing effective and responsible
recommender systems for news publishing. The dataset is available at:
https://recsys.eb.dk.",http://arxiv.org/pdf/2410.03432v1
Cayley Graph Propagation,"JJ Wilson, Maya Bechler-Speicher, Petar Veličković",2024-10-04,"In spite of the plethora of success stories with graph neural networks (GNNs)
on modelling graph-structured data, they are notoriously vulnerable to
over-squashing, whereby tasks necessitate the mixing of information between
distance pairs of nodes. To address this problem, prior work suggests rewiring
the graph structure to improve information flow. Alternatively, a significant
body of research has dedicated itself to discovering and precomputing
bottleneck-free graph structures to ameliorate over-squashing. One well
regarded family of bottleneck-free graphs within the mathematical community are
expander graphs, with prior work$\unicode{x2014}$Expander Graph Propagation
(EGP)$\unicode{x2014}$proposing the use of a well-known expander graph
family$\unicode{x2014}$the Cayley graphs of the $\mathrm{SL}(2,\mathbb{Z}_n)$
special linear group$\unicode{x2014}$as a computational template for GNNs.
However, in EGP the computational graphs used are truncated to align with a
given input graph. In this work, we show that truncation is detrimental to the
coveted expansion properties. Instead, we propose CGP, a method to propagate
information over a complete Cayley graph structure, thereby ensuring it is
bottleneck-free to better alleviate over-squashing. Our empirical evidence
across several real-world datasets not only shows that CGP recovers significant
improvements as compared to EGP, but it is also akin to or outperforms
computationally complex graph rewiring techniques.",http://arxiv.org/pdf/2410.03424v1
One2set + Large Language Model: Best Partners for Keyphrase Generation,"Liangying Shao, Liang Zhang, Minlong Peng, Guoqi Ma, Hao Yue, Mingming Sun, Jinsong Su",2024-10-04,"Keyphrase generation (KPG) aims to automatically generate a collection of
phrases representing the core concepts of a given document. The dominant
paradigms in KPG include one2seq and one2set. Recently, there has been
increasing interest in applying large language models (LLMs) to KPG. Our
preliminary experiments reveal that it is challenging for a single model to
excel in both recall and precision. Further analysis shows that: 1) the one2set
paradigm owns the advantage of high recall, but suffers from improper
assignments of supervision signals during training; 2) LLMs are powerful in
keyphrase selection, but existing selection methods often make redundant
selections. Given these observations, we introduce a generate-then-select
framework decomposing KPG into two steps, where we adopt a one2set-based model
as generator to produce candidates and then use an LLM as selector to select
keyphrases from these candidates. Particularly, we make two important
improvements on our generator and selector: 1) we design an Optimal
Transport-based assignment strategy to address the above improper assignments;
2) we model the keyphrase selection as a sequence labeling task to alleviate
redundant selections. Experimental results on multiple benchmark datasets show
that our framework significantly surpasses state-of-the-art models, especially
in absent keyphrase prediction.",http://arxiv.org/pdf/2410.03421v1
Towards Real-time Intrahepatic Vessel Identification in Intraoperative Ultrasound-Guided Liver Surgery,"Karl-Philippe Beaudet, Alexandros Karargyris, Sidaty El Hadramy, Stéphane Cotin, Jean-Paul Mazellier, Nicolas Padoy, Juan Verde",2024-10-04,"While laparoscopic liver resection is less prone to complications and
maintains patient outcomes compared to traditional open surgery, its complexity
hinders widespread adoption due to challenges in representing the liver's
internal structure. Laparoscopic intraoperative ultrasound offers efficient,
cost-effective and radiation-free guidance. Our objective is to aid physicians
in identifying internal liver structures using laparoscopic intraoperative
ultrasound. We propose a patient-specific approach using preoperative 3D
ultrasound liver volume to train a deep learning model for real-time
identification of portal tree and branch structures. Our personalized AI model,
validated on ex vivo swine livers, achieved superior precision (0.95) and
recall (0.93) compared to surgeons, laying groundwork for precise vessel
identification in ultrasound-based liver resection. Its adaptability and
potential clinical impact promise to advance surgical interventions and improve
patient care.",http://arxiv.org/pdf/2410.03420v1
Comparative study of regression vs pairwise models for surrogate-based heuristic optimisation,"Pablo S. Naharro, Pablo Toharia, Antonio LaTorre, José-María Peña",2024-10-04,"Heuristic optimisation algorithms explore the search space by sampling
solutions, evaluating their fitness, and biasing the search in the direction of
promising solutions. However, in many cases, this fitness function involves
executing expensive computational calculations, drastically reducing the
reasonable number of evaluations. In this context, surrogate models have
emerged as an excellent alternative to alleviate these computational problems.
This paper addresses the formulation of surrogate problems as both regression
models that approximate fitness (surface surrogate models) and a novel way to
connect classification models (pairwise surrogate models). The pairwise
approach can be directly exploited by some algorithms, such as Differential
Evolution, in which the fitness value is not actually needed to drive the
search, and it is sufficient to know whether a solution is better than another
one or not. Based on these modelling approaches, we have conducted a
multidimensional analysis of surrogate models under different configurations:
different machine learning algorithms (regularised regression, neural networks,
decision trees, boosting methods, and random forests), different surrogate
strategies (encouraging diversity or relaxing prediction thresholds), and
compare them for both surface and pairwise surrogate models. The experimental
part of the article includes the benchmark problems already proposed for the
SOCO2011 competition in continuous optimisation and a simulation problem
included in the recent GECCO2021 Industrial Challenge. This paper shows that
the performance of the overall search, when using online machine learning-based
surrogate models, depends not only on the accuracy of the predictive model but
also on both the kind of bias towards positive or negative cases and how the
optimisation uses those predictions to decide whether to execute the actual
fitness function.",http://arxiv.org/pdf/2410.03409v1
EBES: Easy Benchmarking for Event Sequences,"Dmitry Osin, Igor Udovichenko, Viktor Moskvoretskii, Egor Shvetsov, Evgeny Burnaev",2024-10-04,"Event sequences, characterized by irregular sampling intervals and a mix of
categorical and numerical features, are common data structures in various
real-world domains such as healthcare, finance, and user interaction logs.
Despite advances in temporal data modeling techniques, there is no standardized
benchmarks for evaluating their performance on event sequences. This
complicates result comparison across different papers due to varying evaluation
protocols, potentially misleading progress in this field. We introduce EBES, a
comprehensive benchmarking tool with standardized evaluation scenarios and
protocols, focusing on regression and classification problems with
sequence-level targets. Our library simplifies benchmarking, dataset addition,
and method integration through a unified interface. It includes a novel
synthetic dataset and provides preprocessed real-world datasets, including the
largest publicly available banking dataset. Our results provide an in-depth
analysis of datasets, identifying some as unsuitable for model comparison. We
investigate the importance of modeling temporal and sequential components, as
well as the robustness and scaling properties of the models. These findings
highlight potential directions for future research. Our benchmark aim is to
facilitate reproducible research, expediting progress and increasing real-world
impacts.",http://arxiv.org/pdf/2410.03399v1
GraphCroc: Cross-Correlation Autoencoder for Graph Structural Reconstruction,"Shijin Duan, Ruyi Ding, Jiaxing He, Aidong Adam Ding, Yunsi Fei, Xiaolin Xu",2024-10-04,"Graph-structured data is integral to many applications, prompting the
development of various graph representation methods. Graph autoencoders (GAEs),
in particular, reconstruct graph structures from node embeddings. Current GAE
models primarily utilize self-correlation to represent graph structures and
focus on node-level tasks, often overlooking multi-graph scenarios. Our
theoretical analysis indicates that self-correlation generally falls short in
accurately representing specific graph features such as islands, symmetrical
structures, and directional edges, particularly in smaller or multiple graph
contexts. To address these limitations, we introduce a cross-correlation
mechanism that significantly enhances the GAE representational capabilities.
Additionally, we propose GraphCroc, a new GAE that supports flexible encoder
architectures tailored for various downstream tasks and ensures robust
structural reconstruction, through a mirrored encoding-decoding process. This
model also tackles the challenge of representation bias during optimization by
implementing a loss-balancing strategy. Both theoretical analysis and numerical
evaluations demonstrate that our methodology significantly outperforms existing
self-correlation-based GAEs in graph structure reconstruction.",http://arxiv.org/pdf/2410.03396v1
Predicting perturbation targets with causal differential networks,"Menghua Wu, Umesh Padia, Sean H. Murphy, Regina Barzilay, Tommi Jaakkola",2024-10-04,"Rationally identifying variables responsible for changes to a biological
system can enable myriad applications in disease understanding and cell
engineering. From a causality perspective, we are given two datasets generated
by the same causal model, one observational (control) and one interventional
(perturbed). The goal is to isolate the subset of measured variables (e.g.
genes) that were the targets of the intervention, i.e. those whose conditional
independencies have changed. Knowing the causal graph would limit the search
space, allowing us to efficiently pinpoint these variables. However, current
algorithms that infer causal graphs in the presence of unknown intervention
targets scale poorly to the hundreds or thousands of variables in biological
data, as they must jointly search the combinatorial spaces of graphs and
consistent intervention targets. In this work, we propose a causality-inspired
approach for predicting perturbation targets that decouples the two search
steps. First, we use an amortized causal discovery model to separately infer
causal graphs from the observational and interventional datasets. Then, we
learn to map these paired graphs to the sets of variables that were intervened
upon, in a supervised learning framework. This approach consistently
outperforms baselines for perturbation modeling on seven single-cell
transcriptomics datasets, each with thousands of measured variables. We also
demonstrate significant improvements over six causal discovery algorithms in
predicting intervention targets across a variety of tractable, synthetic
datasets.",http://arxiv.org/pdf/2410.03380v1
Mitigating Adversarial Perturbations for Deep Reinforcement Learning via Vector Quantization,"Tung M. Luu, Thanh Nguyen, Tee Joshua Tian Jin, Sungwoon Kim, Chang D. Yoo",2024-10-04,"Recent studies reveal that well-performing reinforcement learning (RL) agents
in training often lack resilience against adversarial perturbations during
deployment. This highlights the importance of building a robust agent before
deploying it in the real world. Most prior works focus on developing robust
training-based procedures to tackle this problem, including enhancing the
robustness of the deep neural network component itself or adversarially
training the agent on strong attacks. In this work, we instead study an input
transformation-based defense for RL. Specifically, we propose using a variant
of vector quantization (VQ) as a transformation for input observations, which
is then used to reduce the space of adversarial attacks during testing,
resulting in the transformed observations being less affected by attacks. Our
method is computationally efficient and seamlessly integrates with adversarial
training, further enhancing the robustness of RL agents against adversarial
attacks. Through extensive experiments in multiple environments, we demonstrate
that using VQ as the input transformation effectively defends against
adversarial attacks on the agent's observations.",http://arxiv.org/pdf/2410.03376v1
SoundSignature: What Type of Music Do You Like?,"Brandon James Carone, Pablo Ripollés",2024-10-04,"SoundSignature is a music application that integrates a custom OpenAI
Assistant to analyze users' favorite songs. The system incorporates
state-of-the-art Music Information Retrieval (MIR) Python packages to combine
extracted acoustic/musical features with the assistant's extensive knowledge of
the artists and bands. Capitalizing on this combined knowledge, SoundSignature
leverages semantic audio and principles from the emerging Internet of Sounds
(IoS) ecosystem, integrating MIR with AI to provide users with personalized
insights into the acoustic properties of their music, akin to a musical
preference personality report. Users can then interact with the chatbot to
explore deeper inquiries about the acoustic analyses performed and how they
relate to their musical taste. This interactivity transforms the application,
acting not only as an informative resource about familiar and/or favorite
songs, but also as an educational platform that enables users to deepen their
understanding of musical features, music theory, acoustic properties commonly
used in signal processing, and the artists behind the music. Beyond general
usability, the application also incorporates several well-established
open-source musician-specific tools, such as a chord recognition algorithm
(CREMA), a source separation algorithm (DEMUCS), and an audio-to-MIDI converter
(basic-pitch). These features allow users without coding skills to access
advanced, open-source music processing algorithms simply by interacting with
the chatbot (e.g., can you give me the stems of this song?). In this paper, we
highlight the application's innovative features and educational potential, and
present findings from a pilot user study that evaluates its efficacy and
usability.",http://arxiv.org/pdf/2410.03375v1
Make Interval Bound Propagation great again,"Patryk Krukowski, Daniel Wilczak, Jacek Tabor, Anna Bielawska, Przemysław Spurek",2024-10-04,"In various scenarios motivated by real life, such as medical data analysis,
autonomous driving, and adversarial training, we are interested in robust deep
networks. A network is robust when a relatively small perturbation of the input
cannot lead to drastic changes in output (like change of class, etc.). This
falls under the broader scope field of Neural Network Certification (NNC). Two
crucial problems in NNC are of profound interest to the scientific community:
how to calculate the robustness of a given pre-trained network and how to
construct robust networks. The common approach to constructing robust networks
is Interval Bound Propagation (IBP). This paper demonstrates that IBP is
sub-optimal in the first case due to its susceptibility to the wrapping effect.
Even for linear activation, IBP gives strongly sub-optimal bounds.
Consequently, one should use strategies immune to the wrapping effect to obtain
bounds close to optimal ones. We adapt two classical approaches dedicated to
strict computations -- Dubleton Arithmetic and Affine Arithmetic -- to mitigate
the wrapping effect in neural networks. These techniques yield precise results
for networks with linear activation functions, thus resisting the wrapping
effect. As a result, we achieve bounds significantly closer to the optimal
level than IBPs.",http://arxiv.org/pdf/2410.03373v1
An Enhanced Harmonic Densely Connected Hybrid Transformer Network Architecture for Chronic Wound Segmentation Utilising Multi-Colour Space Tensor Merging,"Bill Cassidy, Christian Mcbride, Connah Kendrick, Neil D. Reeves, Joseph M. Pappachan, Cornelius J. Fernandez, Elias Chacko, Raphael Brüngel, Christoph M. Friedrich, Metib Alotaibi, Abdullah Abdulaziz AlWabel, Mohammad Alderwish, Kuan-Ying Lai, Moi Hoon Yap",2024-10-04,"Chronic wounds and associated complications present ever growing burdens for
clinics and hospitals world wide. Venous, arterial, diabetic, and pressure
wounds are becoming increasingly common globally. These conditions can result
in highly debilitating repercussions for those affected, with limb amputations
and increased mortality risk resulting from infection becoming more common. New
methods to assist clinicians in chronic wound care are therefore vital to
maintain high quality care standards. This paper presents an improved HarDNet
segmentation architecture which integrates a contrast-eliminating component in
the initial layers of the network to enhance feature learning. We also utilise
a multi-colour space tensor merging process and adjust the harmonic shape of
the convolution blocks to facilitate these additional features. We train our
proposed model using wound images from light-skinned patients and test the
model on two test sets (one set with ground truth, and one without) comprising
only darker-skinned cases. Subjective ratings are obtained from clinical wound
experts with intraclass correlation coefficient used to determine inter-rater
reliability. For the dark-skin tone test set with ground truth, we demonstrate
improvements in terms of Dice similarity coefficient (+0.1221) and intersection
over union (+0.1274). Qualitative analysis showed high expert ratings, with
improvements of >3% demonstrated when comparing the baseline model with the
proposed model. This paper presents the first study to focus on darker-skin
tones for chronic wound segmentation using models trained only on wound images
exhibiting lighter skin. Diabetes is highly prevalent in countries where
patients have darker skin tones, highlighting the need for a greater focus on
such cases. Additionally, we conduct the largest qualitative study to date for
chronic wound segmentation.",http://arxiv.org/pdf/2410.03359v1
LANTERN: Accelerating Visual Autoregressive Models with Relaxed Speculative Decoding,"Doohyuk Jang, Sihwan Park, June Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, Sung-Yub Kim, Eunho Yang",2024-10-04,"Auto-Regressive (AR) models have recently gained prominence in image
generation, often matching or even surpassing the performance of diffusion
models. However, one major limitation of AR models is their sequential nature,
which processes tokens one at a time, slowing down generation compared to
models like GANs or diffusion-based methods that operate more efficiently.
While speculative decoding has proven effective for accelerating LLMs by
generating multiple tokens in a single forward, its application in visual AR
models remains largely unexplored. In this work, we identify a challenge in
this setting, which we term \textit{token selection ambiguity}, wherein visual
AR models frequently assign uniformly low probabilities to tokens, hampering
the performance of speculative decoding. To overcome this challenge, we propose
a relaxed acceptance condition referred to as LANTERN that leverages the
interchangeability of tokens in latent space. This relaxation restores the
effectiveness of speculative decoding in visual AR models by enabling more
flexible use of candidate tokens that would otherwise be prematurely rejected.
Furthermore, by incorporating a total variation distance bound, we ensure that
these speed gains are achieved without significantly compromising image quality
or semantic coherence. Experimental results demonstrate the efficacy of our
method in providing a substantial speed-up over speculative decoding. In
specific, compared to a na\""ive application of the state-of-the-art speculative
decoding, LANTERN increases speed-ups by $\mathbf{1.75}\times$ and
$\mathbf{1.76}\times$, as compared to greedy decoding and random sampling,
respectively, when applied to LlamaGen, a contemporary visual AR model.",http://arxiv.org/pdf/2410.03355v1
An X-Ray Is Worth 15 Features: Sparse Autoencoders for Interpretable Radiology Report Generation,"Ahmed Abdulaal, Hugo Fry, Nina Montaña-Brown, Ayodeji Ijishakin, Jack Gao, Stephanie Hyland, Daniel C. Alexander, Daniel C. Castro",2024-10-04,"Radiological services are experiencing unprecedented demand, leading to
increased interest in automating radiology report generation. Existing
Vision-Language Models (VLMs) suffer from hallucinations, lack
interpretability, and require expensive fine-tuning. We introduce SAE-Rad,
which uses sparse autoencoders (SAEs) to decompose latent representations from
a pre-trained vision transformer into human-interpretable features. Our hybrid
architecture combines state-of-the-art SAE advancements, achieving accurate
latent reconstructions while maintaining sparsity. Using an off-the-shelf
language model, we distil ground-truth reports into radiological descriptions
for each SAE feature, which we then compile into a full report for each image,
eliminating the need for fine-tuning large models for this task. To the best of
our knowledge, SAE-Rad represents the first instance of using mechanistic
interpretability techniques explicitly for a downstream multi-modal reasoning
task. On the MIMIC-CXR dataset, SAE-Rad achieves competitive radiology-specific
metrics compared to state-of-the-art models while using significantly fewer
computational resources for training. Qualitative analysis reveals that SAE-Rad
learns meaningful visual concepts and generates reports aligning closely with
expert interpretations. Our results suggest that SAEs can enhance multimodal
reasoning in healthcare, providing a more interpretable alternative to existing
VLMs.",http://arxiv.org/pdf/2410.03334v1
Comparative Analysis and Ensemble Enhancement of Leading CNN Architectures for Breast Cancer Classification,"Gary Murphy, Raghubir Singh",2024-10-04,"This study introduces a novel and accurate approach to breast cancer
classification using histopathology images. It systematically compares leading
Convolutional Neural Network (CNN) models across varying image datasets,
identifies their optimal hyperparameters, and ranks them based on
classification efficacy. To maximize classification accuracy for each model we
explore, the effects of data augmentation, alternative fully-connected layers,
model training hyperparameter settings, and, the advantages of retraining
models versus using pre-trained weights. Our methodology includes several
original concepts, including serializing generated datasets to ensure
consistent data conditions across training runs and significantly reducing
training duration. Combined with automated curation of results, this enabled
the exploration of over 2,000 training permutations -- such a comprehensive
comparison is as yet unprecedented. Our findings establish the settings
required to achieve exceptional classification accuracy for standalone CNN
models and rank them by model efficacy. Based on these results, we propose
ensemble architectures that stack three high-performing standalone CNN models
together with diverse classifiers, resulting in improved classification
accuracy. The ability to systematically run so many model permutations to get
the best outcomes gives rise to very high quality results, including 99.75% for
BreakHis x40 and BreakHis x200 and 95.18% for the Bach datasets when split into
train, validation and test datasets. The Bach Online blind challenge, yielded
89% using this approach. Whilst this study is based on breast cancer
histopathology image datasets, the methodology is equally applicable to other
medical image datasets.",http://arxiv.org/pdf/2410.03333v1
Influence-oriented Personalized Federated Learning,"Yue Tan, Guodong Long, Jing Jiang, Chengqi Zhang",2024-10-04,"Traditional federated learning (FL) methods often rely on fixed weighting for
parameter aggregation, neglecting the mutual influence by others. Hence, their
effectiveness in heterogeneous data contexts is limited. To address this
problem, we propose an influence-oriented federated learning framework, namely
FedC^2I, which quantitatively measures Client-level and Class-level Influence
to realize adaptive parameter aggregation for each client. Our core idea is to
explicitly model the inter-client influence within an FL system via the
well-crafted influence vector and influence matrix. The influence vector
quantifies client-level influence, enables clients to selectively acquire
knowledge from others, and guides the aggregation of feature representation
layers. Meanwhile, the influence matrix captures class-level influence in a
more fine-grained manner to achieve personalized classifier aggregation. We
evaluate the performance of FedC^2I against existing federated learning methods
under non-IID settings and the results demonstrate the superiority of our
method.",http://arxiv.org/pdf/2410.03315v1
Comparing zero-shot self-explanations with human rationales in multilingual text classification,"Stephanie Brandl, Oliver Eberle",2024-10-04,"Instruction-tuned LLMs are able to provide an explanation about their output
to users by generating self-explanations that do not require gradient
computations or the application of possibly complex XAI methods. In this paper,
we analyse whether this ability results in a good explanation by evaluating
self-explanations in the form of input rationales with respect to their
plausibility to humans as well as their faithfulness to models. For this, we
apply two text classification tasks: sentiment classification and forced labour
detection. Next to English, we further include Danish and Italian translations
of the sentiment classification task and compare self-explanations to human
annotations for all samples. To allow for direct comparisons, we also compute
post-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and
apply this pipeline to 4 LLMs (Llama2, Llama3, Mistral and Mixtral). Our
results show that self-explanations align more closely with human annotations
compared to LRP, while maintaining a comparable level of faithfulness.",http://arxiv.org/pdf/2410.03296v1
Five Years of COVID-19 Discourse on Instagram: A Labeled Instagram Dataset of Over Half a Million Posts for Multilingual Sentiment Analysis,Nirmalya Thakur,2024-10-04,"The work presented in this paper makes three scientific contributions with a
specific focus on mining and analysis of COVID-19-related posts on Instagram.
First, it presents a multilingual dataset of 500,153 Instagram posts about
COVID-19 published between January 2020 and September 2024. This dataset,
available at https://dx.doi.org/10.21227/d46p-v480, contains Instagram posts in
161 different languages as well as 535,021 distinct hashtags. After the
development of this dataset, multilingual sentiment analysis was performed,
which involved classifying each post as positive, negative, or neutral. The
results of sentiment analysis are presented as a separate attribute in this
dataset. Second, it presents the results of performing sentiment analysis per
year from 2020 to 2024. The findings revealed the trends in sentiment related
to COVID-19 on Instagram since the beginning of the pandemic. For instance,
between 2020 and 2024, the sentiment trends show a notable shift, with positive
sentiment decreasing from 38.35% to 28.69%, while neutral sentiment rising from
44.19% to 58.34%. Finally, the paper also presents findings of
language-specific sentiment analysis. This analysis highlighted similar and
contrasting trends of sentiment across posts published in different languages
on Instagram. For instance, out of all English posts, 49.68% were positive,
14.84% were negative, and 35.48% were neutral. In contrast, among Hindi posts,
4.40% were positive, 57.04% were negative, and 38.56% were neutral, reflecting
distinct differences in the sentiment distribution between these two languages.",http://arxiv.org/pdf/2410.03293v1
Enhanced Transformer architecture for in-context learning of dynamical systems,"Matteo Rufolo, Dario Piga, Gabriele Maroni, Marco Forgione",2024-10-04,"Recently introduced by some of the authors, the in-context identification
paradigm aims at estimating, offline and based on synthetic data, a meta-model
that describes the behavior of a whole class of systems. Once trained, this
meta-model is fed with an observed input/output sequence (context) generated by
a real system to predict its behavior in a zero-shot learning fashion. In this
paper, we enhance the original meta-modeling framework through three key
innovations: by formulating the learning task within a probabilistic framework;
by managing non-contiguous context and query windows; and by adopting recurrent
patching to effectively handle long context sequences. The efficacy of these
modifications is demonstrated through a numerical example focusing on the
Wiener-Hammerstein system class, highlighting the model's enhanced performance
and scalability.",http://arxiv.org/pdf/2410.03291v1
Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models,"Haibo Wang, Zhiyang Xu, Yu Cheng, Shizhe Diao, Yufan Zhou, Yixin Cao, Qifan Wang, Weifeng Ge, Lifu Huang",2024-10-04,"Video Large Language Models (Video-LLMs) have demonstrated remarkable
capabilities in coarse-grained video understanding, however, they struggle with
fine-grained temporal grounding. In this paper, we introduce Grounded-VideoLLM,
a novel Video-LLM adept at perceiving and reasoning over specific video moments
in a fine-grained manner. We identify that current Video-LLMs have limitations
for fine-grained video understanding since they lack effective temporal
modeling and timestamp representation. In light of this, we sharpen our model
by incorporating (1) an additional temporal stream to encode the relationships
between frames and (2) discrete temporal tokens enriched with specific time
knowledge to represent timestamps. To optimize the training of
Grounded-VideoLLM, we employ a multi-stage training scheme, beginning with
simple video-captioning tasks and progressively introducing video temporal
grounding tasks of increasing complexity. To further enhance
Grounded-VideoLLM's temporal reasoning capability, we also curate a grounded
VideoQA dataset by an automatic annotation pipeline. Extensive experiments
demonstrate that Grounded-VideoLLM not only excels in fine-grained grounding
tasks such as temporal sentence grounding, dense video captioning, and grounded
VideoQA, but also shows great potential as a versatile video assistant for
general video understanding.",http://arxiv.org/pdf/2410.03290v1
Manikin-Recorded Cardiopulmonary Sounds Dataset Using Digital Stethoscope,"Yasaman Torabi, Shahram Shirani, James P. Reilly",2024-10-04,"Heart and lung sounds are crucial for healthcare monitoring. Recent
improvements in stethoscope technology have made it possible to capture patient
sounds with enhanced precision. In this dataset, we used a digital stethoscope
to capture both heart and lung sounds, including individual and mixed
recordings. To our knowledge, this is the first dataset to offer both separate
and mixed cardiorespiratory sounds. The recordings were collected from a
clinical manikin, a patient simulator designed to replicate human physiological
conditions, generating clean heart and lung sounds at different body locations.
This dataset includes both normal sounds and various abnormalities (i.e.,
murmur, atrial fibrillation, tachycardia, atrioventricular block, third and
fourth heart sound, wheezing, crackles, rhonchi, pleural rub, and gurgling
sounds). The dataset includes audio recordings of chest examinations performed
at different anatomical locations, as determined by specialist nurses. Each
recording has been enhanced using frequency filters to highlight specific sound
types. This dataset is useful for applications in artificial intelligence, such
as automated cardiopulmonary disease detection, sound classification,
unsupervised separation techniques, and deep learning algorithms related to
audio signal processing.",http://arxiv.org/pdf/2410.03280v1
Test-time Adaptation for Regression by Subspace Alignment,"Kazuki Adachi, Shin'ya Yamaguchi, Atsutoshi Kumagai, Tomoki Hamagami",2024-10-04,"This paper investigates test-time adaptation (TTA) for regression, where a
regression model pre-trained in a source domain is adapted to an unknown target
distribution with unlabeled target data. Although regression is one of the
fundamental tasks in machine learning, most of the existing TTA methods have
classification-specific designs, which assume that models output
class-categorical predictions, whereas regression models typically output only
single scalar values. To enable TTA for regression, we adopt a feature
alignment approach, which aligns the feature distributions between the source
and target domains to mitigate the domain gap. However, we found that naive
feature alignment employed in existing TTA methods for classification is
ineffective or even worse for regression because the features are distributed
in a small subspace and many of the raw feature dimensions have little
significance to the output. For an effective feature alignment in TTA for
regression, we propose Significant-subspace Alignment (SSA). SSA consists of
two components: subspace detection and dimension weighting. Subspace detection
finds the feature subspace that is representative and significant to the
output. Then, the feature alignment is performed in the subspace during TTA.
Meanwhile, dimension weighting raises the importance of the dimensions of the
feature subspace that have greater significance to the output. We
experimentally show that SSA outperforms various baselines on real-world
datasets.",http://arxiv.org/pdf/2410.03263v1
Towards a Benchmark for Large Language Models for Business Process Management Tasks,"Kiran Busch, Henrik Leopold",2024-10-04,"An increasing number of organizations are deploying Large Language Models
(LLMs) for a wide range of tasks. Despite their general utility, LLMs are prone
to errors, ranging from inaccuracies to hallucinations. To objectively assess
the capabilities of existing LLMs, performance benchmarks are conducted.
However, these benchmarks often do not translate to more specific real-world
tasks. This paper addresses the gap in benchmarking LLM performance in the
Business Process Management (BPM) domain. Currently, no BPM-specific benchmarks
exist, creating uncertainty about the suitability of different LLMs for BPM
tasks. This paper systematically compares LLM performance on four BPM tasks
focusing on small open-source models. The analysis aims to identify
task-specific performance variations, compare the effectiveness of open-source
versus commercial models, and assess the impact of model size on BPM task
performance. This paper provides insights into the practical applications of
LLMs in BPM, guiding organizations in selecting appropriate models for their
specific needs.",http://arxiv.org/pdf/2410.03255v1
How much can we forget about Data Contamination?,"Sebastian Bordt, Suraj Srinivas, Valentyn Boreiko, Ulrike von Luxburg",2024-10-04,"The leakage of benchmark data into the training data has emerged as a
significant challenge for evaluating the capabilities of large language models
(LLMs). In this work, we use experimental evidence and theoretical estimates to
challenge the common assumption that small-scale contamination renders
benchmark evaluations invalid. First, we experimentally quantify the magnitude
of benchmark overfitting based on scaling along three dimensions: The number of
model parameters (up to 1.6B), the number of times an example is seen (up to
144), and the number of training tokens (up to 40B). We find that if model and
data follow the Chinchilla scaling laws, minor contamination indeed leads to
overfitting. At the same time, even 144 times of contamination can be forgotten
if the training data is scaled beyond five times Chinchilla, a regime
characteristic of many modern LLMs. We then derive a simple theory of example
forgetting via cumulative weight decay. It allows us to bound the number of
gradient steps required to forget past data for any training run where we know
the hyperparameters of AdamW. This indicates that many LLMs, including Llama 3,
have forgotten the data seen at the beginning of training. Experimentally, we
demonstrate that forgetting occurs faster than what is predicted by our bounds.
Taken together, our results suggest that moderate amounts of contamination can
be forgotten at the end of realistically scaled training runs.",http://arxiv.org/pdf/2410.03249v1
Latent Action Priors From a Single Gait Cycle Demonstration for Online Imitation Learning,"Oliver Hausdörfer, Alexander von Rohr, Éric Lefort, Angela Schoellig",2024-10-04,"Deep Reinforcement Learning (DRL) in simulation often results in brittle and
unrealistic learning outcomes. To push the agent towards more desirable
solutions, prior information can be injected in the learning process through,
for instance, reward shaping, expert data, or motion primitives. We propose an
additional inductive bias for robot learning: latent actions learned from
expert demonstration as priors in the action space. We show that these action
priors can be learned from only a single open-loop gait cycle using a simple
autoencoder. Using these latent action priors combined with established style
rewards for imitation in DRL achieves above expert demonstration level of
performance and leads to more desirable gaits. Further, action priors
substantially improve the performance on transfer tasks, even leading to gait
transitions for higher target speeds. Videos and code are available at
https://sites.google.com/view/latent-action-priors.",http://arxiv.org/pdf/2410.03246v1
Enriching Ontologies with Disjointness Axioms using Large Language Models,"Elias Crum, Antonio De Santis, Manon Ovide, Jiaxin Pan, Alessia Pisu, Nicolas Lazzari, Sebastian Rudolph",2024-10-04,"Ontologies often lack explicit disjointness declarations between classes,
despite their usefulness for sophisticated reasoning and consistency checking
in Knowledge Graphs. In this study, we explore the potential of Large Language
Models (LLMs) to enrich ontologies by identifying and asserting class
disjointness axioms. Our approach aims at leveraging the implicit knowledge
embedded in LLMs, using prompt engineering to elicit this knowledge for
classifying ontological disjointness. We validate our methodology on the
DBpedia ontology, focusing on open-source LLMs. Our findings suggest that LLMs,
when guided by effective prompt strategies, can reliably identify disjoint
class relationships, thus streamlining the process of ontology completion
without extensive manual input. For comprehensive disjointness enrichment, we
propose a process that takes logical relationships between disjointness and
subclass statements into account in order to maintain satisfiability and reduce
the number of calls to the LLM. This work provides a foundation for future
applications of LLMs in automated ontology enhancement and offers insights into
optimizing LLM performance through strategic prompt design. Our code is
publicly available on GitHub at https://github.com/n28div/llm-disjointness.",http://arxiv.org/pdf/2410.03235v1
AutoPenBench: Benchmarking Generative Agents for Penetration Testing,"Luca Gioacchini, Marco Mellia, Idilio Drago, Alexander Delsanto, Giuseppe Siracusano, Roberto Bifulco",2024-10-04,"Generative AI agents, software systems powered by Large Language Models
(LLMs), are emerging as a promising approach to automate cybersecurity tasks.
Among the others, penetration testing is a challenging field due to the task
complexity and the diverse strategies to simulate cyber-attacks. Despite
growing interest and initial studies in automating penetration testing with
generative agents, there remains a significant gap in the form of a
comprehensive and standard framework for their evaluation and development. This
paper introduces AutoPenBench, an open benchmark for evaluating generative
agents in automated penetration testing. We present a comprehensive framework
that includes 33 tasks, each representing a vulnerable system that the agent
has to attack. Tasks are of increasing difficulty levels, including in-vitro
and real-world scenarios. We assess the agent performance with generic and
specific milestones that allow us to compare results in a standardised manner
and understand the limits of the agent under test. We show the benefits of
AutoPenBench by testing two agent architectures: a fully autonomous and a
semi-autonomous supporting human interaction. We compare their performance and
limitations. For example, the fully autonomous agent performs unsatisfactorily
achieving a 21% Success Rate (SR) across the benchmark, solving 27% of the
simple tasks and only one real-world task. In contrast, the assisted agent
demonstrates substantial improvements, with 64% of SR. AutoPenBench allows us
also to observe how different LLMs like GPT-4o or OpenAI o1 impact the ability
of the agents to complete the tasks. We believe that our benchmark fills the
gap with a standard and flexible framework to compare penetration testing
agents on a common ground. We hope to extend AutoPenBench along with the
research community by making it available under
https://github.com/lucagioacchini/auto-pen-bench.",http://arxiv.org/pdf/2410.03225v1
ScriptViz: A Visualization Tool to Aid Scriptwriting based on a Large Movie Database,"Anyi Rao, Jean-Peïc Chou, Maneesh Agrawala",2024-10-04,"Scriptwriters usually rely on their mental visualization to create a vivid
story by using their imagination to see, feel, and experience the scenes they
are writing. Besides mental visualization, they often refer to existing images
or scenes in movies and analyze the visual elements to create a certain mood or
atmosphere. In this paper, we develop ScriptViz to provide external
visualization based on a large movie database for the screenwriting process. It
retrieves reference visuals on the fly based on scripts' text and dialogue from
a large movie database. The tool provides two types of control on visual
elements that enable writers to 1) see exactly what they want with fixed visual
elements and 2) see variances in uncertain elements. User evaluation among 15
scriptwriters shows that ScriptViz is able to present scriptwriters with
consistent yet diverse visual possibilities, aligning closely with their
scripts and helping their creation.",http://arxiv.org/pdf/2410.03224v1
"A Tutorial on the Design, Experimentation and Application of Metaheuristic Algorithms to Real-World Optimization Problems","Eneko Osaba, Esther Villar-Rodriguez, Javier Del Ser, Antonio J. Nebro, Daniel Molina, Antonio LaTorre, Ponnuthurai N. Suganthan, Carlos A. Coello Coello, Francisco Herrera",2024-10-04,"In the last few years, the formulation of real-world optimization problems
and their efficient solution via metaheuristic algorithms has been a catalyst
for a myriad of research studies. In spite of decades of historical
advancements on the design and use of metaheuristics, large difficulties still
remain in regards to the understandability, algorithmic design uprightness, and
performance verifiability of new technical achievements. A clear example stems
from the scarce replicability of works dealing with metaheuristics used for
optimization, which is often infeasible due to ambiguity and lack of detail in
the presentation of the methods to be reproduced. Additionally, in many cases,
there is a questionable statistical significance of their reported results.
This work aims at providing the audience with a proposal of good practices
which should be embraced when conducting studies about metaheuristics methods
used for optimization in order to provide scientific rigor, value and
transparency. To this end, we introduce a step by step methodology covering
every research phase that should be followed when addressing this scientific
field. Specifically, frequently overlooked yet crucial aspects and useful
recommendations will be discussed in regards to the formulation of the problem,
solution encoding, implementation of search operators, evaluation metrics,
design of experiments, and considerations for real-world performance, among
others. Finally, we will outline important considerations, challenges, and
research directions for the success of newly developed optimization
metaheuristics in their deployment and operation over real-world application
environments.",http://arxiv.org/pdf/2410.03205v1
MultiVerse: Efficient and Expressive Zero-Shot Multi-Task Text-to-Speech,"Taejun Bak, Youngsik Eom, SeungJae Choi, Young-Sun Joo",2024-10-04,"Text-to-speech (TTS) systems that scale up the amount of training data have
achieved significant improvements in zero-shot speech synthesis. However, these
systems have certain limitations: they require a large amount of training data,
which increases costs, and often overlook prosody similarity. To address these
issues, we propose MultiVerse, a zero-shot multi-task TTS system that is able
to perform TTS or speech style transfer in zero-shot and cross-lingual
conditions. MultiVerse requires much less training data than traditional
data-driven approaches. To ensure zero-shot performance even with limited data,
we leverage source-filter theory-based disentanglement, utilizing the prompt
for modeling filter-related and source-related representations. Additionally,
to further enhance prosody similarity, we adopt a prosody modeling approach
combining prompt-based autoregressive and non-autoregressive methods.
Evaluations demonstrate the remarkable zero-shot multi-task TTS performance of
MultiVerse and show that MultiVerse not only achieves zero-shot TTS performance
comparable to data-driven TTS systems with much less data, but also
significantly outperforms other zero-shot TTS systems trained with the same
small amount of data. In particular, our novel prosody modeling technique
significantly contributes to MultiVerse's ability to generate speech with high
prosody similarity to the given prompts. Our samples are available at
https://nc-ai.github.io/speech/publications/multiverse/index.html",http://arxiv.org/pdf/2410.03192v1
Looking into Concept Explanation Methods for Diabetic Retinopathy Classification,"Andrea M. Storås, Josefine V. Sundgaard",2024-10-04,"Diabetic retinopathy is a common complication of diabetes, and monitoring the
progression of retinal abnormalities using fundus imaging is crucial. Because
the images must be interpreted by a medical expert, it is infeasible to screen
all individuals with diabetes for diabetic retinopathy. Deep learning has shown
impressive results for automatic analysis and grading of fundus images. One
drawback is, however, the lack of interpretability, which hampers the
implementation of such systems in the clinic. Explainable artificial
intelligence methods can be applied to explain the deep neural networks.
Explanations based on concepts have shown to be intuitive for humans to
understand, but have not yet been explored in detail for diabetic retinopathy
grading. This work investigates and compares two concept-based explanation
techniques for explaining deep neural networks developed for automatic
diagnosis of diabetic retinopathy: Quantitative Testing with Concept Activation
Vectors and Concept Bottleneck Models. We found that both methods have
strengths and weaknesses, and choice of method should take the available data
and the end user's preferences into account.",http://arxiv.org/pdf/2410.03188v1
EXAQ: Exponent Aware Quantization For LLMs Acceleration,"Moran Shkolnik, Maxim Fishman, Brian Chmiel, Hilla Ben-Yaacov, Ron Banner, Kfir Yehuda Levy",2024-10-04,"Quantization has established itself as the primary approach for decreasing
the computational and storage expenses associated with Large Language Models
(LLMs) inference. The majority of current research emphasizes quantizing
weights and activations to enable low-bit general-matrix-multiply (GEMM)
operations, with the remaining non-linear operations executed at higher
precision. In our study, we discovered that following the application of these
techniques, the primary bottleneck in LLMs inference lies in the softmax layer.
The softmax operation comprises three phases: exponent calculation,
accumulation, and normalization, Our work focuses on optimizing the first two
phases. We propose an analytical approach to determine the optimal clipping
value for the input to the softmax function, enabling sub-4-bit quantization
for LLMs inference. This method accelerates the calculations of both $e^x$ and
$\sum(e^x)$ with minimal to no accuracy degradation. For example, in
LLaMA1-30B, we achieve baseline performance with 2-bit quantization on the
well-known ""Physical Interaction: Question Answering"" (PIQA) dataset
evaluation. This ultra-low bit quantization allows, for the first time, an
acceleration of approximately 4x in the accumulation phase. The combination of
accelerating both $e^x$ and $\sum(e^x)$ results in a 36.9% acceleration in the
softmax operation.",http://arxiv.org/pdf/2410.03185v1
Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models,"Yufang Liu, Tao Ji, Changzhi Sun, Yuanbin Wu, Aimin Zhou",2024-10-04,"Large Vision-Language Models (LVLMs) have achieved impressive performance,
yet research has pointed out a serious issue with object hallucinations within
these models. However, there is no clear conclusion as to which part of the
model these hallucinations originate from. In this paper, we present an
in-depth investigation into the object hallucination problem specifically
within the CLIP model, which serves as the backbone for many state-of-the-art
vision-language systems. We unveil that even in isolation, the CLIP model is
prone to object hallucinations, suggesting that the hallucination problem is
not solely due to the interaction between vision and language modalities. To
address this, we propose a counterfactual data augmentation method by creating
negative samples with a variety of hallucination issues. We demonstrate that
our method can effectively mitigate object hallucinations for CLIP model, and
we show the the enhanced model can be employed as a visual encoder, effectively
alleviating the object hallucination issue in LVLMs.",http://arxiv.org/pdf/2410.03176v1
Adaptive Masking Enhances Visual Grounding,"Sen Jia, Lei Li",2024-10-04,"In recent years, zero-shot and few-shot learning in visual grounding have
garnered considerable attention, largely due to the success of large-scale
vision-language pre-training on expansive datasets such as LAION-5B and
DataComp-1B. However, the continuous expansion of these datasets presents
significant challenges, particularly with respect to data availability and
computational overhead, thus creating a bottleneck in the advancement of
low-shot learning capabilities. In this paper, we propose IMAGE, Interpretative
MAsking with Gaussian radiation modEling, aimed at enhancing vocabulary
grounding in low-shot learning scenarios without necessitating an increase in
dataset size. Drawing inspiration from cognitive science and the recent success
of masked autoencoders (MAE), our method leverages adaptive masking on salient
regions of the feature maps generated by the vision backbone. This enables the
model to learn robust, generalized representations through the reconstruction
of occluded information, thereby facilitating effective attention to both local
and global features. We evaluate the efficacy of our approach on benchmark
datasets, including COCO and ODinW, demonstrating its superior performance in
zero-shot and few-shot tasks. Experimental results consistently show that IMAGE
outperforms baseline models, achieving enhanced generalization and improved
performance in low-shot scenarios. These findings highlight the potential of
adaptive feature manipulation through attention mechanisms and Gaussian
modeling as a promising alternative to approaches that rely on the continual
scaling of dataset sizes for the advancement of zero-shot and few-shot
learning. Our code is publicly available at https://github.com/git-lenny/IMAGE.",http://arxiv.org/pdf/2410.03161v1
Autoregressive Moving-average Attention Mechanism for Time Series Forecasting,"Jiecheng Lu, Xu Han, Yan Sun, Shihao Yang",2024-10-04,"We propose an Autoregressive (AR) Moving-average (MA) attention structure
that can adapt to various linear attention mechanisms, enhancing their ability
to capture long-range and local temporal patterns in time series. In this
paper, we first demonstrate that, for the time series forecasting (TSF) task,
the previously overlooked decoder-only autoregressive Transformer model can
achieve results comparable to the best baselines when appropriate tokenization
and training methods are applied. Moreover, inspired by the ARMA model from
statistics and recent advances in linear attention, we introduce the full ARMA
structure into existing autoregressive attention mechanisms. By using an
indirect MA weight generation method, we incorporate the MA term while
maintaining the time complexity and parameter size of the underlying efficient
attention models. We further explore how indirect parameter generation can
produce implicit MA weights that align with the modeling requirements for local
temporal impacts. Experimental results show that incorporating the ARMA
structure consistently improves the performance of various AR attentions on TSF
tasks, achieving state-of-the-art results.",http://arxiv.org/pdf/2410.03159v1
Mathematical Formalism for Memory Compression in Selective State Space Models,Siddhanth Bhat,2024-10-04,"State space models (SSMs) have emerged as a powerful framework for modelling
long-range dependencies in sequence data. Unlike traditional recurrent neural
networks (RNNs) and convolutional neural networks (CNNs), SSMs offer a
structured and stable approach to sequence modelling, leveraging principles
from control theory and dynamical systems. However, a key challenge in sequence
modelling is compressing long-term dependencies into a compact hidden state
representation without losing critical information.
  In this paper, we develop a rigorous mathematical framework for understanding
memory compression in selective state space models. We introduce a selective
gating mechanism that dynamically filters and updates the hidden state based on
input relevance, allowing for efficient memory compression. We formalize the
trade-off between memory efficiency and information retention using
information-theoretic tools, such as mutual information and rate-distortion
theory. Our analysis provides theoretical bounds on the amount of information
that can be compressed without sacrificing model performance.
  We also derive theorems that prove the stability and convergence of the
hidden state in selective SSMs, ensuring reliable long-term memory retention.
Computational complexity analysis reveals that selective SSMs offer significant
improvements in memory efficiency and processing speed compared to traditional
RNN-based models. Through empirical validation on sequence modelling tasks such
as time-series forecasting and natural language processing, we demonstrate that
selective SSMs achieve state-of-the-art performance while using less memory and
computational resources.",http://arxiv.org/pdf/2410.03158v1
MELODI: Exploring Memory Compression for Long Contexts,"Yinpeng Chen, DeLesley Hutchins, Aren Jansen, Andrey Zhmoginov, David Racz, Jesper Andersen",2024-10-04,"We present MELODI, a novel memory architecture designed to efficiently
process long documents using short context windows. The key principle behind
MELODI is to represent short-term and long-term memory as a hierarchical
compression scheme across both network layers and context windows.
Specifically, the short-term memory is achieved through recurrent compression
of context windows across multiple layers, ensuring smooth transitions between
windows. In contrast, the long-term memory performs further compression within
a single middle layer and aggregates information across context windows,
effectively consolidating crucial information from the entire history. Compared
to a strong baseline - the Memorizing Transformer employing dense attention
over a large long-term memory (64K key-value pairs) - our method demonstrates
superior performance on various long-context datasets while remarkably reducing
the memory footprint by a factor of 8.",http://arxiv.org/pdf/2410.03156v1
Remaining Useful Life Prediction: A Study on Multidimensional Industrial Signal Processing and Efficient Transfer Learning Based on Large Language Models,"Yan Chen, Cheng Liu",2024-10-04,"Remaining useful life (RUL) prediction is crucial for maintaining modern
industrial systems, where equipment reliability and operational safety are
paramount. Traditional methods, based on small-scale deep learning or
physical/statistical models, often struggle with complex, multidimensional
sensor data and varying operating conditions, limiting their generalization
capabilities. To address these challenges, this paper introduces an innovative
regression framework utilizing large language models (LLMs) for RUL prediction.
By leveraging the modeling power of LLMs pre-trained on corpus data, the
proposed model can effectively capture complex temporal dependencies and
improve prediction accuracy. Extensive experiments on the Turbofan engine's RUL
prediction task show that the proposed model surpasses state-of-the-art (SOTA)
methods on the challenging FD002 and FD004 subsets and achieves near-SOTA
results on the other subsets. Notably, different from previous research, our
framework uses the same sliding window length and all sensor signals for all
subsets, demonstrating strong consistency and generalization. Moreover,
transfer learning experiments reveal that with minimal target domain data for
fine-tuning, the model outperforms SOTA methods trained on full target domain
data. This research highlights the significant potential of LLMs in industrial
signal processing and RUL prediction, offering a forward-looking solution for
health management in future intelligent industrial systems.",http://arxiv.org/pdf/2410.03134v1
Autoregressive Action Sequence Learning for Robotic Manipulation,"Xinyu Zhang, Yuhan Liu, Haonan Chang, Liam Schramm, Abdeslam Boularias",2024-10-04,"Autoregressive models have demonstrated remarkable success in natural
language processing. In this work, we design a simple yet effective
autoregressive architecture for robotic manipulation tasks. We propose the
Chunking Causal Transformer (CCT), which extends the next-single-token
prediction of causal transformers to support multi-token prediction in a single
pass. Further, we design a novel attention interleaving strategy that allows
CCT to be trained efficiently with teacher-forcing. Based on CCT, we propose
the Autoregressive Policy (ARP) model, which learns to generate action
sequences autoregressively. We find that action sequence learning enables
better leverage of the underlying causal relationships in robotic tasks. We
evaluate ARP across diverse robotic manipulation environments, including
Push-T, ALOHA, and RLBench, and show that it outperforms the state-of-the-art
methods in all tested environments, while being more efficient in computation
and parameter sizes. Video demonstrations, our source code, and the models of
ARP can be found at http://github.com/mlzxy/arp.",http://arxiv.org/pdf/2410.03132v1
AIME: AI System Optimization via Multiple LLM Evaluators,"Bhrij Patel, Souradip Chakraborty, Wesley A. Suttle, Mengdi Wang, Amrit Singh Bedi, Dinesh Manocha",2024-10-04,"Text-based AI system optimization typically involves a feedback loop scheme
where a single LLM generates an evaluation in natural language of the current
output to improve the next iteration's output. However, in this work, we
empirically demonstrate that for a practical and complex task (code generation)
with multiple criteria to evaluate, utilizing only one LLM evaluator tends to
let errors in generated code go undetected, thus leading to incorrect
evaluations and ultimately suboptimal test case performance. Motivated by this
failure case, we assume there exists an optimal evaluation policy that samples
an evaluation between response and ground truth. We then theoretically prove
that a linear combination of multiple evaluators can approximate this optimal
policy. From this insight, we propose AI system optimization via Multiple LLM
Evaluators (AIME). AIME is an evaluation protocol that utilizes multiple LLMs
that each independently generate an evaluation on separate criteria and then
combine them via concatenation. We provide an extensive empirical study showing
AIME outperforming baseline methods in code generation tasks, with up to $62\%$
higher error detection rate and up to $16\%$ higher success rate than a single
LLM evaluation protocol on LeetCodeHard and HumanEval datasets. We also show
that the selection of the number of evaluators and which criteria to utilize is
non-trivial as it can impact pact success rate by up to $12\%$.",http://arxiv.org/pdf/2410.03131v1
ARB-LLM: Alternating Refined Binarizations for Large Language Models,"Zhiteng Li, Xianglong Yan, Tianao Zhang, Haotong Qin, Dong Xie, Jiang Tian, zhongchao shi, Linghe Kong, Yulun Zhang, Xiaokang Yang",2024-10-04,"Large Language Models (LLMs) have greatly pushed forward advancements in
natural language processing, yet their high memory and computational demands
hinder practical deployment. Binarization, as an effective compression
technique, can shrink model weights to just 1 bit, significantly reducing the
high demands on computation and memory. However, current binarization methods
struggle to narrow the distribution gap between binarized and full-precision
weights, while also overlooking the column deviation in LLM weight
distribution. To tackle these issues, we propose ARB-LLM, a novel 1-bit
post-training quantization (PTQ) technique tailored for LLMs. To narrow the
distribution shift between binarized and full-precision weights, we first
design an alternating refined binarization (ARB) algorithm to progressively
update the binarization parameters, which significantly reduces the
quantization error. Moreover, considering the pivot role of calibration data
and the column deviation in LLM weights, we further extend ARB to ARB-X and
ARB-RC. In addition, we refine the weight partition strategy with column-group
bitmap (CGB), which further enhance performance. Equipping ARB-X and ARB-RC
with CGB, we obtain ARB-LLM$_\text{X}$ and ARB-LLM$_\text{RC}$ respectively,
which significantly outperform state-of-the-art (SOTA) binarization methods for
LLMs. As a binary PTQ method, our ARB-LLM$_\text{RC}$ is the first to surpass
FP16 models of the same size. The code and models will be available at
https://github.com/ZHITENGLI/ARB-LLM.",http://arxiv.org/pdf/2410.03129v1
Understanding Decision Subjects' Engagement with and Perceived Fairness of AI Models When Opportunities of Qualification Improvement Exist,"Meric Altug Gemalmaz, Ming Yin",2024-10-04,"We explore how an AI model's decision fairness affects people's engagement
with and perceived fairness of the model if they are subject to its decisions,
but could repeatedly and strategically respond to these decisions. Two types of
strategic responses are considered -- people could determine whether to
continue interacting with the model, and whether to invest in themselves to
improve their chance of future favorable decisions from the model. Via three
human-subject experiments, we found that in decision subjects' strategic,
repeated interactions with an AI model, the model's decision fairness does not
change their willingness to interact with the model or to improve themselves,
even when the model exhibits unfairness on salient protected attributes.
However, decision subjects still perceive the AI model to be less fair when it
systematically biases against their group, especially if the difficulty of
improving one's qualification for the favorable decision is larger for the
lowly-qualified people.",http://arxiv.org/pdf/2410.03126v1
RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning,"Zihao Zhao, Yuchen Yang, Yijiang Li, Yinzhi Cao",2024-10-04,"The ripple effect poses a significant challenge in knowledge editing for
large language models. Namely, when a single fact is edited, the model
struggles to accurately update the related facts in a sequence, which is
evaluated by multi-hop questions linked to a chain of related facts. Recent
strategies have moved away from traditional parameter updates to more flexible,
less computation-intensive methods, proven to be more effective in addressing
the ripple effect. In-context learning (ICL) editing uses a simple
demonstration `Imagine that + new fact` to guide LLMs, but struggles with
complex multi-hop questions as the new fact alone fails to specify the chain of
facts involved in such scenarios. Besides, memory-based editing maintains
additional storage for all edits and related facts, requiring continuous
updates to stay effective. As a result of these design limitations, the
challenge remains, with the highest accuracy being only 33.8% on the MQuAKE-cf
benchmarks for Vicuna-7B. To address this, we propose RippleCOT, a novel ICL
editing approach integrating Chain-of-Thought (COT) reasoning. RippleCOT
structures demonstrations as `newfact, question, thought, answer`,
incorporating a thought component to identify and decompose the multi-hop logic
within questions. This approach effectively guides the model through complex
multi-hop questions with chains of related facts. Comprehensive experiments
demonstrate that RippleCOT significantly outperforms the state-of-the-art on
the ripple effect, achieving accuracy gains ranging from 7.8% to 87.1%.",http://arxiv.org/pdf/2410.03122v1
ProcBench: Benchmark for Multi-Step Reasoning and Following Procedure,"Ippei Fujisawa, Sensho Nobe, Hiroki Seto, Rina Onda, Yoshiaki Uchida, Hiroki Ikoma, Pei-Chun Chien, Ryota Kanai",2024-10-04,"Reasoning is central to a wide range of intellectual activities, and while
the capabilities of large language models (LLMs) continue to advance, their
performance in reasoning tasks remains limited. The processes and mechanisms
underlying reasoning are not yet fully understood, but key elements include
path exploration, selection of relevant knowledge, and multi-step inference.
Problems are solved through the synthesis of these components. In this paper,
we propose a benchmark that focuses on a specific aspect of reasoning ability:
the direct evaluation of multi-step inference. To this end, we design a special
reasoning task where multi-step inference is specifically focused by largely
eliminating path exploration and implicit knowledge utilization. Our dataset
comprises pairs of explicit instructions and corresponding questions, where the
procedures necessary for solving the questions are entirely detailed within the
instructions. This setup allows models to solve problems solely by following
the provided directives. By constructing problems that require varying numbers
of steps to solve and evaluating responses at each step, we enable a thorough
assessment of state-of-the-art LLMs' ability to follow instructions. To ensure
the robustness of our evaluation, we include multiple distinct tasks.
Furthermore, by comparing accuracy across tasks, utilizing step-aware metrics,
and applying separately defined measures of complexity, we conduct experiments
that offer insights into the capabilities and limitations of LLMs in reasoning
tasks. Our findings have significant implications for the development of LLMs
and highlight areas for future research in advancing their reasoning abilities.
Our dataset is available at
\url{https://huggingface.co/datasets/ifujisawa/procbench} and code at
\url{https://github.com/ifujisawa/proc-bench}.",http://arxiv.org/pdf/2410.03117v1
LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive Compression Strategy,"Rongzhi Zhang, Kuang Wang, Liyuan Liu, Shuohang Wang, Hao Cheng, Chao Zhang, Yelong Shen",2024-10-04,"The Key-Value (KV) cache is a crucial component in serving transformer-based
autoregressive large language models (LLMs), enabling faster inference by
storing previously computed KV vectors. However, its memory consumption scales
linearly with sequence length and batch size, posing a significant bottleneck
in LLM deployment. Existing approaches to mitigate this issue include: (1)
efficient attention variants integrated in upcycling stages, which requires
extensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache
compression at test time, primarily through token eviction policies, which
often overlook inter-layer dependencies and can be task-specific.
  This paper introduces an orthogonal approach to KV cache compression. We
propose a low-rank approximation of KV weight matrices, allowing for plug-in
integration with existing transformer-based LLMs without model retraining. To
effectively compress KV cache at the weight level, we adjust for layerwise
sensitivity and introduce a progressive compression strategy, which is
supported by our theoretical analysis on how compression errors accumulate in
deep networks. Our method is designed to function without model tuning in
upcycling stages or task-specific profiling in test stages. Extensive
experiments with LLaMA models ranging from 8B to 70B parameters across various
tasks show that our approach significantly reduces the GPU memory footprint
while maintaining performance.",http://arxiv.org/pdf/2410.03111v1
MBDS: A Multi-Body Dynamics Simulation Dataset for Graph Networks Simulators,"Sheng Yang, Fengge Wu, Junsuo Zhao",2024-10-04,"Modeling the structure and events of the physical world constitutes a
fundamental objective of neural networks. Among the diverse approaches, Graph
Network Simulators (GNS) have emerged as the leading method for modeling
physical phenomena, owing to their low computational cost and high accuracy.
The datasets employed for training and evaluating physical simulation
techniques are typically generated by researchers themselves, often resulting
in limited data volume and quality. Consequently, this poses challenges in
accurately assessing the performance of these methods. In response to this, we
have constructed a high-quality physical simulation dataset encompassing 1D,
2D, and 3D scenes, along with more trajectories and time-steps compared to
existing datasets. Furthermore, our work distinguishes itself by developing
eight complete scenes, significantly enhancing the dataset's comprehensiveness.
A key feature of our dataset is the inclusion of precise multi-body dynamics,
facilitating a more realistic simulation of the physical world. Utilizing our
high-quality dataset, we conducted a systematic evaluation of various existing
GNS methods. Our dataset is accessible for download at
https://github.com/Sherlocktein/MBDS, offering a valuable resource for
researchers to enhance the training and evaluation of their methodologies.",http://arxiv.org/pdf/2410.03107v1
Mamba in Vision: A Comprehensive Survey of Techniques and Applications,"Md Maklachur Rahman, Abdullah Aman Tutul, Ankur Nath, Lamyanba Laishram, Soon Ki Jung, Tracy Hammond",2024-10-04,"Mamba is emerging as a novel approach to overcome the challenges faced by
Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) in computer
vision. While CNNs excel at extracting local features, they often struggle to
capture long-range dependencies without complex architectural modifications. In
contrast, ViTs effectively model global relationships but suffer from high
computational costs due to the quadratic complexity of their self-attention
mechanisms. Mamba addresses these limitations by leveraging Selective
Structured State Space Models to effectively capture long-range dependencies
with linear computational complexity. This survey analyzes the unique
contributions, computational benefits, and applications of Mamba models while
also identifying challenges and potential future research directions. We
provide a foundational resource for advancing the understanding and growth of
Mamba models in computer vision. An overview of this work is available at
https://github.com/maklachur/Mamba-in-Computer-Vision.",http://arxiv.org/pdf/2410.03105v1
Combing Text-based and Drag-based Editing for Precise and Flexible Image Editing,"Ziqi Jiang, Zhen Wang, Long Chen",2024-10-04,"Precise and flexible image editing remains a fundamental challenge in
computer vision. Based on the modified areas, most editing methods can be
divided into two main types: global editing and local editing. In this paper,
we choose the two most common editing approaches (ie text-based editing and
drag-based editing) and analyze their drawbacks. Specifically, text-based
methods often fail to describe the desired modifications precisely, while
drag-based methods suffer from ambiguity. To address these issues, we proposed
\textbf{CLIPDrag}, a novel image editing method that is the first to combine
text and drag signals for precise and ambiguity-free manipulations on diffusion
models. To fully leverage these two signals, we treat text signals as global
guidance and drag points as local information. Then we introduce a novel
global-local motion supervision method to integrate text signals into existing
drag-based methods by adapting a pre-trained language-vision model like CLIP.
Furthermore, we also address the problem of slow convergence in CLIPDrag by
presenting a fast point-tracking method that enforces drag points moving toward
correct directions. Extensive experiments demonstrate that CLIPDrag outperforms
existing single drag-based methods or text-based methods.",http://arxiv.org/pdf/2410.03097v1
Strategic Insights from Simulation Gaming of AI Race Dynamics,"Ross Gruetzemacher, Shahar Avin, James Fox, Alexander K Saeri",2024-10-04,"We present insights from ""Intelligence Rising"", a scenario exploration
exercise about possible AI futures. Drawing on the experiences of facilitators
who have overseen 43 games over a four-year period, we illuminate recurring
patterns, strategies, and decision-making processes observed during gameplay.
Our analysis reveals key strategic considerations about AI development
trajectories in this simulated environment, including: the destabilising
effects of AI races, the crucial role of international cooperation in
mitigating catastrophic risks, the challenges of aligning corporate and
national interests, and the potential for rapid, transformative change in AI
capabilities. We highlight places where we believe the game has been effective
in exposing participants to the complexities and uncertainties inherent in AI
governance. Key recurring gameplay themes include the emergence of
international agreements, challenges to the robustness of such agreements, the
critical role of cybersecurity in AI development, and the potential for
unexpected crises to dramatically alter AI trajectories. By documenting these
insights, we aim to provide valuable foresight for policymakers, industry
leaders, and researchers navigating the complex landscape of AI development and
governance.",http://arxiv.org/pdf/2410.03092v1
Scaling Parameter-Constrained Language Models with Quality Data,"Ernie Chang, Matteo Paltenghi, Yang Li, Pin-Jie Lin, Changsheng Zhao, Patrick Huber, Zechun Liu, Rastislav Rabatin, Yangyang Shi, Vikas Chandra",2024-10-04,"Scaling laws in language modeling traditionally quantify training loss as a
function of dataset size and model parameters, providing compute-optimal
estimates but often neglecting the impact of data quality on model
generalization. In this paper, we extend the conventional understanding of
scaling law by offering a microscopic view of data quality within the original
formulation -- effective training tokens -- which we posit to be a critical
determinant of performance for parameter-constrained language models.
Specifically, we formulate the proposed term of effective training tokens to be
a combination of two readily-computed indicators of text: (i) text diversity
and (ii) syntheticity as measured by a teacher model. We pretrained over $200$
models of 25M to 1.5B parameters on a diverse set of sampled, synthetic data,
and estimated the constants that relate text quality, model size, training
tokens, and eight reasoning task accuracy scores. We demonstrated the estimated
constants yield +0.83 Pearson correlation with true accuracies, and analyzed it
in scenarios involving widely-used data techniques such as data sampling and
synthesis which aim to improve data quality.",http://arxiv.org/pdf/2410.03083v1
CommonIT: Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions,"Jun Rao, Xuebo Liu, Lian Lian, Shengjun Cheng, Yunjie Liao, Min Zhang",2024-10-04,"With instruction tuning, Large Language Models (LLMs) can enhance their
ability to adhere to commands. Diverging from most works focusing on data
mixing, our study concentrates on enhancing the model's capabilities from the
perspective of data sampling during training. Drawing inspiration from the
human learning process, where it is generally easier to master solutions to
similar topics through focused practice on a single type of topic, we introduce
a novel instruction tuning strategy termed CommonIT: Commonality-aware
Instruction Tuning. Specifically, we cluster instruction datasets into distinct
groups with three proposed metrics (Task, Embedding and Length). We ensure each
training mini-batch, or ""partition"", consists solely of data from a single
group, which brings about both data randomness across mini-batches and
intra-batch data similarity. Rigorous testing on LLaMa models demonstrates
CommonIT's effectiveness in enhancing the instruction-following capabilities of
LLMs through IT datasets (FLAN, CoT, and Alpaca) and models (LLaMa2-7B,
Qwen2-7B, LLaMa 13B, and BLOOM 7B). CommonIT consistently boosts an average
improvement of 2.1\% on the general domain (i.e., the average score of
Knowledge, Reasoning, Multilinguality and Coding) with the Length metric, and
5.2\% on the special domain (i.e., GSM, Openfunctions and Code) with the Task
metric, and 3.8\% on the specific tasks (i.e., MMLU) with the Embedding metric.
Code is available at \url{https://github.com/raojay7/CommonIT}.",http://arxiv.org/pdf/2410.03077v1
Multi-Robot Motion Planning with Diffusion Models,"Yorai Shaoul, Itamar Mishani, Shivam Vats, Jiaoyang Li, Maxim Likhachev",2024-10-04,"Diffusion models have recently been successfully applied to a wide range of
robotics applications for learning complex multi-modal behaviors from data.
However, prior works have mostly been confined to single-robot and small-scale
environments due to the high sample complexity of learning multi-robot
diffusion models. In this paper, we propose a method for generating
collision-free multi-robot trajectories that conform to underlying data
distributions while using only single-robot data. Our algorithm, Multi-robot
Multi-model planning Diffusion (MMD), does so by combining learned diffusion
models with classical search-based techniques -- generating data-driven motions
under collision constraints. Scaling further, we show how to compose multiple
diffusion models to plan in large environments where a single diffusion model
fails to generalize well. We demonstrate the effectiveness of our approach in
planning for dozens of robots in a variety of simulated scenarios motivated by
logistics environments. View video demonstrations in our supplementary
material, and our code at: https://github.com/yoraish/mmd.",http://arxiv.org/pdf/2410.03072v1
Integrating Natural Language Prompting Tasks in Introductory Programming Courses,"Chris Kerslake, Paul Denny, David H Smith IV, James Prather, Juho Leinonen, Andrew Luxton-Reilly, Stephen MacNeil",2024-10-04,"Introductory programming courses often emphasize mastering syntax and basic
constructs before progressing to more complex and interesting programs. This
bottom-up approach can be frustrating for novices, shifting the focus away from
problem solving and potentially making computing less appealing to a broad
range of students. The rise of generative AI for code production could
partially address these issues by fostering new skills via interaction with AI
models, including constructing high-level prompts and evaluating code that is
automatically generated. In this experience report, we explore the inclusion of
two prompt-focused activities in an introductory course, implemented across
four labs in a six-week module. The first requires students to solve
computational problems by writing natural language prompts, emphasizing
problem-solving over syntax. The second involves students crafting prompts to
generate code equivalent to provided fragments, to foster an understanding of
the relationship between prompts and code. Most of the students in the course
had reported finding programming difficult to learn, often citing frustrations
with syntax and debugging. We found that self-reported difficulty with learning
programming had a strong inverse relationship with performance on traditional
programming assessments such as tests and projects, as expected. However,
performance on the natural language tasks was less strongly related to
self-reported difficulty, suggesting they may target different skills. Learning
how to communicate with AI coding models is becoming an important skill, and
natural language prompting tasks may appeal to a broad range of students.",http://arxiv.org/pdf/2410.03063v1
Image First or Text First? Optimising the Sequencing of Modalities in Large Language Model Prompting and Reasoning Tasks,"Grant Wardle, Teo Susnjak",2024-10-04,"This paper examines how the sequencing of images and text within multi-modal
prompts influences the reasoning performance of large language models (LLMs).
We performed empirical evaluations using three commercial LLMs. Our results
demonstrate that the order in which modalities are presented can significantly
affect performance, particularly in tasks of varying complexity. For simpler
tasks involving a single image, modality sequencing had a clear impact on
accuracy. However, in more complex tasks involving multiple images and
intricate reasoning steps, the effect of sequencing diminished, likely due to
the increased cognitive demands of the task. Our findings also highlight the
importance of question/prompt structure. In nested and multi-step reasoning
tasks, modality sequencing played a key role in shaping model performance.
While LLMs excelled in the initial stages of reasoning, they struggled to
re-incorporate earlier information, underscoring the challenges of multi-hop
reasoning within transformer architectures. This suggests that aligning the
sequence of modalities with the logical flow of reasoning steps is more
critical than modality order alone. These insights offer valuable implications
for improving multi-modal prompt design, with broader applications across
fields such as education, medical imaging, and cross-modal learning.",http://arxiv.org/pdf/2410.03062v1
Towards an Improved Metric for Evaluating Disentangled Representations,"Sahib Julka, Yashu Wang, Michael Granitzer",2024-10-04,"Disentangled representation learning plays a pivotal role in making
representations controllable, interpretable and transferable. Despite its
significance in the domain, the quest for reliable and consistent quantitative
disentanglement metric remains a major challenge. This stems from the
utilisation of diverse metrics measuring different properties and the potential
bias introduced by their design. Our work undertakes a comprehensive
examination of existing popular disentanglement evaluation metrics, comparing
them in terms of measuring aspects of disentanglement (viz. Modularity,
Compactness, and Explicitness), detecting the factor-code relationship, and
describing the degree of disentanglement. We propose a new framework for
quantifying disentanglement, introducing a metric entitled \emph{EDI}, that
leverages the intuitive concept of \emph{exclusivity} and improved factor-code
relationship to minimize ad-hoc decisions. An in-depth analysis reveals that
EDI measures essential properties while offering more stability than existing
metrics, advocating for its adoption as a standardised approach.",http://arxiv.org/pdf/2410.03056v1
Permissive Information-Flow Analysis for Large Language Models,"Shoaib Ahmed Siddiqui, Radhika Gaonkar, Boris Köpf, David Krueger, Andrew Paverd, Ahmed Salem, Shruti Tople, Lukas Wutschitz, Menglin Xia, Santiago Zanella-Béguelin",2024-10-04,"Large Language Models (LLMs) are rapidly becoming commodity components of
larger software systems. This poses natural security and privacy problems:
poisoned data retrieved from one component can change the model's behavior and
compromise the entire system, including coercing the model to spread
confidential data to untrusted components. One promising approach is to tackle
this problem at the system level via dynamic information flow (aka taint)
tracking. Unfortunately, the traditional approach of propagating the most
restrictive input label to the output is too conservative for applications
where LLMs operate on inputs retrieved from diverse sources. In this paper, we
propose a novel, more permissive approach to propagate information flow labels
through LLM queries. The key idea behind our approach is to propagate only the
labels of the samples that were influential in generating the model output and
to eliminate the labels of unnecessary input. We implement and investigate the
effectiveness of two variations of this approach, based on (i) prompt-based
retrieval augmentation, and (ii) a $k$-nearest-neighbors language model. We
compare these with the baseline of an introspection-based influence estimator
that directly asks the language model to predict the output label. The results
obtained highlight the superiority of our prompt-based label propagator, which
improves the label in more than 85% of the cases in an LLM agent setting. These
findings underscore the practicality of permissive label propagation for
retrieval augmentation.",http://arxiv.org/pdf/2410.03055v1
Scalable Frame-based Construction of Sociocultural NormBases for Socially-Aware Dialogues,"Shilin Qu, Weiqing Wang, Xin Zhou, Haolan Zhan, Zhuang Li, Lizhen Qu, Linhao Luo, Yuan-Fang Li, Gholamreza Haffari",2024-10-04,"Sociocultural norms serve as guiding principles for personal conduct in
social interactions, emphasizing respect, cooperation, and appropriate
behavior, which is able to benefit tasks including conversational information
retrieval, contextual information retrieval and retrieval-enhanced machine
learning. We propose a scalable approach for constructing a Sociocultural Norm
(SCN) Base using Large Language Models (LLMs) for socially aware dialogues. We
construct a comprehensive and publicly accessible Chinese Sociocultural
NormBase. Our approach utilizes socially aware dialogues, enriched with
contextual frames, as the primary data source to constrain the generating
process and reduce the hallucinations. This enables extracting of high-quality
and nuanced natural-language norm statements, leveraging the pragmatic
implications of utterances with respect to the situation. As real dialogue
annotated with gold frames are not readily available, we propose using
synthetic data. Our empirical results show: (i) the quality of the SCNs derived
from synthetic data is comparable to that from real dialogues annotated with
gold frames, and (ii) the quality of the SCNs extracted from real data,
annotated with either silver (predicted) or gold frames, surpasses that without
the frame annotations. We further show the effectiveness of the extracted SCNs
in a RAG-based (Retrieval-Augmented Generation) model to reason about multiple
downstream dialogue tasks.",http://arxiv.org/pdf/2410.03049v1
Revealing the Unseen: Guiding Personalized Diffusion Models to Expose Training Data,"Xiaoyu Wu, Jiaru Zhang, Steven Wu",2024-10-03,"Diffusion Models (DMs) have evolved into advanced image generation tools,
especially for few-shot fine-tuning where a pretrained DM is fine-tuned on a
small set of images to capture specific styles or objects. Many people upload
these personalized checkpoints online, fostering communities such as Civitai
and HuggingFace. However, model owners may overlook the potential risks of data
leakage by releasing their fine-tuned checkpoints. Moreover, concerns regarding
copyright violations arise when unauthorized data is used during fine-tuning.
In this paper, we ask: ""Can training data be extracted from these fine-tuned
DMs shared online?"" A successful extraction would present not only data leakage
threats but also offer tangible evidence of copyright infringement. To answer
this, we propose FineXtract, a framework for extracting fine-tuning data. Our
method approximates fine-tuning as a gradual shift in the model's learned
distribution -- from the original pretrained DM toward the fine-tuning data. By
extrapolating the models before and after fine-tuning, we guide the generation
toward high-probability regions within the fine-tuned data distribution. We
then apply a clustering algorithm to extract the most probable images from
those generated using this extrapolated guidance. Experiments on DMs fine-tuned
with datasets such as WikiArt, DreamBooth, and real-world checkpoints posted
online validate the effectiveness of our method, extracting approximately 20%
of fine-tuning data in most cases, significantly surpassing baseline
performance.",http://arxiv.org/pdf/2410.03039v1
SPINE: Online Semantic Planning for Missions with Incomplete Natural Language Specifications in Unstructured Environments,"Zachary Ravichandran, Varun Murali, Mariliza Tzes, George J. Pappas, Vijay Kumar",2024-10-03,"As robots become increasingly capable, users will want to describe high-level
missions and have robots fill in the gaps. In many realistic settings,
pre-built maps are difficult to obtain, so execution requires exploration and
mapping that are necessary and specific to the mission. Consider an emergency
response scenario where a user commands a robot, ""triage impacted regions."" The
robot must infer relevant semantics (victims, etc.) and exploration targets
(damaged regions) based on priors or other context, then explore and refine its
plan online. These missions are incompletely specified, meaning they imply
subtasks and semantics. While many semantic planning methods operate online,
they are typically designed for well specified tasks such as object search or
exploration. Recently, Large Language Models (LLMs) have demonstrated powerful
contextual reasoning over a range of robotic tasks described in natural
language. However, existing LLM planners typically do not consider online
planning or complex missions; rather, relevant subtasks are provided by a
pre-built map or a user. We address these limitations via SPINE (online
Semantic Planner for missions with Incomplete Natural language specifications
in unstructured Environments). SPINE uses an LLM to reason about subtasks
implied by the mission then realizes these subtasks in a receding horizon
framework. Tasks are automatically validated for safety and refined online with
new observations. We evaluate SPINE in simulation and real-world settings.
Evaluation missions require multiple steps of semantic reasoning and
exploration in cluttered outdoor environments of over 20,000m$^2$ area. We
evaluate SPINE against competitive baselines in single-agent and air-ground
teaming applications. Please find videos and software on our project page:
https://zacravichandran.github.io/SPINE",http://arxiv.org/pdf/2410.03035v1
CounterQuill: Investigating the Potential of Human-AI Collaboration in Online Counterspeech Writing,"Xiaohan Ding, Kaike Ping, Uma Sushmitha Gunturi, Buse Carik, Sophia Stil, Lance T Wilhelm, Taufiq Daryanto, James Hawdon, Sang Won Lee, Eugenia H Rho",2024-10-03,"Online hate speech has become increasingly prevalent on social media
platforms, causing harm to individuals and society. While efforts have been
made to combat this issue through content moderation, the potential of
user-driven counterspeech as an alternative solution remains underexplored.
Existing counterspeech methods often face challenges such as fear of
retaliation and skill-related barriers. To address these challenges, we
introduce CounterQuill, an AI-mediated system that assists users in composing
effective and empathetic counterspeech. CounterQuill provides a three-step
process: (1) a learning session to help users understand hate speech and
counterspeech; (2) a brainstorming session that guides users in identifying key
elements of hate speech and exploring counterspeech strategies; and (3) a
co-writing session that enables users to draft and refine their counterspeech
with CounterQuill. We conducted a within-subjects user study with 20
participants to evaluate CounterQuill in comparison to ChatGPT. Results show
that CounterQuill's guidance and collaborative writing process provided users a
stronger sense of ownership over their co-authored counterspeech. Users
perceived CounterQuill as a writing partner and thus were more willing to post
the co-written counterspeech online compared to the one written with ChatGPT.",http://arxiv.org/pdf/2410.03032v1
Dynamic Sparse Training versus Dense Training: The Unexpected Winner in Image Corruption Robustness,"Boqian Wu, Qiao Xiao, Shunxin Wang, Nicola Strisciuglio, Mykola Pechenizkiy, Maurice van Keulen, Decebal Constantin Mocanu, Elena Mocanu",2024-10-03,"It is generally perceived that Dynamic Sparse Training opens the door to a
new era of scalability and efficiency for artificial neural networks at,
perhaps, some costs in accuracy performance for the classification task. At the
same time, Dense Training is widely accepted as being the ""de facto"" approach
to train artificial neural networks if one would like to maximize their
robustness against image corruption. In this paper, we question this general
practice. Consequently, we claim that, contrary to what is commonly thought,
the Dynamic Sparse Training methods can consistently outperform Dense Training
in terms of robustness accuracy, particularly if the efficiency aspect is not
considered as a main objective (i.e., sparsity levels between 10% and up to
50%), without adding (or even reducing) resource cost. We validate our claim on
two types of data, images and videos, using several traditional and modern deep
learning architectures for computer vision and three widely studied Dynamic
Sparse Training algorithms. Our findings reveal a new yet-unknown benefit of
Dynamic Sparse Training and open new possibilities in improving deep learning
robustness beyond the current state of the art.",http://arxiv.org/pdf/2410.03030v1
Flow Matching with Gaussian Process Priors for Probabilistic Time Series Forecasting,"Marcel Kollovieh, Marten Lienen, David Lüdke, Leo Schwinn, Stephan Günnemann",2024-10-03,"Recent advancements in generative modeling, particularly diffusion models,
have opened new directions for time series modeling, achieving state-of-the-art
performance in forecasting and synthesis. However, the reliance of
diffusion-based models on a simple, fixed prior complicates the generative
process since the data and prior distributions differ significantly. We
introduce TSFlow, a conditional flow matching (CFM) model for time series that
simplifies the generative problem by combining Gaussian processes, optimal
transport paths, and data-dependent prior distributions. By incorporating
(conditional) Gaussian processes, TSFlow aligns the prior distribution more
closely with the temporal structure of the data, enhancing both unconditional
and conditional generation. Furthermore, we propose conditional prior sampling
to enable probabilistic forecasting with an unconditionally trained model. In
our experimental evaluation on eight real-world datasets, we demonstrate the
generative capabilities of TSFlow, producing high-quality unconditional
samples. Finally, we show that both conditionally and unconditionally trained
models achieve competitive results in forecasting benchmarks, surpassing other
methods on 6 out of 8 datasets.",http://arxiv.org/pdf/2410.03024v1
Is Your Paper Being Reviewed by an LLM? Investigating AI Text Detectability in Peer Review,"Sungduk Yu, Man Luo, Avinash Madasu, Vasudev Lal, Phillip Howard",2024-10-03,"Peer review is a critical process for ensuring the integrity of published
scientific research. Confidence in this process is predicated on the assumption
that experts in the relevant domain give careful consideration to the merits of
manuscripts which are submitted for publication. With the recent rapid
advancements in the linguistic capabilities of large language models (LLMs), a
new potential risk to the peer review process is that negligent reviewers will
rely on LLMs to perform the often time consuming process of reviewing a paper.
In this study, we investigate the ability of existing AI text detection
algorithms to distinguish between peer reviews written by humans and different
state-of-the-art LLMs. Our analysis shows that existing approaches fail to
identify many GPT-4o written reviews without also producing a high number of
false positive classifications. To address this deficiency, we propose a new
detection approach which surpasses existing methods in the identification of
GPT-4o written peer reviews at low levels of false positive classifications.
Our work reveals the difficulty of accurately identifying AI-generated text at
the individual review level, highlighting the urgent need for new tools and
methods to detect this type of unethical application of generative AI.",http://arxiv.org/pdf/2410.03019v1
"Transforming Teachers' Roles and Agencies in the Era of Generative AI: Perceptions, Acceptance, Knowledge, and Practices",Xiaoming Zhai,2024-10-03,"This paper explores the transformative impact of Generative Artificial
Intelligence (GenAI) on teachers' roles and agencies in education, presenting a
comprehensive framework that addresses teachers' perceptions, knowledge,
acceptance, and practices of GenAI. As GenAI technologies, such as ChatGPT,
become increasingly integrated into educational settings, teachers are required
to adapt to evolving classroom dynamics, where AI plays a significant role in
content creation, personalized learning, and student engagement. However,
existing literature often treats these factors in isolation, overlooking how
they collectively influence teachers' ability to effectively integrate GenAI
into their pedagogical practices. This paper fills this gap by proposing a
framework that categorizes teachers into four roles -- Observer, Adopter,
Collaborator, and Innovator -- each representing different levels of GenAI
engagement, outlining teachers' agencies in GenAI classrooms. By highlighting
the need for continuous professional development and institutional support, we
demonstrate how teachers can evolve from basic GenAI users to co-creators of
knowledge alongside GenAI systems. The findings emphasize that for GenAI to
reach its full educational potential, teachers must not only accept and
understand its capabilities but also integrate it deeply into their teaching
strategies. This study contributes to the growing literature on GenAI in
education, offering practical implications for supporting teachers in
navigating the complexities of GenAI adoption.",http://arxiv.org/pdf/2410.03018v1
FastAdaSP: Multitask-Adapted Efficient Inference for Large Speech Language Model,"Yichen Lu, Jiaqi Song, Chao-Han Huck Yang, Shinji Watanabe",2024-10-03,"In this study, we aim to explore Multitask Speech Language Model (SpeechLM)
efficient inference via token reduction. Unlike other modalities such as vision
or text, speech has unique temporal dependencies, making previous efficient
inference works on other modalities not directly applicable. Furthermore,
methods for efficient SpeechLM inference on long sequence and sparse signals
remain largely unexplored. Then we propose FastAdaSP, a weighted token merging
framework specifically designed for various speech-related tasks to improve the
trade-off between efficiency and performance. Experimental results on WavLLM
and Qwen-Audio show that our method achieves the state-of-the-art (SOTA)
efficiency-performance trade-off compared with other baseline methods.
Specifically, FastAdaSP achieved 7x memory efficiency and 1.83x decoding
throughput without any degradation on tasks like Emotion Recognition (ER) and
Spoken Question Answering (SQA). The code will be available at
https://github.com/yichen14/FastAdaSP",http://arxiv.org/pdf/2410.03007v1
Task-unaware Lifelong Robot Learning with Retrieval-based Weighted Local Adaptation,"Pengzhi Yang, Xinyu Wang, Ruipeng Zhang, Cong Wang, Frans Oliehoek, Jens Kober",2024-10-03,"Real-world environments require robots to continuously acquire new skills
while retaining previously learned abilities, all without the need for clearly
defined task boundaries. Storing all past data to prevent forgetting is
impractical due to storage and privacy concerns. To address this, we propose a
method that efficiently restores a robot's proficiency in previously learned
tasks over its lifespan. Using an Episodic Memory (EM), our approach enables
experience replay during training and retrieval during testing for local
fine-tuning, allowing rapid adaptation to previously encountered problems
without explicit task identifiers. Additionally, we introduce a selective
weighting mechanism that emphasizes the most challenging segments of retrieved
demonstrations, focusing local adaptation where it is most needed. This
framework offers a scalable solution for lifelong learning in dynamic,
task-unaware environments, combining retrieval-based adaptation with selective
weighting to enhance robot performance in open-ended scenarios.",http://arxiv.org/pdf/2410.02995v1
Guided Stream of Search: Learning to Better Search with Language Models via Optimal Path Guidance,"Seungyong Moon, Bumsoo Park, Hyun Oh Song",2024-10-03,"While language models have demonstrated impressive capabilities across a
range of tasks, they still struggle with tasks that require complex planning
and reasoning. Recent studies have proposed training language models on search
processes rather than optimal solutions, resulting in better generalization
performance even though search processes are noisy and even suboptimal.
However, these studies overlook the value of optimal solutions, which can serve
as step-by-step landmarks to guide more effective search. In this work, we
explore how to leverage optimal solutions to enhance the search and planning
abilities of language models. To this end, we propose guided stream of search
(GSoS), which seamlessly incorporates optimal solutions into the
self-generation process in a progressive manner, producing high-quality search
trajectories. These trajectories are then distilled into the pre-trained model
via supervised fine-tuning. Our approach significantly enhances the search and
planning abilities of language models on Countdown, a simple yet challenging
mathematical reasoning task. Notably, combining our method with RL fine-tuning
yields further improvements, whereas previous supervised fine-tuning methods do
not benefit from RL. Furthermore, our approach exhibits greater effectiveness
than leveraging optimal solutions in the form of subgoal rewards.",http://arxiv.org/pdf/2410.02992v1
Differentiation and Specialization of Attention Heads via the Refined Local Learning Coefficient,"George Wang, Jesse Hoogland, Stan van Wingerden, Zach Furman, Daniel Murfet",2024-10-03,"We introduce refined variants of the Local Learning Coefficient (LLC), a
measure of model complexity grounded in singular learning theory, to study the
development of internal structure in transformer language models during
training. By applying these \textit{refined LLCs} (rLLCs) to individual
components of a two-layer attention-only transformer, we gain novel insights
into the progressive differentiation and specialization of attention heads. Our
methodology reveals how attention heads differentiate into distinct functional
roles over the course of training, analyzes the types of data these heads
specialize to process, and discovers a previously unidentified multigram
circuit. These findings demonstrate that rLLCs provide a principled,
quantitative toolkit for \textit{developmental interpretability}, which aims to
understand models through their evolution across the learning process. More
broadly, this work takes a step towards establishing the correspondence between
data distributional structure, geometric properties of the loss landscape,
learning dynamics, and emergent computational structures in neural networks.",http://arxiv.org/pdf/2410.02984v1
An explainable approach to detect case law on housing and eviction issues within the HUDOC database,"Mohammad Mohammadi, Martijn Wieling, Michel Vols",2024-10-03,"Case law is instrumental in shaping our understanding of human rights,
including the right to adequate housing. The HUDOC database provides access to
the textual content of case law from the European Court of Human Rights
(ECtHR), along with some metadata. While this metadata includes valuable
information, such as the application number and the articles addressed in a
case, it often lacks detailed substantive insights, such as the specific issues
a case covers. This underscores the need for detailed analysis to extract such
information. However, given the size of the database - containing over 40,000
cases - an automated solution is essential.
  In this study, we focus on the right to adequate housing and aim to build
models to detect cases related to housing and eviction issues. Our experiments
show that the resulting models not only provide performance comparable to more
sophisticated approaches but are also interpretable, offering explanations for
their decisions by highlighting the most influential words. The application of
these models led to the identification of new cases that were initially
overlooked during data collection. This suggests that NLP approaches can be
effectively applied to categorise case law based on the specific issues they
address.",http://arxiv.org/pdf/2410.02978v1
Harm Ratio: A Novel and Versatile Fairness Criterion,"Soroush Ebadian, Rupert Freeman, Nisarg Shah",2024-10-03,"Envy-freeness has become the cornerstone of fair division research. In
settings where each individual is allocated a disjoint share of collective
resources, it is a compelling fairness axiom which demands that no individual
strictly prefer the allocation of another individual to their own.
Unfortunately, in many real-life collective decision-making problems, the goal
is to choose a (common) public outcome that is equally applicable to all
individuals, and the notion of envy becomes vacuous. Consequently, this
literature has avoided studying fairness criteria that focus on individuals
feeling a sense of jealousy or resentment towards other individuals (rather
than towards the system), missing out on a key aspect of fairness.
  In this work, we propose a novel fairness criterion, individual harm ratio,
which is inspired by envy-freeness but applies to a broad range of collective
decision-making settings. Theoretically, we identify minimal conditions under
which this criterion and its groupwise extensions can be guaranteed, and study
the computational complexity of related problems. Empirically, we conduct
experiments with real data to show that our fairness criterion is powerful
enough to differentiate between prominent decision-making algorithms for a
range of tasks from voting and fair division to participatory budgeting and
peer review.",http://arxiv.org/pdf/2410.02977v1
F-Fidelity: A Robust Framework for Faithfulness Evaluation of Explainable AI,"Xu Zheng, Farhad Shirani, Zhuomin Chen, Chaohao Lin, Wei Cheng, Wenbo Guo, Dongsheng Luo",2024-10-03,"Recent research has developed a number of eXplainable AI (XAI) techniques.
Although extracting meaningful insights from deep learning models, how to
properly evaluate these XAI methods remains an open problem. The most widely
used approach is to perturb or even remove what the XAI method considers to be
the most important features in an input and observe the changes in the output
prediction. This approach although efficient suffers the Out-of-Distribution
(OOD) problem as the perturbed samples may no longer follow the original data
distribution. A recent method RemOve And Retrain (ROAR) solves the OOD issue by
retraining the model with perturbed samples guided by explanations. However,
the training may not always converge given the distribution difference.
Furthermore, using the model retrained based on XAI methods to evaluate these
explainers may cause information leakage and thus lead to unfair comparisons.
We propose Fine-tuned Fidelity F-Fidelity, a robust evaluation framework for
XAI, which utilizes i) an explanation-agnostic fine-tuning strategy, thus
mitigating the information leakage issue and ii) a random masking operation
that ensures that the removal step does not generate an OOD input. We designed
controlled experiments with state-of-the-art (SOTA) explainers and their
degraded version to verify the correctness of our framework. We conducted
experiments on multiple data structures, such as images, time series, and
natural language. The results demonstrate that F-Fidelity significantly
improves upon prior evaluation metrics in recovering the ground-truth ranking
of the explainers. Furthermore, we show both theoretically and empirically
that, given a faithful explainer, F-Fidelity metric can be used to compute the
sparsity of influential input components, i.e., to extract the true explanation
size.",http://arxiv.org/pdf/2410.02970v1

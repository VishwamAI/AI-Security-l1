title,authors,published,summary,url
Dorami: Privilege Separating Security Monitor on RISC-V TEEs,"Mark Kuhne, Stavros Volos, Shweta Shinde",2024-10-04,"TEE implementations on RISC-V offer an enclave abstraction by introducing a
trusted component called the security monitor (SM). The SM performs critical
tasks such as isolating enclaves from each other as well as from the OS by
using privileged ISA instructions that enforce the physical memory protection.
However, the SM executes at the highest privilege layer on the platform
(machine-mode) along side firmware that is not only large in size but also
includes third-party vendor code specific to the platform. In this paper, we
present Dorami - a privilege separation approach that isolates the SM from the
firmware thus reducing the attack surface on TEEs. Dorami re-purposes existing
ISA features to enforce its isolation and achieves its goals without large
overheads.",http://arxiv.org/pdf/2410.03653v1
Ward: Provable RAG Dataset Inference via LLM Watermarks,"Nikola Jovanović, Robin Staab, Maximilian Baader, Martin Vechev",2024-10-04,"Retrieval-Augmented Generation (RAG) improves LLMs by enabling them to
incorporate external data during generation. This raises concerns for data
owners regarding unauthorized use of their content in RAG systems. Despite its
importance, the challenge of detecting such unauthorized usage remains
underexplored, with existing datasets and methodologies from adjacent fields
being ill-suited for its study. In this work, we take several steps to bridge
this gap. First, we formalize this problem as (black-box) RAG Dataset Inference
(RAG-DI). To facilitate research on this challenge, we further introduce a
novel dataset specifically designed for benchmarking RAG-DI methods under
realistic conditions, and propose a set of baseline approaches. Building on
this foundation, we introduce Ward, a RAG-DI method based on LLM watermarks
that enables data owners to obtain rigorous statistical guarantees regarding
the usage of their dataset in a RAG system. In our experimental evaluation, we
show that Ward consistently outperforms all baselines across many challenging
settings, achieving higher accuracy, superior query efficiency and robustness.
Our work provides a foundation for future studies of RAG-DI and highlights LLM
watermarks as a promising approach to this problem.",http://arxiv.org/pdf/2410.03537v1
Gradient-based Jailbreak Images for Multimodal Fusion Models,"Javier Rando, Hannah Korevaar, Erik Brinkman, Ivan Evtimov, Florian Tramèr",2024-10-04,"Augmenting language models with image inputs may enable more effective
jailbreak attacks through continuous optimization, unlike text inputs that
require discrete optimization. However, new multimodal fusion models tokenize
all input modalities using non-differentiable functions, which hinders
straightforward attacks. In this work, we introduce the notion of a tokenizer
shortcut that approximates tokenization with a continuous function and enables
continuous optimization. We use tokenizer shortcuts to create the first
end-to-end gradient image attacks against multimodal fusion models. We evaluate
our attacks on Chameleon models and obtain jailbreak images that elicit harmful
information for 72.5% of prompts. Jailbreak images outperform text jailbreaks
optimized with the same objective and require 3x lower compute budget to
optimize 50x more input tokens. Finally, we find that representation
engineering defenses, like Circuit Breakers, trained only on text attacks can
effectively transfer to adversarial image inputs.",http://arxiv.org/pdf/2410.03489v1
A New World in the Depths of Microcrypt: Separating OWSGs and Quantum Money from QEFID,"Amit Behera, Giulio Malavolta, Tomoyuki Morimae, Tamer Mour, Takashi Yamakawa",2024-10-04,"While in classical cryptography, one-way functions (OWFs) are widely regarded
as the ""minimal assumption,"" the situation in quantum cryptography is less
clear. Recent works have put forward two concurrent candidates for the minimal
assumption in quantum cryptography: One-way state generators (OWSGs),
postulating the existence of a hard search problem with an efficient
verification algorithm, and EFI pairs, postulating the existence of a hard
distinguishing problem. Two recent papers [Khurana and Tomer STOC'24; Batra and
Jain FOCS'24] showed that OWSGs imply EFI pairs, but the reverse direction
remained open. In this work, we give strong evidence that the opposite
direction does not hold: We show that there is a quantum unitary oracle
relative to which EFI pairs exist, but OWSGs do not. In fact, we show a
slightly stronger statement that holds also for EFI pairs that output classical
bits (QEFID). As a consequence, we separate, via our oracle, QEFID, and one-way
puzzles from OWSGs and several other Microcrypt primitives, including
efficiently verifiable one-way puzzles and unclonable state generators. In
particular, this solves a problem left open in [Chung, Goldin, and Gray
Crypto'24]. Using similar techniques, we also establish a fully black-box
separation (which is slightly weaker than an oracle separation) between
private-key quantum money schemes and QEFID pairs. One conceptual implication
of our work is that the existence of an efficient verification algorithm may
lead to qualitatively stronger primitives in quantum cryptography.",http://arxiv.org/pdf/2410.03453v1
A Simple Framework for Secure Key Leasing,"Fuyuki Kitagawa, Tomoyuki Morimae, Takashi Yamakawa",2024-10-04,"Secure key leasing (a.k.a. key-revocable cryptography) enables us to lease a
cryptographic key as a quantum state in such a way that the key can be later
revoked in a verifiable manner. We propose a simple framework for constructing
cryptographic primitives with secure key leasing via the certified deletion
property of BB84 states. Based on our framework, we obtain the following
schemes.
  - A public key encryption scheme with secure key leasing that has classical
revocation based on any IND-CPA secure public key encryption scheme. Prior
works rely on either quantum revocation or stronger assumptions such as the
quantum hardness of the learning with errors (LWE) problem.
  - A pseudorandom function with secure key leasing that has classical
revocation based on one-way functions. Prior works rely on stronger assumptions
such as the quantum hardness of the LWE problem.
  - A digital signature scheme with secure key leasing that has classical
revocation based on the quantum hardness of the short integer solution (SIS)
problem. Our construction has static signing keys, i.e., the state of a signing
key almost does not change before and after signing. Prior constructions either
rely on non-static signing keys or indistinguishability obfuscation to achieve
a stronger goal of copy-protection.
  In addition, all of our schemes remain secure even if a verification key for
revocation is leaked after the adversary submits a valid certificate of
deletion. To our knowledge, all prior constructions are totally broken in this
setting. Moreover, in our view, our security proofs are much simpler than those
for existing schemes.",http://arxiv.org/pdf/2410.03413v1
Camel: Communication-Efficient and Maliciously Secure Federated Learning in the Shuffle Model of Differential Privacy,"Shuangqing Xu, Yifeng Zheng, Zhongyun Hua",2024-10-04,"Federated learning (FL) has rapidly become a compelling paradigm that enables
multiple clients to jointly train a model by sharing only gradient updates for
aggregation, without revealing their local private data. In order to protect
the gradient updates which could also be privacy-sensitive, there has been a
line of work studying local differential privacy (LDP) mechanisms to provide a
formal privacy guarantee. With LDP mechanisms, clients locally perturb their
gradient updates before sharing them out for aggregation. However, such
approaches are known for greatly degrading the model utility, due to heavy
noise addition. To enable a better privacy-utility tradeoff, a recently
emerging trend is to apply the shuffle model of DP in FL, which relies on an
intermediate shuffling operation on the perturbed gradient updates to achieve
privacy amplification. Following this trend, in this paper, we present Camel, a
new communication-efficient and maliciously secure FL framework in the shuffle
model of DP. Camel first departs from existing works by ambitiously supporting
integrity check for the shuffle computation, achieving security against
malicious adversary. Specifically, Camel builds on the trending cryptographic
primitive of secret-shared shuffle, with custom techniques we develop for
optimizing system-wide communication efficiency, and for lightweight integrity
checks to harden the security of server-side computation. In addition, we also
derive a significantly tighter bound on the privacy loss through analyzing the
Renyi differential privacy (RDP) of the overall FL process. Extensive
experiments demonstrate that Camel achieves better privacy-utility trade-offs
than the state-of-the-art work, with promising performance.",http://arxiv.org/pdf/2410.03407v1
Oracle Separation Between Quantum Commitments and Quantum One-wayness,"John Bostanci, Boyang Chen, Barak Nehoran",2024-10-04,"We show that there exists a unitary quantum oracle relative to which quantum
commitments exist but no (efficiently verifiable) one-way state generators
exist. Both have been widely considered candidates for replacing one-way
functions as the minimal assumption for cryptography: the weakest cryptographic
assumption implied by all of computational cryptography. Recent work has shown
that commitments can be constructed from one-way state generators, but the
other direction has remained open. Our results rule out any black-box
construction, and thus settle this crucial open problem, suggesting that
quantum commitments (as well as its equivalency class of EFI pairs, quantum
oblivious transfer, and secure quantum multiparty computation) appear to be
strictly weakest among all known cryptographic primitives.",http://arxiv.org/pdf/2410.03358v1
Practical Light Clients for Committee-Based Blockchains,"Frederik Armknecht, Ghassan Karame, Malcom Mohamed, Christiane Weis",2024-10-04,"Light clients are gaining increasing attention in the literature since they
obviate the need for users to set up dedicated blockchain full nodes. While the
literature features a number of light client instantiations, most light client
protocols optimize for long offline phases and implicitly assume that the block
headers to be verified are signed by highly dynamic validators.
  In this paper, we show that (i) most light clients are rarely offline for
more than a week, and (ii) validators are unlikely to drastically change in
most permissioned blockchains and in a number of permissionless blockchains,
such as Cosmos and Polkadot. Motivated by these findings, we propose a novel
practical system that optimizes for such realistic assumptions and achieves
minimal communication and computational costs for light clients when compared
to existing protocols. By means of a prototype implementation of our solution,
we show that our protocol achieves a reduction by up to $90$ and $40000\times$
(respectively) in end-to-end latency and up to $1000$ and $10000\times$
(respectively) smaller proof size when compared to two state-of-the-art light
client instantiations from the literature.",http://arxiv.org/pdf/2410.03347v1
AutoPenBench: Benchmarking Generative Agents for Penetration Testing,"Luca Gioacchini, Marco Mellia, Idilio Drago, Alexander Delsanto, Giuseppe Siracusano, Roberto Bifulco",2024-10-04,"Generative AI agents, software systems powered by Large Language Models
(LLMs), are emerging as a promising approach to automate cybersecurity tasks.
Among the others, penetration testing is a challenging field due to the task
complexity and the diverse strategies to simulate cyber-attacks. Despite
growing interest and initial studies in automating penetration testing with
generative agents, there remains a significant gap in the form of a
comprehensive and standard framework for their evaluation and development. This
paper introduces AutoPenBench, an open benchmark for evaluating generative
agents in automated penetration testing. We present a comprehensive framework
that includes 33 tasks, each representing a vulnerable system that the agent
has to attack. Tasks are of increasing difficulty levels, including in-vitro
and real-world scenarios. We assess the agent performance with generic and
specific milestones that allow us to compare results in a standardised manner
and understand the limits of the agent under test. We show the benefits of
AutoPenBench by testing two agent architectures: a fully autonomous and a
semi-autonomous supporting human interaction. We compare their performance and
limitations. For example, the fully autonomous agent performs unsatisfactorily
achieving a 21% Success Rate (SR) across the benchmark, solving 27% of the
simple tasks and only one real-world task. In contrast, the assisted agent
demonstrates substantial improvements, with 64% of SR. AutoPenBench allows us
also to observe how different LLMs like GPT-4o or OpenAI o1 impact the ability
of the agents to complete the tasks. We believe that our benchmark fills the
gap with a standard and flexible framework to compare penetration testing
agents on a common ground. We hope to extend AutoPenBench along with the
research community by making it available under
https://github.com/lucagioacchini/auto-pen-bench.",http://arxiv.org/pdf/2410.03225v1
An Intelligent Quantum Cyber-Security Framework for Healthcare Data Management,"Kishu Gupta, Deepika Saxena, Pooja Rani, Jitendra Kumar, Aaisha Makkar, Ashutosh Kumar Singh, Chung-Nan Lee",2024-10-04,"Digital healthcare is essential to facilitate consumers to access and
disseminate their medical data easily for enhanced medical care services.
However, the significant concern with digitalization across healthcare systems
necessitates for a prompt, productive, and secure storage facility along with a
vigorous communication strategy, to stimulate sensitive digital healthcare data
sharing and proactive estimation of malicious entities. In this context, this
paper introduces a comprehensive quantum-based framework to overwhelm the
potential security and privacy issues for secure healthcare data management. It
equips quantum encryption for the secured storage and dispersal of healthcare
data over the shared cloud platform by employing quantum encryption. Also, the
framework furnishes a quantum feed-forward neural network unit to examine the
intention behind the data request before granting access, for proactive
estimation of potential data breach. In this way, the proposed framework
delivers overall healthcare data management by coupling the advanced and more
competent quantum approach with machine learning to safeguard the data storage,
access, and prediction of malicious entities in an automated manner. Thus, the
proposed IQ-HDM leads to more cooperative and effective healthcare delivery and
empowers individuals with adequate custody of their health data. The
experimental evaluation and comparison of the proposed IQ-HDM framework with
state-of-the-art methods outline a considerable improvement up to 67.6%, in
tackling cyber threats related to healthcare data security.",http://arxiv.org/pdf/2410.03217v1
Research Directions for Verifiable Crypto-Physically Secure TEEs,Sylvain Bellemare,2024-10-04,"A niche corner of the Web3 world is increasingly making use of hardware-based
Trusted Execution Environments (TEEs) to build decentralized infrastructure.
One of the motivations to use TEEs is to go beyond the current performance
limitations of cryptography-based alternatives such as zero-knowledge proofs
(ZKP), fully homomorphic encryption (FHE), and multi-party computation (MPC).
Despite their appealing advantages, current TEEs suffer from serious
limitations as they are not secure against physical attacks, and their
attestation mechanism is rooted in the chip manufacturer's trust. As a result,
Web3 applications have to rely on cloud infrastruture to act as trusted
guardians of hardware-based TEEs and have to accept to trust chip
manufacturers. This work aims at exploring how we could potentially architect
and implement chips that would be secure against physical attacks and would not
require putting trust in chip manufacturers. One goal of this work is to
motivate the Web3 movement to acknowledge and leverage the substantial amount
of relevant hardware research that already exists. In brief, a combination of:
(1) physical unclonable functions (PUFs) to secure the root-of-trust; (2)
masking and redundancy techniques to secure computations; (3) open source
hardware and imaging techniques to verify that a chip matches its expected
design; can help move towards attesting that a given TEE can be trusted without
the need to trust a cloud provider and a chip manufacturer.",http://arxiv.org/pdf/2410.03183v1
Can Watermarked LLMs be Identified by Users via Crafted Prompts?,"Aiwei Liu, Sheng Guan, Yiming Liu, Leyi Pan, Yifei Zhang, Liancheng Fang, Lijie Wen, Philip S. Yu, Xuming Hu",2024-10-04,"Text watermarking for Large Language Models (LLMs) has made significant
progress in detecting LLM outputs and preventing misuse. Current watermarking
techniques offer high detectability, minimal impact on text quality, and
robustness to text editing. However, current researches lack investigation into
the imperceptibility of watermarking techniques in LLM services. This is
crucial as LLM providers may not want to disclose the presence of watermarks in
real-world scenarios, as it could reduce user willingness to use the service
and make watermarks more vulnerable to attacks. This work is the first to
investigate the imperceptibility of watermarked LLMs. We design an
identification algorithm called Water-Probe that detects watermarks through
well-designed prompts to the LLM. Our key motivation is that current
watermarked LLMs expose consistent biases under the same watermark key,
resulting in similar differences across prompts under different watermark keys.
Experiments show that almost all mainstream watermarking algorithms are easily
identified with our well-designed prompts, while Water-Probe demonstrates a
minimal false positive rate for non-watermarked LLMs. Finally, we propose that
the key to enhancing the imperceptibility of watermarked LLMs is to increase
the randomness of watermark key selection. Based on this, we introduce the
Water-Bag strategy, which significantly improves watermark imperceptibility by
merging multiple watermark keys.",http://arxiv.org/pdf/2410.03168v1
FedCert: Federated Accuracy Certification,"Minh Hieu Nguyen, Huu Tien Nguyen, Trung Thanh Nguyen, Manh Duong Nguyen, Trong Nghia Hoang, Truong Thao Nguyen, Phi Le Nguyen",2024-10-04,"Federated Learning (FL) has emerged as a powerful paradigm for training
machine learning models in a decentralized manner, preserving data privacy by
keeping local data on clients. However, evaluating the robustness of these
models against data perturbations on clients remains a significant challenge.
Previous studies have assessed the effectiveness of models in centralized
training based on certified accuracy, which guarantees that a certain
percentage of the model's predictions will remain correct even if the input
data is perturbed. However, the challenge of extending these evaluations to FL
remains unresolved due to the unknown client's local data. To tackle this
challenge, this study proposed a method named FedCert to take the first step
toward evaluating the robustness of FL systems. The proposed method is designed
to approximate the certified accuracy of a global model based on the certified
accuracy and class distribution of each client. Additionally, considering the
Non-Independent and Identically Distributed (Non-IID) nature of data in
real-world scenarios, we introduce the client grouping algorithm to ensure
reliable certified accuracy during the aggregation step of the approximation
algorithm. Through theoretical analysis, we demonstrate the effectiveness of
FedCert in assessing the robustness and reliability of FL systems. Moreover,
experimental results on the CIFAR-10 and CIFAR-100 datasets under various
scenarios show that FedCert consistently reduces the estimation error compared
to baseline methods. This study offers a solution for evaluating the robustness
of FL systems and lays the groundwork for future research to enhance the
dependability of decentralized learning. The source code is available at
https://github.com/thanhhff/FedCert/.",http://arxiv.org/pdf/2410.03067v1
Towards Universal Certified Robustness with Multi-Norm Training,"Enyi Jiang, Gagandeep Singh",2024-10-03,"Existing certified training methods can only train models to be robust
against a certain perturbation type (e.g. $l_\infty$ or $l_2$). However, an
$l_\infty$ certifiably robust model may not be certifiably robust against $l_2$
perturbation (and vice versa) and also has low robustness against other
perturbations (e.g. geometric transformation). To this end, we propose the
first multi-norm certified training framework \textbf{CURE}, consisting of a
new $l_2$ deterministic certified training defense and several multi-norm
certified training methods, to attain better \emph{union robustness} when
training from scratch or fine-tuning a pre-trained certified model. Further, we
devise bound alignment and connect natural training with certified training for
better union robustness. Compared with SOTA certified training, \textbf{CURE}
improves union robustness up to $22.8\%$ on MNIST, $23.9\%$ on CIFAR-10, and
$8.0\%$ on TinyImagenet. Further, it leads to better generalization on a
diverse set of challenging unseen geometric perturbations, up to $6.8\%$ on
CIFAR-10. Overall, our contributions pave a path towards \textit{universal
certified robustness}.",http://arxiv.org/pdf/2410.03000v1
A Simple Method for Secret-Key Generation Between Mobile Users Across Networks,Yingbo Hua,2024-10-03,"Two or more mobiles users can continuously superimpose sequences of bits
chosen from different packets or files already exchanged and authenticated
between themselves to continuously renew a secret key for continuous
strengthening of their privacy and authentication. This accumulative, adaptable
and additive (AAA) method is discussed in this paper. The equivocation to Eve
of any bit in the generated key by the AAA method equals to the probability
that not all corresponding independent bits exchanged between the users are
intercepted by Eve. This performance, achieved without using any knowledge of
non-stationary probabilities of bits being intercepted by Eve, is compared to
an established capacity achievable using that knowledge. A secrecy robustness
of the AAA method against some correlations known to Eve is also discussed.",http://arxiv.org/pdf/2410.02964v1
Safeguard is a Double-edged Sword: Denial-of-service Attack on Large Language Models,"Qingzhao Zhang, Ziyang Xiong, Z. Morley Mao",2024-10-03,"Safety is a paramount concern of large language models (LLMs) in their open
deployment. To this end, safeguard methods aim to enforce the ethical and
responsible use of LLMs through safety alignment or guardrail mechanisms.
However, we found that the malicious attackers could exploit false positives of
safeguards, i.e., fooling the safeguard model to block safe content mistakenly,
leading to a new denial-of-service (DoS) attack on LLMs. Specifically, by
software or phishing attacks on user client software, attackers insert a short,
seemingly innocuous adversarial prompt into to user prompt templates in
configuration files; thus, this prompt appears in final user requests without
visibility in the user interface and is not trivial to identify. By designing
an optimization process that utilizes gradient and attention information, our
attack can automatically generate seemingly safe adversarial prompts,
approximately only 30 characters long, that universally block over 97\% of user
requests on Llama Guard 3. The attack presents a new dimension of evaluating
LLM safeguards focusing on false positives, fundamentally different from the
classic jailbreak.",http://arxiv.org/pdf/2410.02916v1
Fine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation,"Xianzhi Li, Ran Zmigrod, Zhiqiang Ma, Xiaomo Liu, Xiaodan Zhu",2024-10-03,"Language models are capable of memorizing detailed patterns and information,
leading to a double-edged effect: they achieve impressive modeling performance
on downstream tasks with the stored knowledge but also raise significant
privacy concerns. Traditional differential privacy based training approaches
offer robust safeguards by employing a uniform noise distribution across all
parameters. However, this overlooks the distinct sensitivities and
contributions of individual parameters in privacy protection and often results
in suboptimal models. To address these limitations, we propose ANADP, a novel
algorithm that adaptively allocates additive noise based on the importance of
model parameters. We demonstrate that ANADP narrows the performance gap between
regular fine-tuning and traditional DP fine-tuning on a series of datasets
while maintaining the required privacy constraints.",http://arxiv.org/pdf/2410.02912v1
Universally Optimal Watermarking Schemes for LLMs: from Theory to Practice,"Haiyun He, Yepeng Liu, Ziqiao Wang, Yongyi Mao, Yuheng Bu",2024-10-03,"Large Language Models (LLMs) boosts human efficiency but also poses misuse
risks, with watermarking serving as a reliable method to differentiate
AI-generated content from human-created text. In this work, we propose a novel
theoretical framework for watermarking LLMs. Particularly, we jointly optimize
both the watermarking scheme and detector to maximize detection performance,
while controlling the worst-case Type-I error and distortion in the watermarked
text. Within our framework, we characterize the universally minimum Type-II
error, showing a fundamental trade-off between detection performance and
distortion. More importantly, we identify the optimal type of detectors and
watermarking schemes. Building upon our theoretical analysis, we introduce a
practical, model-agnostic and computationally efficient token-level
watermarking algorithm that invokes a surrogate model and the Gumbel-max trick.
Empirical results on Llama-13B and Mistral-8$\times$7B demonstrate the
effectiveness of our method. Furthermore, we also explore how robustness can be
integrated into our theoretical framework, which provides a foundation for
designing future watermarking systems with improved resilience to adversarial
attacks.",http://arxiv.org/pdf/2410.02890v1
SteerDiff: Steering towards Safe Text-to-Image Diffusion Models,"Hongxiang Zhang, Yifeng He, Hao Chen",2024-10-03,"Text-to-image (T2I) diffusion models have drawn attention for their ability
to generate high-quality images with precise text alignment. However, these
models can also be misused to produce inappropriate content. Existing safety
measures, which typically rely on text classifiers or ControlNet-like
approaches, are often insufficient. Traditional text classifiers rely on
large-scale labeled datasets and can be easily bypassed by rephrasing. As
diffusion models continue to scale, fine-tuning these safeguards becomes
increasingly challenging and lacks flexibility. Recent red-teaming attack
researches further underscore the need for a new paradigm to prevent the
generation of inappropriate content. In this paper, we introduce SteerDiff, a
lightweight adaptor module designed to act as an intermediary between user
input and the diffusion model, ensuring that generated images adhere to ethical
and safety standards with little to no impact on usability. SteerDiff
identifies and manipulates inappropriate concepts within the text embedding
space to guide the model away from harmful outputs. We conduct extensive
experiments across various concept unlearning tasks to evaluate the
effectiveness of our approach. Furthermore, we benchmark SteerDiff against
multiple red-teaming strategies to assess its robustness. Finally, we explore
the potential of SteerDiff for concept forgetting tasks, demonstrating its
versatility in text-conditioned image generation.",http://arxiv.org/pdf/2410.02710v1
Discovering Clues of Spoofed LM Watermarks,"Thibaud Gloaguen, Nikola Jovanović, Robin Staab, Martin Vechev",2024-10-03,"LLM watermarks stand out as a promising way to attribute ownership of
LLM-generated text. One threat to watermark credibility comes from spoofing
attacks, where an unauthorized third party forges the watermark, enabling it to
falsely attribute arbitrary texts to a particular LLM. While recent works have
demonstrated that state-of-the-art schemes are in fact vulnerable to spoofing,
they lack deeper qualitative analysis of the texts produced by spoofing
methods. In this work, we for the first time reveal that there are observable
differences between genuine and spoofed watermark texts. Namely, we show that
regardless of their underlying approach, all current spoofing methods
consistently leave observable artifacts in spoofed texts, indicative of
watermark forgery. We build upon these findings to propose rigorous statistical
tests that reliably reveal the presence of such artifacts, effectively
discovering that a watermark was spoofed. Our experimental evaluation shows
high test power across all current spoofing methods, providing insights into
their fundamental limitations, and suggesting a way to mitigate this threat.",http://arxiv.org/pdf/2410.02693v1
Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents,"Hanrong Zhang, Jingyuan Huang, Kai Mei, Yifei Yao, Zhenting Wang, Chenlu Zhan, Hongwei Wang, Yongfeng Zhang",2024-10-03,"Although LLM-based agents, powered by Large Language Models (LLMs), can use
external tools and memory mechanisms to solve complex real-world tasks, they
may also introduce critical security vulnerabilities. However, the existing
literature does not comprehensively evaluate attacks and defenses against
LLM-based agents. To address this, we introduce Agent Security Bench (ASB), a
comprehensive framework designed to formalize, benchmark, and evaluate the
attacks and defenses of LLM-based agents, including 10 scenarios (e.g.,
e-commerce, autonomous driving, finance), 10 agents targeting the scenarios,
over 400 tools, 23 different types of attack/defense methods, and 8 evaluation
metrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory
poisoning attack, a novel Plan-of-Thought backdoor attack, a mixed attack, and
10 corresponding defenses across 13 LLM backbones with nearly 90,000 testing
cases in total. Our benchmark results reveal critical vulnerabilities in
different stages of agent operation, including system prompt, user prompt
handling, tool usage, and memory retrieval, with the highest average attack
success rate of 84.30\%, but limited effectiveness shown in current defenses,
unveiling important works to be done in terms of agent security for the
community. Our code can be found at https://github.com/agiresearch/ASB.",http://arxiv.org/pdf/2410.02644v1
Inapproximability of Sparsest Vector in a Real Subspace,"Vijay Bhattiprolu, Euiwoong Lee",2024-10-03,"We establish strong inapproximability for finding the sparsest nonzero vector
in a real subspace. We show that it is NP-Hard (under randomized reductions) to
approximate the sparsest vector in a subspace within any constant factor (or
almost polynomial factors in quasipolynomial time). We recover as a corollary
state of the art inapproximability for the shortest vector problem (SVP), a
foundational problem in lattice based cryptography. Our proof is surprisingly
simple, bypassing even the PCP theorem. We are inspired by the homogenization
framework from the inapproximability theory of minimum distance problems (MDC)
in integer lattices and error correcting codes. We use a combination of (a)
\emph{product testing via tensor codes} and (b) \emph{encoding an assignment as
a coset of a random code in higher dimensional space} in order to embed
non-homogeneous quadratic equations into the sparsest vector problem. (a) is
inspired by Austrin and Khot's simplified proof of hardness of MDC over finite
fields, and (b) is inspired by Micciancio's semi-derandomization of hardness of
SVP. Our reduction involves the challenge of performing (a) over the reals. We
prove that tensoring of the kernel of a +1/-1 random matrix furnishes an
adequate product test (while still allowing (b)). The proof exposes a
connection to Littlewood-Offord theory and relies on a powerful
anticoncentration result of Rudelson and Vershynin. Our main motivation in this
work is the development of inapproximability theory for problems over the
reals. Analytic variants of sparsest vector have connections to small set
expansion, quantum separability and polynomial maximization over convex sets,
all of which cause similar barriers to inapproximability. The approach we
develop could lead to progress on the hardness of some of these problems.",http://arxiv.org/pdf/2410.02636v1
Assessing the Viability of Synthetic Physical Copy Detection Patterns on Different Imaging Systems,"Roman Chaban, Brian Pulfer, Slava Voloshynovskiy",2024-10-03,"This paper explores the potential of synthetic physical Copy Detection
Patterns (CDP) to improve the robustness of anti-counterfeiting systems. By
leveraging synthetic physical CDP, we aim at enhancing security and
cost-effectiveness across various real-world applications. Our research
demonstrates that synthetic CDP offer substantial improvements in
authentication accuracy compared to one based on traditional digital templates.
We conducted extensive tests using both a scanner and a diverse range of mobile
phones, validating our approach through ROC analysis. The results indicate that
synthetic CDP can reliably differentiate between original and fake samples,
making this approach a viable solution for real-world applications, though
requires an additional research to make this technology scalable across a
variety of imaging devices.",http://arxiv.org/pdf/2410.02575v1
Exploiting HDMI and USB Ports for GPU Side-Channel Insights,"Sayed Erfan Arefin, Abdul Serwadda",2024-10-03,"Modern computers rely on USB and HDMI ports for connecting external
peripherals and display devices. Despite their built-in security measures,
these ports remain susceptible to passive power-based side-channel attacks.
This paper presents a new class of attacks that exploit power consumption
patterns at these ports to infer GPU activities. We develop a custom device
that plugs into these ports and demonstrate that its high-resolution power
measurements can drive successful inferences about GPU processes, such as
neural network computations and video rendering. The ubiquitous presence of USB
and HDMI ports allows for discreet placement of the device, and its
non-interference with data channels ensures that no security alerts are
triggered. Our findings underscore the need to reevaluate and strengthen the
current generation of HDMI and USB port security defenses.",http://arxiv.org/pdf/2410.02539v1
An Edge-Computing based Industrial Gateway for Industry 4.0 using ARM TrustZone Technology,Sandeep Gupta,2024-10-03,"Secure and efficient communication to establish a seamless nexus between the
five levels of a typical automation pyramid is paramount to Industry 4.0.
Specifically, vertical and horizontal integration of these levels is an
overarching requirement to accelerate productivity and improve operational
activities. Vertical integration can improve visibility, flexibility, and
productivity by connecting systems and applications. Horizontal integration can
provide better collaboration and adaptability by connecting internal production
facilities, multi-site operations, and third-party partners in a supply chain.
In this paper, we propose an Edge-computing-based Industrial Gateway for
interfacing information technology and operational technology that can enable
Industry 4.0 vertical and horizontal integration. Subsequently, we design and
develop a working prototype to demonstrate a remote production-line maintenance
use case with a strong focus on security aspects and the edge paradigm to bring
computational resources and data storage closer to data sources.",http://arxiv.org/pdf/2410.02529v1
Encryption-Friendly LLM Architecture,"Donghwan Rho, Taeseong Kim, Minje Park, Jung Woo Kim, Hyunsik Chae, Jung Hee Cheon, Ernest K. Ryu",2024-10-03,"Large language models (LLMs) offer personalized responses based on user
interactions, but this use case raises serious privacy concerns. Homomorphic
encryption (HE) is a cryptographic protocol supporting arithmetic computations
in encrypted states and provides a potential solution for privacy-preserving
machine learning (PPML). However, the computational intensity of transformers
poses challenges for applying HE to LLMs. In this work, we propose a modified
HE-friendly transformer architecture with an emphasis on inference following
personalized (private) fine-tuning. Utilizing LoRA fine-tuning and Gaussian
kernels, we achieve significant computational speedups -- 6.94x for fine-tuning
and 2.3x for inference -- while maintaining performance comparable to plaintext
models. Our findings provide a viable proof of concept for offering
privacy-preserving LLM services in areas where data protection is crucial.",http://arxiv.org/pdf/2410.02486v1
Towards a Theoretical Understanding of Memorization in Diffusion Models,"Yunhao Chen, Xingjun Ma, Difan Zou, Yu-Gang Jiang",2024-10-03,"As diffusion probabilistic models (DPMs) are being employed as mainstream
models for Generative Artificial Intelligence (GenAI), the study of their
memorization of training data has attracted growing attention. Existing works
in this direction aim to establish an understanding of whether or to what
extent DPMs learn via memorization. Such an understanding is crucial for
identifying potential risks of data leakage and copyright infringement in
diffusion models and, more importantly, for trustworthy application of GenAI.
Existing works revealed that conditional DPMs are more prone to training data
memorization than unconditional DPMs, and the motivated data extraction methods
are mostly for conditional DPMs. However, these understandings are primarily
empirical, and extracting training data from unconditional models has been
found to be extremely challenging. In this work, we provide a theoretical
understanding of memorization in both conditional and unconditional DPMs under
the assumption of model convergence. Our theoretical analysis indicates that
extracting data from unconditional models can also be effective by constructing
a proper surrogate condition. Based on this result, we propose a novel data
extraction method named \textbf{Surrogate condItional Data Extraction (SIDE)}
that leverages a time-dependent classifier trained on the generated data as a
surrogate condition to extract training data from unconditional DPMs. Empirical
results demonstrate that our SIDE can extract training data in challenging
scenarios where previous methods fail, and it is, on average, over 50\% more
effective across different scales of the CelebA dataset.",http://arxiv.org/pdf/2410.02467v1
Demonstration Attack against In-Context Learning for Code Intelligence,"Yifei Ge, Weisong Sun, Yihang Lou, Chunrong Fang, Yiran Zhang, Yiming Li, Xiaofang Zhang, Yang Liu, Zhihong Zhao, Zhenyu Chen",2024-10-03,"Recent advancements in large language models (LLMs) have revolutionized code
intelligence by improving programming productivity and alleviating challenges
faced by software developers. To further improve the performance of LLMs on
specific code intelligence tasks and reduce training costs, researchers reveal
a new capability of LLMs: in-context learning (ICL). ICL allows LLMs to learn
from a few demonstrations within a specific context, achieving impressive
results without parameter updating. However, the rise of ICL introduces new
security vulnerabilities in the code intelligence field. In this paper, we
explore a novel security scenario based on the ICL paradigm, where attackers
act as third-party ICL agencies and provide users with bad ICL content to
mislead LLMs outputs in code intelligence tasks. Our study demonstrates the
feasibility and risks of such a scenario, revealing how attackers can leverage
malicious demonstrations to construct bad ICL content and induce LLMs to
produce incorrect outputs, posing significant threats to system security. We
propose a novel method to construct bad ICL content called DICE, which is
composed of two stages: Demonstration Selection and Bad ICL Construction,
constructing targeted bad ICL content based on the user query and transferable
across different query inputs. Ultimately, our findings emphasize the critical
importance of securing ICL mechanisms to protect code intelligence systems from
adversarial manipulation.",http://arxiv.org/pdf/2410.02841v1
Towards a Self-rescuing System for UAVs Under GNSS Attack,"Giulio Rigoni, Nicola Scremin, Mauro Conti",2024-10-03,"There has been substantial growth in the UAV market along with an expansion
in their applications. However, the successful execution of a UAV mission is
very often dependent on the use of a GNSS. Unfortunately, the vulnerability of
GNSS signals, due to their lack of encryption and authentication, poses a
significant cybersecurity issue. This vulnerability makes various attacks,
particularly the ""GNSS spoofing attack,"" and ""GNSS jamming attack"" easily
executable. Generally speaking, during this attack, the drone is manipulated
into altering its path, usually resulting in an immediate forced landing or
crash. As far as we know, we are the first to propose a lightweight-solution
that enable a drone to autonomously rescue itself, assuming it is under GNSS
attack and the GNSS is no longer available, and return safely to its initial
takeoff position, thereby preventing any potential crashes. During the flight,
wind plays a critical role as it can instantaneously alter the drone's
position. To solve this problem, we have devised a highly effective 2-phases
solution: (i) Forward Phase, for monitoring and recording the forward journey,
and (ii) Backward Phase, that generates a backward route, based on the Forward
Phase and wind presence. The final solution ensures strong performance in
consistently returning the drone to the original position, even in wind
situations, while maintaining a very fast computation time.",http://arxiv.org/pdf/2410.02442v1
Optimizing Adaptive Attacks against Content Watermarks for Language Models,"Abdulrahman Diaa, Toluwani Aremu, Nils Lukas",2024-10-03,"Large Language Models (LLMs) can be \emph{misused} to spread online spam and
misinformation. Content watermarking deters misuse by hiding a message in
model-generated outputs, enabling their detection using a secret watermarking
key. Robustness is a core security property, stating that evading detection
requires (significant) degradation of the content's quality. Many LLM
watermarking methods have been proposed, but robustness is tested only against
\emph{non-adaptive} attackers who lack knowledge of the watermarking method and
can find only suboptimal attacks. We formulate the robustness of LLM
watermarking as an objective function and propose preference-based optimization
to tune \emph{adaptive} attacks against the specific watermarking method. Our
evaluation shows that (i) adaptive attacks substantially outperform
non-adaptive baselines. (ii) Even in a non-adaptive setting, adaptive attacks
optimized against a few known watermarks remain highly effective when tested
against other unseen watermarks, and (iii) optimization-based attacks are
practical and require less than seven GPU hours. Our findings underscore the
need to test robustness against adaptive attackers.",http://arxiv.org/pdf/2410.02440v1
Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models,"Guobin Shen, Dongcheng Zhao, Yiting Dong, Xiang He, Yi Zeng",2024-10-03,"As large language models (LLMs) become integral to various applications,
ensuring both their safety and utility is paramount. Jailbreak attacks, which
manipulate LLMs into generating harmful content, pose significant challenges to
this balance. Existing defenses, such as prompt engineering and safety
fine-tuning, often introduce computational overhead, increase inference
latency, and lack runtime flexibility. Moreover, overly restrictive safety
measures can degrade model utility by causing refusals of benign queries. In
this paper, we introduce Jailbreak Antidote, a method that enables real-time
adjustment of LLM safety preferences by manipulating a sparse subset of the
model's internal states during inference. By shifting the model's hidden
representations along a safety direction with varying strengths, we achieve
flexible control over the safety-utility balance without additional token
overhead or inference delays. Our analysis reveals that safety-related
information in LLMs is sparsely distributed; adjusting approximately 5% of the
internal state is as effective as modifying the entire state. Extensive
experiments on nine LLMs (ranging from 2 billion to 72 billion parameters),
evaluated against ten jailbreak attack methods and compared with six defense
strategies, validate the effectiveness and efficiency of our approach. By
directly manipulating internal states during reasoning, Jailbreak Antidote
offers a lightweight, scalable solution that enhances LLM safety while
preserving utility, opening new possibilities for real-time safety mechanisms
in widely-deployed AI systems.",http://arxiv.org/pdf/2410.02298v1
Alignment of Cybersecurity Incident Prioritisation with Incident Response Management Maturity Capabilities,"Abdulaziz Gulay, Leandros Maglaras",2024-10-03,"The increasing frequency and sophistication of cybersecurity incidents pose
significant challenges to organisations, highlighting the critical need for
robust incident response capabilities. This paper explores a possible
utilisation of IR CMMs assessments to systematically prioritise incidents based
on their impact, severity, and the incident response capabilities of an
organisation in specific areas associated with human and organisational
factors. The findings reveal common weaknesses in incident response, such as
inadequate training and poor communication, and highlight best practices,
including regular training programs, clear communication protocols, and
well-documented response procedures. The analysis also emphasises the
importance of organisational culture in enhancing incident response
capabilities. By addressing the gap in understanding how the output of IRM
assessments can be immediately utilised to prioritise high-risk incidents, this
paper contributes valuable insights to academia and practice, offering a
structured approach to enhancing organisational resilience against
cybersecurity threats.",http://arxiv.org/pdf/2410.02259v1
MTDNS: Moving Target Defense for Resilient DNS Infrastructure,"Abdullah Aydeger, Pei Zhou, Sanzida Hoque, Marco Carvalho, Engin Zeydan",2024-10-03,"One of the most critical components of the Internet that an attacker could
exploit is the DNS (Domain Name System) protocol and infrastructure.
Researchers have been constantly developing methods to detect and defend
against the attacks against DNS, specifically DNS flooding attacks. However,
most solutions discard packets for defensive approaches, which can cause
legitimate packets to be dropped, making them highly dependable on detection
strategies. In this paper, we propose MTDNS, a resilient MTD-based approach
that employs Moving Target Defense techniques through Software Defined
Networking (SDN) switches to redirect traffic to alternate DNS servers that are
dynamically created and run under the Network Function Virtualization (NFV)
framework. The proposed approach is implemented in a testbed environment by
running our DNS servers as separate Virtual Network Functions, NFV Manager, SDN
switches, and an SDN Controller. The experimental result shows that the MTDNS
approach achieves a much higher success rate in resolving DNS queries and
significantly reduces average latency even if there is a DNS flooding attack.",http://arxiv.org/pdf/2410.02254v1
Mitigating Downstream Model Risks via Model Provenance,"Keyu Wang, Abdullah Norozi Iranzad, Scott Schaffter, Doina Precup, Jonathan Lebensold",2024-10-03,"Research and industry are rapidly advancing the innovation and adoption of
foundation model-based systems, yet the tools for managing these models have
not kept pace. Understanding the provenance and lineage of models is critical
for researchers, industry, regulators, and public trust. While model cards and
system cards were designed to provide transparency, they fall short in key
areas: tracing model genealogy, enabling machine readability, offering reliable
centralized management systems, and fostering consistent creation incentives.
This challenge mirrors issues in software supply chain security, but AI/ML
remains at an earlier stage of maturity. Addressing these gaps requires
industry-standard tooling that can be adopted by foundation model publishers,
open-source model innovators, and major distribution platforms. We propose a
machine-readable model specification format to simplify the creation of model
records, thereby reducing error-prone human effort, notably when a new model
inherits most of its design from a foundation model. Our solution explicitly
traces relationships between upstream and downstream models, enhancing
transparency and traceability across the model lifecycle. To facilitate the
adoption, we introduce the unified model record (UMR) repository , a
semantically versioned system that automates the publication of model records
to multiple formats (PDF, HTML, LaTeX) and provides a hosted web interface
(https://modelrecord.com/). This proof of concept aims to set a new standard
for managing foundation models, bridging the gap between innovation and
responsible model management.",http://arxiv.org/pdf/2410.02230v1
The Role of piracy in quantum proofs,"Anne Broadbent, Alex B. Grilo, Supartha Podder, Jamie Sikora",2024-10-03,"A well-known feature of quantum information is that it cannot, in general, be
cloned. Recently, a number of quantum-enabled information-processing tasks have
demonstrated various forms of uncloneability; among these forms, piracy is an
adversarial model that gives maximal power to the adversary, in controlling
both a cloning-type attack, as well as the evaluation/verification stage. Here,
we initiate the study of anti-piracy proof systems, which are proof systems
that inherently prevent piracy attacks. We define anti-piracy proof systems,
demonstrate such a proof system for an oracle problem, and also describe a
candidate anti-piracy proof system for NP. We also study quantum proof systems
that are cloneable and settle the famous QMA vs. QMA(2) debate in this setting.
Lastly, we discuss how one can approach the QMA vs. QCMA question, by studying
its cloneable variants.",http://arxiv.org/pdf/2410.02228v1
Buckle Up: Robustifying LLMs at Every Customization Stage via Data Curation,"Xiaoqun Liu, Jiacheng Liang, Luoxi Tang, Chenyu You, Muchao Ye, Zhaohan Xi",2024-10-03,"Large language models (LLMs) are extensively adapted for downstream
applications through a process known as ""customization,"" with fine-tuning being
a common method for integrating domain-specific expertise. However, recent
studies have revealed a vulnerability that tuning LLMs with malicious samples
can compromise their robustness and amplify harmful content, an attack known as
""jailbreaking."" To mitigate such attack, we propose an effective defensive
framework utilizing data curation to revise commonsense texts and enhance their
safety implication from the perspective of LLMs. The curated texts can mitigate
jailbreaking attacks at every stage of the customization process: before
customization to immunize LLMs against future jailbreak attempts, during
customization to neutralize jailbreaking risks, or after customization to
restore the compromised models. Since the curated data strengthens LLMs through
the standard fine-tuning workflow, we do not introduce additional modules
during LLM inference, thereby preserving the original customization process.
Experimental results demonstrate a substantial reduction in jailbreaking
effects, with up to a 100% success in generating responsible responses.
Notably, our method is effective even with commonsense texts, which are often
more readily available than safety-relevant data. With the every-stage
defensive framework and supporting experimental performance, this work
represents a significant advancement in mitigating jailbreaking risks and
ensuring the secure customization of LLMs.",http://arxiv.org/pdf/2410.02220v2
BACKTIME: Backdoor Attacks on Multivariate Time Series Forecasting,"Xiao Lin, Zhining Liu, Dongqi Fu, Ruizhong Qiu, Hanghang Tong",2024-10-03,"Multivariate Time Series (MTS) forecasting is a fundamental task with
numerous real-world applications, such as transportation, climate, and
epidemiology. While a myriad of powerful deep learning models have been
developed for this task, few works have explored the robustness of MTS
forecasting models to malicious attacks, which is crucial for their trustworthy
employment in high-stake scenarios. To address this gap, we dive deep into the
backdoor attacks on MTS forecasting models and propose an effective attack
method named BackTime.By subtly injecting a few stealthy triggers into the MTS
data, BackTime can alter the predictions of the forecasting model according to
the attacker's intent. Specifically, BackTime first identifies vulnerable
timestamps in the data for poisoning, and then adaptively synthesizes stealthy
and effective triggers by solving a bi-level optimization problem with a
GNN-based trigger generator. Extensive experiments across multiple datasets and
state-of-the-art MTS forecasting models demonstrate the effectiveness,
versatility, and stealthiness of \method{} attacks. The code is available at
\url{https://github.com/xiaolin-cs/BackTime}.",http://arxiv.org/pdf/2410.02195v1
BadCM: Invisible Backdoor Attack Against Cross-Modal Learning,"Zheng Zhang, Xu Yuan, Lei Zhu, Jingkuan Song, Liqiang Nie",2024-10-03,"Despite remarkable successes in unimodal learning tasks, backdoor attacks
against cross-modal learning are still underexplored due to the limited
generalization and inferior stealthiness when involving multiple modalities.
Notably, since works in this area mainly inherit ideas from unimodal visual
attacks, they struggle with dealing with diverse cross-modal attack
circumstances and manipulating imperceptible trigger samples, which hinders
their practicability in real-world applications. In this paper, we introduce a
novel bilateral backdoor to fill in the missing pieces of the puzzle in the
cross-modal backdoor and propose a generalized invisible backdoor framework
against cross-modal learning (BadCM). Specifically, a cross-modal mining scheme
is developed to capture the modality-invariant components as target poisoning
areas, where well-designed trigger patterns injected into these regions can be
efficiently recognized by the victim models. This strategy is adapted to
different image-text cross-modal models, making our framework available to
various attack scenarios. Furthermore, for generating poisoned samples of high
stealthiness, we conceive modality-specific generators for visual and
linguistic modalities that facilitate hiding explicit trigger patterns in
modality-invariant regions. To the best of our knowledge, BadCM is the first
invisible backdoor method deliberately designed for diverse cross-modal attacks
within one unified framework. Comprehensive experimental evaluations on two
typical applications, i.e., cross-modal retrieval and VQA, demonstrate the
effectiveness and generalization of our method under multiple kinds of attack
scenarios. Moreover, we show that BadCM can robustly evade existing backdoor
defenses. Our code is available at https://github.com/xandery-geek/BadCM.",http://arxiv.org/pdf/2410.02182v1
Controlled Generation of Natural Adversarial Documents for Stealthy Retrieval Poisoning,"Collin Zhang, Tingwei Zhang, Vitaly Shmatikov",2024-10-03,"Recent work showed that retrieval based on embedding similarity (e.g., for
retrieval-augmented generation) is vulnerable to poisoning: an adversary can
craft malicious documents that are retrieved in response to broad classes of
queries. We demonstrate that previous, HotFlip-based techniques produce
documents that are very easy to detect using perplexity filtering. Even if
generation is constrained to produce low-perplexity text, the resulting
documents are recognized as unnatural by LLMs and can be automatically filtered
from the retrieval corpus.
  We design, implement, and evaluate a new controlled generation technique that
combines an adversarial objective (embedding similarity) with a ""naturalness""
objective based on soft scores computed using an open-source, surrogate LLM.
The resulting adversarial documents (1) cannot be automatically detected using
perplexity filtering and/or other LLMs, except at the cost of significant false
positives in the retrieval corpus, yet (2) achieve similar poisoning efficacy
to easily-detectable documents generated using HotFlip, and (3) are
significantly more effective than prior methods for energy-guided generation,
such as COLD.",http://arxiv.org/pdf/2410.02163v1
RiskSEA : A Scalable Graph Embedding for Detecting On-chain Fraudulent Activities on the Ethereum Blockchain,"Ayush Agarwal, Lv Lu, Arjun Maheswaran, Varsha Mahadevan, Bhaskar Krishnamachari",2024-10-03,"Like any other useful technology, cryptocurrencies are sometimes used for
criminal activities. While transactions are recorded on the blockchain, there
exists a need for a more rapid and scalable method to detect addresses
associated with fraudulent activities. We present RiskSEA, a scalable risk
scoring system capable of effectively handling the dynamic nature of
large-scale blockchain transaction graphs. The risk scoring system, which we
implement for Ethereum, consists of 1. a scalable approach to generating
node2vec embedding for entire set of addresses to capture the graph topology 2.
transaction-based features to capture the transactional behavioral pattern of
an address 3. a classifier model to generate risk score for addresses that
combines the node2vec embedding and behavioral features. Efficiently generating
node2vec embedding for large scale and dynamically evolving blockchain
transaction graphs is challenging, we present two novel approaches for
generating node2vec embeddings and effectively scaling it to the entire set of
blockchain addresses: 1. node2vec embedding propagation and 2. dynamic node2vec
embedding. We present a comprehensive analysis of the proposed approaches. Our
experiments show that combining both behavioral and node2vec features boosts
the classification performance significantly, and that the dynamic node2vec
embeddings perform better than the node2vec propagated embeddings.",http://arxiv.org/pdf/2410.02160v1
A Watermark for Black-Box Language Models,"Dara Bahri, John Wieting, Dana Alon, Donald Metzler",2024-10-02,"Watermarking has recently emerged as an effective strategy for detecting the
outputs of large language models (LLMs). Most existing schemes require
\emph{white-box} access to the model's next-token probability distribution,
which is typically not accessible to downstream users of an LLM API. In this
work, we propose a principled watermarking scheme that requires only the
ability to sample sequences from the LLM (i.e. \emph{black-box} access), boasts
a \emph{distortion-free} property, and can be chained or nested using multiple
secret keys. We provide performance guarantees, demonstrate how it can be
leveraged when white-box access is available, and show when it can outperform
existing white-box schemes via comprehensive experiments.",http://arxiv.org/pdf/2410.02099v1
DomainHarvester: Harvesting Infrequently Visited Yet Trustworthy Domain Names,"Daiki Chiba, Hiroki Nakano, Takashi Koide",2024-10-02,"In cybersecurity, allow lists play a crucial role in distinguishing safe
websites from potential threats. Conventional methods for compiling allow
lists, focusing heavily on website popularity, often overlook infrequently
visited legitimate domains. This paper introduces DomainHarvester, a system
aimed at generating allow lists that include trustworthy yet infrequently
visited domains. By adopting an innovative bottom-up methodology that leverages
the web's hyperlink structure, DomainHarvester identifies legitimate yet
underrepresented domains. The system uses seed URLs to gather domain names,
employing machine learning with a Transformer-based approach to assess their
trustworthiness. DomainHarvester has developed two distinct allow lists: one
with a global focus and another emphasizing local relevance. Compared to six
existing top lists, DomainHarvester's allow lists show minimal overlaps, 4\%
globally and 0.1\% locally, while significantly reducing the risk of including
malicious domains, thereby enhancing security. The contributions of this
research are substantial, illuminating the overlooked aspect of trustworthy yet
underrepresented domains and introducing DomainHarvester, a system that goes
beyond traditional popularity-based metrics. Our methodology enhances the
inclusivity and precision of allow lists, offering significant advantages to
users and businesses worldwide, especially in non-English speaking regions.",http://arxiv.org/pdf/2410.02097v1
DomainDynamics: Lifecycle-Aware Risk Timeline Construction for Domain Names,"Daiki Chiba, Hiroki Nakano, Takashi Koide",2024-10-02,"The persistent threat posed by malicious domain names in cyber-attacks
underscores the urgent need for effective detection mechanisms. Traditional
machine learning methods, while capable of identifying such domains, often
suffer from high false positive and false negative rates due to their extensive
reliance on historical data. Conventional approaches often overlook the dynamic
nature of domain names, the purposes and ownership of which may evolve,
potentially rendering risk assessments outdated or irrelevant. To address these
shortcomings, we introduce DomainDynamics, a novel system designed to predict
domain name risks by considering their lifecycle stages. DomainDynamics
constructs a timeline for each domain, evaluating the characteristics of each
domain at various points in time to make informed, temporal risk
determinations. In an evaluation experiment involving over 85,000 actual
malicious domains from malware and phishing incidents, DomainDynamics
demonstrated a significant improvement in detection rates, achieving an 82.58\%
detection rate with a low false positive rate of 0.41\%. This performance
surpasses that of previous studies and commercial services, improving detection
capability substantially.",http://arxiv.org/pdf/2410.02096v1
DomainLynx: Leveraging Large Language Models for Enhanced Domain Squatting Detection,"Daiki Chiba, Hiroki Nakano, Takashi Koide",2024-10-02,"Domain squatting poses a significant threat to Internet security, with
attackers employing increasingly sophisticated techniques. This study
introduces DomainLynx, an innovative compound AI system leveraging Large
Language Models (LLMs) for enhanced domain squatting detection. Unlike existing
methods focusing on predefined patterns for top-ranked domains, DomainLynx
excels in identifying novel squatting techniques and protecting less prominent
brands. The system's architecture integrates advanced data processing,
intelligent domain pairing, and LLM-powered threat assessment. Crucially,
DomainLynx incorporates specialized components that mitigate LLM
hallucinations, ensuring reliable and context-aware detection. This approach
enables efficient analysis of vast security data from diverse sources,
including Certificate Transparency logs, Passive DNS records, and zone files.
Evaluated on a curated dataset of 1,649 squatting domains, DomainLynx achieved
94.7\% accuracy using Llama-3-70B. In a month-long real-world test, it detected
34,359 squatting domains from 2.09 million new domains, outperforming baseline
methods by 2.5 times. This research advances Internet security by providing a
versatile, accurate, and adaptable tool for combating evolving domain squatting
threats. DomainLynx's approach paves the way for more robust, AI-driven
cybersecurity solutions, enhancing protection for a broader range of online
entities and contributing to a safer digital ecosystem.",http://arxiv.org/pdf/2410.02095v1
Impact of White-Box Adversarial Attacks on Convolutional Neural Networks,"Rakesh Podder, Sudipto Ghosh",2024-10-02,"Autonomous vehicle navigation and healthcare diagnostics are among the many
fields where the reliability and security of machine learning models for image
data are critical. We conduct a comprehensive investigation into the
susceptibility of Convolutional Neural Networks (CNNs), which are widely used
for image data, to white-box adversarial attacks. We investigate the effects of
various sophisticated attacks -- Fast Gradient Sign Method, Basic Iterative
Method, Jacobian-based Saliency Map Attack, Carlini & Wagner, Projected
Gradient Descent, and DeepFool -- on CNN performance metrics, (e.g., loss,
accuracy), the differential efficacy of adversarial techniques in increasing
error rates, the relationship between perceived image quality metrics (e.g.,
ERGAS, PSNR, SSIM, and SAM) and classification performance, and the comparative
effectiveness of iterative versus single-step attacks. Using the MNIST,
CIFAR-10, CIFAR-100, and Fashio_MNIST datasets, we explore the effect of
different attacks on the CNNs performance metrics by varying the
hyperparameters of CNNs. Our study provides insights into the robustness of
CNNs against adversarial threats, pinpoints vulnerabilities, and underscores
the urgent need for developing robust defense mechanisms to protect CNNs and
ensuring their trustworthy deployment in real-world scenarios.",http://arxiv.org/pdf/2410.02043v1
EAB-FL: Exacerbating Algorithmic Bias through Model Poisoning Attacks in Federated Learning,"Syed Irfan Ali Meerza, Jian Liu",2024-10-02,"Federated Learning (FL) is a technique that allows multiple parties to train
a shared model collaboratively without disclosing their private data. It has
become increasingly popular due to its distinct privacy advantages. However, FL
models can suffer from biases against certain demographic groups (e.g., racial
and gender groups) due to the heterogeneity of data and party selection.
Researchers have proposed various strategies for characterizing the group
fairness of FL algorithms to address this issue. However, the effectiveness of
these strategies in the face of deliberate adversarial attacks has not been
fully explored. Although existing studies have revealed various threats (e.g.,
model poisoning attacks) against FL systems caused by malicious participants,
their primary aim is to decrease model accuracy, while the potential of
leveraging poisonous model updates to exacerbate model unfairness remains
unexplored. In this paper, we propose a new type of model poisoning attack,
EAB-FL, with a focus on exacerbating group unfairness while maintaining a good
level of model utility. Extensive experiments on three datasets demonstrate the
effectiveness and efficiency of our attack, even with state-of-the-art fairness
optimization algorithms and secure aggregation rules employed.",http://arxiv.org/pdf/2410.02042v1
XChainWatcher: Monitoring and Identifying Attacks in Cross-Chain Bridges,"André Augusto, Rafael Belchior, Jonas Pfannschmidt, André Vasconcelos, Miguel Correia",2024-10-02,"Cross-chain bridges are widely used blockchain interoperability mechanisms.
However, several of these bridges have vulnerabilities that have caused 3.2
billion dollars in losses since May 2021. Some studies have revealed the
existence of these vulnerabilities, but little quantitative research is
available, and there are no safeguard mechanisms to protect bridges from such
attacks. We propose XChainWatcher, the first mechanism for monitoring bridges
and detecting attacks against them. XChainWatcher relies on a cross-chain model
powered by a Datalog engine, designed to be pluggable into any cross-chain
bridge. Analyzing data from the Ronin and Nomad bridges, we successfully
identified the transactions that led to losses of \$611M and \$190M USD,
respectively. XChainWatcher not only uncovers successful attacks but also
reveals unintended behavior, such as 37 cross-chain transactions (cctx) that
these bridges should not have accepted, failed attempts to exploit Nomad, over
\$7.8M locked on one chain but never released on Ethereum, and \$200K lost due
to inadequate interaction with bridges. We provide the first open-source
dataset of 81,000 cctxs across three blockchains, capturing \$585M and \$3.7B
in token transfers in Nomad and Ronin, respectively.",http://arxiv.org/pdf/2410.02029v1
Adaptively Private Next-Token Prediction of Large Language Models,"James Flemings, Meisam Razaviyayn, Murali Annavaram",2024-10-02,"As Large Language Models (LLMs) proliferate, developing privacy safeguards
for these models is crucial. One popular safeguard involves training LLMs in a
differentially private manner. However, such solutions are shown to be
computationally expensive and detrimental to the utility of these models. Since
LLMs are deployed on the cloud and thus only accessible via an API, a Machine
Learning as a Service (MLaaS) provider can protect its downstream data by
privatizing the predictions during the decoding process. However, the
practicality of such solutions still largely lags behind DP training methods.
One recent promising approach, Private Mixing of Ensemble Distributions
(PMixED), avoids additive noise by sampling from the output distributions of
private LLMs mixed with the output distribution of a public model. Yet, PMixED
must satisfy a fixed privacy level for a given number of queries, which is
difficult for an analyst to estimate before inference and, hence, does not
scale. To this end, we relax the requirements to a more practical setting by
introducing Adaptive PMixED (AdaPMixED), a private decoding framework based on
PMixED that is adaptive to the private and public output distributions
evaluated on a given input query. In this setting, we introduce a noisy
screening mechanism that filters out queries with potentially expensive privacy
loss, and a data-dependent analysis that exploits the divergence of the private
and public output distributions in its privacy loss calculation. Our
experimental evaluations demonstrate that our mechanism and analysis can reduce
the privacy loss by 16x while preserving the utility over the original PMixED.
Furthermore, performing 100K predictions with AdaPMixED still achieves strong
utility and a reasonable data-dependent privacy loss of 5.25.",http://arxiv.org/pdf/2410.02016v1
Differentially Private Parameter-Efficient Fine-tuning for Large ASR Models,"Hongbin Liu, Lun Wang, Om Thakkar, Abhradeep Thakurta, Arun Narayanan",2024-10-02,"Large ASR models can inadvertently leak sensitive information, which can be
mitigated by formal privacy measures like differential privacy (DP). However,
traditional DP training is computationally expensive, and can hurt model
performance. Our study explores DP parameter-efficient fine-tuning as a way to
mitigate privacy risks with smaller computation and performance costs for ASR
models. Through extensive experimentation and progressive optimization, we
achieve 4.6%/8.1% word error rate on LibriSpeech clean/other test-sets, setting
a new performance benchmark while maintaining (10, 3.52e-6)-DP in fine-tuning a
large ASR model with over 600M parameters.",http://arxiv.org/pdf/2410.01948v1
Social Media Authentication and Combating Deepfakes using Semi-fragile Invisible Image Watermarking,"Aakash Varma Nadimpalli, Ajita Rattani",2024-10-02,"With the significant advances in deep generative models for image and video
synthesis, Deepfakes and manipulated media have raised severe societal
concerns. Conventional machine learning classifiers for deepfake detection
often fail to cope with evolving deepfake generation technology and are
susceptible to adversarial attacks. Alternatively, invisible image watermarking
is being researched as a proactive defense technique that allows media
authentication by verifying an invisible secret message embedded in the image
pixels. A handful of invisible image watermarking techniques introduced for
media authentication have proven vulnerable to basic image processing
operations and watermark removal attacks. In response, we have proposed a
semi-fragile image watermarking technique that embeds an invisible secret
message into real images for media authentication. Our proposed watermarking
framework is designed to be fragile to facial manipulations or tampering while
being robust to benign image-processing operations and watermark removal
attacks. This is facilitated through a unique architecture of our proposed
technique consisting of critic and adversarial networks that enforce high image
quality and resiliency to watermark removal efforts, respectively, along with
the backbone encoder-decoder and the discriminator networks. Thorough
experimental investigations on SOTA facial Deepfake datasets demonstrate that
our proposed model can embed a $64$-bit secret as an imperceptible image
watermark that can be recovered with a high-bit recovery accuracy when benign
image processing operations are applied while being non-recoverable when unseen
Deepfake manipulations are applied. In addition, our proposed watermarking
technique demonstrates high resilience to several white-box and black-box
watermark removal attacks. Thus, obtaining state-of-the-art performance.",http://arxiv.org/pdf/2410.01906v1
The potential of LLM-generated reports in DevSecOps,"Nikolaos Lykousas, Vasileios Argyropoulos, Fran Casino",2024-10-02,"Alert fatigue is a common issue faced by software teams using the DevSecOps
paradigm. The overwhelming number of warnings and alerts generated by security
and code scanning tools, particularly in smaller teams where resources are
limited, leads to desensitization and diminished responsiveness to security
warnings, potentially exposing systems to vulnerabilities. This paper explores
the potential of LLMs in generating actionable security reports that emphasize
the financial impact and consequences of detected security issues, such as
credential leaks, if they remain unaddressed. A survey conducted among
developers indicates that LLM-generated reports significantly enhance the
likelihood of immediate action on security issues by providing clear,
comprehensive, and motivating insights. Integrating these reports into
DevSecOps workflows can mitigate attention saturation and alert fatigue,
ensuring that critical security warnings are addressed effectively.",http://arxiv.org/pdf/2410.01899v1
KeyVisor -- A Lightweight ISA Extension for Protected Key Handles with CPU-enforced Usage Policies,"Fabian Schwarz, Jan Philipp Thoma, Christian Rossow, Tim Güneysu",2024-10-02,"The confidentiality of cryptographic keys is essential for the security of
protection schemes used for communication, file encryption, and outsourced
computation. Beyond cryptanalytic attacks, adversaries can steal keys from
memory via software exploits or side channels, enabling them to, e.g., tamper
with secrets or impersonate key owners. Therefore, existing defenses protect
keys in dedicated devices or isolated memory, or store them only in encrypted
form. However, these designs often provide unfavorable tradeoffs, sacrificing
performance, fine-grained access control, or deployability.
  In this paper, we present KeyVisor, a lightweight ISA extension that securely
offloads the handling of cryptographic keys to the CPU. KeyVisor provides CPU
instructions that enable applications to request protected key handles and
perform AEAD cipher operations on them. The underlying keys are accessible only
by KeyVisor, and thus never leak to memory. KeyVisor's direct CPU integration
enables fast crypto operations and hardware-enforced key usage restrictions,
e.g., keys usable only for de-/encryption, with a limited lifetime, or with a
process binding. Furthermore, privileged software, e.g., the monitor firmware
of TEEs, can revoke keys or bind them to a specific process/TEE. We implement
KeyVisor for RISC-V based on Rocket Chip, evaluate its performance, and
demonstrate real-world use cases, including key-value databases, automotive
feature licensing, and a read-only network middlebox.",http://arxiv.org/pdf/2410.01777v1
LightSC: The Making of a Usable Security Classification Tool for DevSecOps,"Manish Shrestha, Christian Johansen, Johanna Johansen",2024-10-02,"DevSecOps, as the extension of DevOps with security training and tools, has
become a popular way of developing modern software, especially in the Internet
of Things arena, due to its focus on rapid development, with short release
cycles, involving the user/client very closely. Security classification
methods, on the other hand, are heavy and slow processes that require high
expertise in security, the same as in other similar areas such as risk analysis
or certification. As such, security classification methods are hardly
compatible with the DevSecOps culture, which to the contrary, has moved away
from the traditional style of penetration testing done only when the software
product is in the final stages or already deployed.
  In this work, we first propose five principles for a security classification
to be \emph{DevOps-ready}, two of which will be the focus for the rest of the
paper, namely to be tool-based and easy to use for non-security experts, such
as ordinary developers or system architects. We then exemplify how one can make
a security classification methodology DevOps-ready. We do this through an
interaction design process, where we create and evaluate the usability of a
tool implementing the chosen methodology. Since such work seems to be new
within the usable security community, and even more so in the software
development (DevOps) community, we extract from our process a general,
three-steps `recipe' that others can follow when making their own security
methodologies DevOps-ready. The tool that we build is in itself a contribution
of this process, as it can be independently used, extended, and/or integrated
by developer teams into their DevSecOps tool-chains. Our tool is perceived (by
the test subjects) as most useful in the design phase, but also during the
testing phase where the security class would be one of the metrics used to
evaluate the quality of their software.",http://arxiv.org/pdf/2410.01762v1
AssessITS: Integrating procedural guidelines and practical evaluation metrics for organizational IT and Cybersecurity risk assessment,"Mir Mehedi Rahman, Naresh Kshetri, Sayed Abu Sayeed, Md Masud Rana",2024-10-02,"In today's digitally driven landscape, robust Information Technology (IT)
risk assessment practices are essential for safeguarding systems, digital
communication, and data. This paper introduces 'AssessITS', an actionable
method designed to provide organizations with comprehensive guidelines for
conducting IT and cybersecurity risk assessments. Drawing extensively from NIST
800-30 Rev 1, COBIT 5, and ISO 31000, 'AssessITS' bridges the gap between
high-level theoretical standards and practical implementation challenges. The
paper outlines a step-by-step methodology that organizations can simply adopt
to systematically identify, analyze, and mitigate IT risks. By simplifying
complex principles into actionable procedures, this framework equips
practitioners with the tools needed to perform risk assessments independently,
without too much reliance on external vendors. The guidelines are developed to
be straightforward, integrating practical evaluation metrics that allow for the
precise quantification of asset values, threat levels, vulnerabilities, and
impacts on confidentiality, integrity, and availability. This approach ensures
that the risk assessment process is not only comprehensive but also accessible,
enabling decision-makers to implement effective risk mitigation strategies
customized to their unique operational contexts. 'AssessITS' aims to enable
organizations to enhance their IT security strength through practical,
actionable guidance based on internationally recognized standards.",http://arxiv.org/pdf/2410.01750v1
On Using Certified Training towards Empirical Robustness,"Alessandro De Palma, Serge Durand, Zakaria Chihani, François Terrier, Caterina Urban",2024-10-02,"Adversarial training is arguably the most popular way to provide empirical
robustness against specific adversarial examples. While variants based on
multi-step attacks incur significant computational overhead, single-step
variants are vulnerable to a failure mode known as catastrophic overfitting,
which hinders their practical utility for large perturbations. A parallel line
of work, certified training, has focused on producing networks amenable to
formal guarantees of robustness against any possible attack. However, the wide
gap between the best-performing empirical and certified defenses has severely
limited the applicability of the latter. Inspired by recent developments in
certified training, which rely on a combination of adversarial attacks with
network over-approximations, and by the connections between local linearity and
catastrophic overfitting, we present experimental evidence on the practical
utility and limitations of using certified training towards empirical
robustness. We show that, when tuned for the purpose, a recent certified
training algorithm can prevent catastrophic overfitting on single-step attacks,
and that it can bridge the gap to multi-step baselines under appropriate
experimental settings. Finally, we present a novel regularizer for network
over-approximations that can achieve similar effects while markedly reducing
runtime.",http://arxiv.org/pdf/2410.01617v1
Adaptive Exploit Generation against Security Devices and Security APIs,"Robert Künnemann, Julian Biehl",2024-10-02,"Proof-of-concept exploits help demonstrate software vulnerability beyond
doubt and communicate attacks to non-experts. But exploits can be
configuration-specific, for example when in Security APIs, where keys are set
up specifically for the application and enterprise the API serves. In this
work, we show how to automatically derive proof-of-concept exploits against
Security APIs using formal methods.
  We extend the popular protocol verifier ProVerif with a language-agnostic
template mechanism. Employing program snippets attached to steps in the model,
we can transform attack traces (which ProVerif typically finds automatically)
into programs. Our method is general, flexible and convenient. We demonstrate
its use for the W3C Web Cryptography API, for PKCS#11 and for the YubiHSM2,
providing the first formal model of the latter.",http://arxiv.org/pdf/2410.01568v1
Signal Adversarial Examples Generation for Signal Detection Network via White-Box Attack,"Dongyang Li, Linyuan Wang, Guangwei Xiong, Bin Yan, Dekui Ma, Jinxian Peng",2024-10-02,"With the development and application of deep learning in signal detection
tasks, the vulnerability of neural networks to adversarial attacks has also
become a security threat to signal detection networks. This paper defines a
signal adversarial examples generation model for signal detection network from
the perspective of adding perturbations to the signal. The model uses the
inequality relationship of L2-norm between time domain and time-frequency
domain to constrain the energy of signal perturbations. Building upon this
model, we propose a method for generating signal adversarial examples utilizing
gradient-based attacks and Short-Time Fourier Transform. The experimental
results show that under the constraint of signal perturbation energy ratio less
than 3%, our adversarial attack resulted in a 28.1% reduction in the mean
Average Precision (mAP), a 24.7% reduction in recall, and a 30.4% reduction in
precision of the signal detection network. Compared to random noise
perturbation of equivalent intensity, our adversarial attack demonstrates a
significant attack effect.",http://arxiv.org/pdf/2410.01393v1
FlipAttack: Jailbreak LLMs via Flipping,"Yue Liu, Xiaoxin He, Miao Xiong, Jinlan Fu, Shumin Deng, Bryan Hooi",2024-10-02,"This paper proposes a simple yet effective jailbreak attack named FlipAttack
against black-box LLMs. First, from the autoregressive nature, we reveal that
LLMs tend to understand the text from left to right and find that they struggle
to comprehend the text when noise is added to the left side. Motivated by these
insights, we propose to disguise the harmful prompt by constructing left-side
noise merely based on the prompt itself, then generalize this idea to 4
flipping modes. Second, we verify the strong ability of LLMs to perform the
text-flipping task, and then develop 4 variants to guide LLMs to denoise,
understand, and execute harmful behaviors accurately. These designs keep
FlipAttack universal, stealthy, and simple, allowing it to jailbreak black-box
LLMs within only 1 query. Experiments on 8 LLMs demonstrate the superiority of
FlipAttack. Remarkably, it achieves $\sim$98\% attack success rate on GPT-4o,
and $\sim$98\% bypass rate against 5 guardrail models on average. The codes are
available at GitHub\footnote{https://github.com/yueliu1999/FlipAttack}.",http://arxiv.org/pdf/2410.02832v1
The Unlikely Hero: Nonideality in Analog Photonic Neural Networks as Built-in Defender Against Adversarial Attacks,"Haotian Lu, Ziang Yin, Partho Bhoumik, Sanmitra Banerjee, Krishnendu Chakrabarty, Jiaqi Gu",2024-10-02,"Electronic-photonic computing systems have emerged as a promising platform
for accelerating deep neural network (DNN) workloads. Major efforts have been
focused on countering hardware non-idealities and boosting efficiency with
various hardware/algorithm co-design methods. However, the adversarial
robustness of such photonic analog mixed-signal AI hardware remains unexplored.
Though the hardware variations can be mitigated with robustness-driven
optimization methods, malicious attacks on the hardware show distinct behaviors
from noises, which requires a customized protection method tailored to optical
analog hardware. In this work, we rethink the role of conventionally undesired
non-idealities in photonic analog accelerators and claim their surprising
effects on defending against adversarial weight attacks. Inspired by the
protection effects from DNN quantization and pruning, we propose a synergistic
defense framework tailored for optical analog hardware that proactively
protects sensitive weights via pre-attack unary weight encoding and post-attack
vulnerability-aware weight locking. Efficiency-reliability trade-offs are
formulated as constrained optimization problems and efficiently solved offline
without model re-training costs. Extensive evaluation of various DNN benchmarks
with a multi-core photonic accelerator shows that our framework maintains
near-ideal on-chip inference accuracy under adversarial bit-flip attacks with
merely <3% memory overhead. Our codes are open-sourced at
https://github.com/ScopeX-ASU/Unlikely_Hero.",http://arxiv.org/pdf/2410.01289v1
"""No Matter What You Do!"": Mitigating Backdoor Attacks in Graph Neural Networks","Jiale Zhang, Chengcheng Zhu, Bosen Rao, Hao Sui, Xiaobing Sun, Bing Chen, Chunyi Zhou, Shouling Ji",2024-10-02,"Recent studies have exposed that GNNs are vulnerable to several adversarial
attacks, among which backdoor attack is one of the toughest. Similar to Deep
Neural Networks (DNNs), backdoor attacks in GNNs lie in the fact that the
attacker modifies a portion of graph data by embedding triggers and enforces
the model to learn the trigger feature during the model training process.
Despite the massive prior backdoor defense works on DNNs, defending against
backdoor attacks in GNNs is largely unexplored, severely hindering the
widespread application of GNNs in real-world tasks. To bridge this gap, we
present GCleaner, the first backdoor mitigation method on GNNs. GCleaner can
mitigate the presence of the backdoor logic within backdoored GNNs by reversing
the backdoor learning procedure, aiming to restore the model performance to a
level similar to that is directly trained on the original clean dataset. To
achieve this objective, we ask: How to recover universal and hard backdoor
triggers in GNNs? How to unlearn the backdoor trigger feature while maintaining
the model performance? We conduct the graph trigger recovery via the
explanation method to identify optimal trigger locations, facilitating the
search of universal and hard backdoor triggers in the feature space of the
backdoored model through maximal similarity. Subsequently, we introduce the
backdoor unlearning mechanism, which combines knowledge distillation and
gradient-based explainable knowledge for fine-grained backdoor erasure.
Extensive experimental evaluations on four benchmark datasets demonstrate that
GCleaner can reduce the backdoor attack success rate to 10% with only 1% of
clean data, and has almost negligible degradation in model performance, which
far outperforms the state-of-the-art (SOTA) defense methods.",http://arxiv.org/pdf/2410.01272v1
Count of Monte Crypto: Accounting-based Defenses for Cross-Chain Bridges,"Enze Liu, Elisa Luo, Jian Chen Yan, Katherine Izhikevich, Stewart Grant, Deian Stefan, Geoffrey M Voelker, Stefan Savage",2024-10-01,"Between 2021 and 2023, crypto assets valued at over \$US2.6 billion were
stolen via attacks on ""bridges"" -- decentralized services designed to allow
inter-blockchain exchange. While the individual exploits in each attack vary, a
single design flaw underlies them all: the lack of end-to-end value accounting
in cross-chain transactions. In this paper, we empirically analyze twenty
million transactions used by key bridges during this period. We show that a
simple invariant that balances cross-chain inflows and outflows is compatible
with legitimate use, yet precisely identifies every known attack (and several
likely attacks) in this data. Further, we show that this approach is not only
sufficient for post-hoc audits, but can be implemented in-line in existing
bridge designs to provide generic protection against a broad array of bridge
vulnerabilities.",http://arxiv.org/pdf/2410.01107v1
Convergent Privacy Loss of Noisy-SGD without Convexity and Smoothness,"Eli Chien, Pan Li",2024-10-01,"We study the Differential Privacy (DP) guarantee of hidden-state Noisy-SGD
algorithms over a bounded domain. Standard privacy analysis for Noisy-SGD
assumes all internal states are revealed, which leads to a divergent R'enyi DP
bound with respect to the number of iterations. Ye & Shokri (2022) and
Altschuler & Talwar (2022) proved convergent bounds for smooth (strongly)
convex losses, and raise open questions about whether these assumptions can be
relaxed. We provide positive answers by proving convergent R'enyi DP bound for
non-convex non-smooth losses, where we show that requiring losses to have
H\""older continuous gradient is sufficient. We also provide a strictly better
privacy bound compared to state-of-the-art results for smooth strongly convex
losses. Our analysis relies on the improvement of shifted divergence analysis
in multiple aspects, including forward Wasserstein distance tracking,
identifying the optimal shifts allocation, and the H""older reduction lemma. Our
results further elucidate the benefit of hidden-state analysis for DP and its
applicability.",http://arxiv.org/pdf/2410.01068v1
A Generalized Approach to Root-based Attacks against PLWE,"Iván Blanco Chacón, Raúl Durán Díaz, Rodrigo Martín Sánchez-Ledesma",2024-10-01,"The Polynomial Learning With Errors problem (PLWE) serves as the background
of two of the three cryptosystems standardized in August 2024 by the National
Institute of Standards and Technology to replace non-quantum resistant current
primitives like those based on RSA, Diffie-Hellman or its elliptic curve
analogue. Although PLWE is highly believed to be quantum resistant, this fact
has not yet been established, contrariwise to other post-quantum proposals like
multivariate and some code based ones. Moreover, several vulnerabilities have
been encountered for a number of specific instances. In a search for more
flexibility, it becomes fully relevant to study the robustness of PLWE based on
other polynomials, not necessarily cyclotomic. In 2015, Elias et al found a
good number of attacks based on different features of the roots of the
polynomial. In the present work we present an overview of the approximations
made against PLWE derived from this and subsequent works, along with several
new attacks which refine those by Elias et al. exploiting the order of the
trace of roots over finite extensions of the finite field under the three
scenarios laid out by Elias et al., allowing to generalize the setting in which
the attacks can be carried out.",http://arxiv.org/pdf/2410.01017v1
Machine Learning-Assisted Intrusion Detection for Enhancing Internet of Things Security,"Mona Esmaeili, Morteza Rahimi, Matin Khajavi, Dorsa Farahmand, Hadi Jabbari Saray",2024-10-01,"Attacks against the Internet of Things (IoT) are rising as devices,
applications, and interactions become more networked and integrated. The
increase in cyber-attacks that target IoT networks poses a huge vulnerability
and threat to the privacy, security, functionality, and availability of
critical systems, which leads to operational disruptions, financial losses,
identity thefts, and data breaches. To efficiently secure IoT devices,
real-time detection of intrusion systems is critical, especially those using
machine learning to identify threats and mitigate risks and vulnerabilities.
This paper investigates the latest research on machine learning-based intrusion
detection strategies for IoT security, concentrating on real-time
responsiveness, detection accuracy, and algorithm efficiency. Key studies were
reviewed from all well-known academic databases, and a taxonomy was provided
for the existing approaches. This review also highlights existing research gaps
and outlines the limitations of current IoT security frameworks to offer
practical insights for future research directions and developments.",http://arxiv.org/pdf/2410.01016v1
Empirical Perturbation Analysis of Linear System Solvers from a Data Poisoning Perspective,"Yixin Liu, Arielle Carr, Lichao Sun",2024-10-01,"The perturbation analysis of linear solvers applied to systems arising
broadly in machine learning settings -- for instance, when using linear
regression models -- establishes an important perspective when reframing these
analyses through the lens of a data poisoning attack. By analyzing solvers'
responses to such attacks, this work aims to contribute to the development of
more robust linear solvers and provide insights into poisoning attacks on
linear solvers. In particular, we investigate how the errors in the input data
will affect the fitting error and accuracy of the solution from a linear
system-solving algorithm under perturbations common in adversarial attacks. We
propose data perturbation through two distinct knowledge levels, developing a
poisoning optimization and studying two methods of perturbation: Label-guided
Perturbation (LP) and Unconditioning Perturbation (UP). Existing works mainly
focus on deriving the worst-case perturbation bound from a theoretical
perspective, and the analysis is often limited to specific kinds of linear
system solvers. Under the circumstance that the data is intentionally perturbed
-- as is the case with data poisoning -- we seek to understand how different
kinds of solvers react to these perturbations, identifying those algorithms
most impacted by different types of adversarial attacks.",http://arxiv.org/pdf/2410.00878v1
PyRIT: A Framework for Security Risk Identification and Red Teaming in Generative AI System,"Gary D. Lopez Munoz, Amanda J. Minnich, Roman Lutz, Richard Lundeen, Raja Sekhar Rao Dheekonda, Nina Chikanov, Bolor-Erdene Jagdagdorj, Martin Pouliot, Shiven Chawla, Whitney Maxwell, Blake Bullwinkel, Katherine Pratt, Joris de Gruyter, Charlotte Siska, Pete Bryan, Tori Westerhoff, Chang Kawaguchi, Christian Seifert, Ram Shankar Siva Kumar, Yonatan Zunger",2024-10-01,"Generative Artificial Intelligence (GenAI) is becoming ubiquitous in our
daily lives. The increase in computational power and data availability has led
to a proliferation of both single- and multi-modal models. As the GenAI
ecosystem matures, the need for extensible and model-agnostic risk
identification frameworks is growing. To meet this need, we introduce the
Python Risk Identification Toolkit (PyRIT), an open-source framework designed
to enhance red teaming efforts in GenAI systems. PyRIT is a model- and
platform-agnostic tool that enables red teamers to probe for and identify novel
harms, risks, and jailbreaks in multimodal generative AI models. Its composable
architecture facilitates the reuse of core building blocks and allows for
extensibility to future models and modalities. This paper details the
challenges specific to red teaming generative AI systems, the development and
features of PyRIT, and its practical applications in real-world scenarios.",http://arxiv.org/pdf/2410.02828v1
Timber! Poisoning Decision Trees,"Stefano Calzavara, Lorenzo Cazzaro, Massimo Vettori",2024-10-01,"We present Timber, the first white-box poisoning attack targeting decision
trees. Timber is based on a greedy attack strategy leveraging sub-tree
retraining to efficiently estimate the damage performed by poisoning a given
training instance. The attack relies on a tree annotation procedure which
enables sorting training instances so that they are processed in increasing
order of computational cost of sub-tree retraining. This sorting yields a
variant of Timber supporting an early stopping criterion designed to make
poisoning attacks more efficient and feasible on larger datasets. We also
discuss an extension of Timber to traditional random forest models, which is
useful because decision trees are normally combined into ensembles to improve
their predictive power. Our experimental evaluation on public datasets shows
that our attacks outperform existing baselines in terms of effectiveness,
efficiency or both. Moreover, we show that two representative defenses can
mitigate the effect of our attacks, but fail at effectively thwarting them.",http://arxiv.org/pdf/2410.00862v1
Enhancing Web Spam Detection through a Blockchain-Enabled Crowdsourcing Mechanism,"Noah Kader, Inwon Kang, Oshani Seneviratne",2024-10-01,"The proliferation of spam on the Web has necessitated the development of
machine learning models to automate their detection. However, the dynamic
nature of spam and the sophisticated evasion techniques employed by spammers
often lead to low accuracy in these models. Traditional machine-learning
approaches struggle to keep pace with spammers' constantly evolving tactics,
resulting in a persistent challenge to maintain high detection rates. To
address this, we propose blockchain-enabled incentivized crowdsourcing as a
novel solution to enhance spam detection systems. We create an incentive
mechanism for data collection and labeling by leveraging blockchain's
decentralized and transparent framework. Contributors are rewarded for accurate
labels and penalized for inaccuracies, ensuring high-quality data. A smart
contract governs the submission and evaluation process, with participants
staking cryptocurrency as collateral to guarantee integrity. Simulations show
that incentivized crowdsourcing improves data quality, leading to more
effective machine-learning models for spam detection. This approach offers a
scalable and adaptable solution to the challenges of traditional methods.",http://arxiv.org/pdf/2410.00860v1
Fast Multiplication and the PLWE-RLWE Equivalence for an Infinite Family of Cyclotomic Subextensions,"Joonas Ahola, Iván Blanco-Chacón, Wilmar Bolaños, Antti Haavikko, Camilla Hollanti, Rodrigo Martín Sánchez-Ledesma",2024-10-01,"We prove the equivalence between the Ring Learning With Errors (RLWE) and the
Polynomial Learning With Errors (PLWE) problems for the maximal totally real
subfield of the $2^r 3^s$-th cyclotomic field for $r \geq 3$ and $s \geq 1$.
Moreover, we describe a fast algorithm for computing the product of two
elements in the ring of integers of these subfields. This multiplication
algorithm has quasilinear complexity in the dimension of the field, as it makes
use of the fast Discrete Cosine Transform (DCT). Our approach assumes that the
two input polynomials are given in a basis of Chebyshev-like polynomials, in
contrast to the customary power basis. To validate this assumption, we prove
that the change of basis from the power basis to the Chebyshev-like basis can
be computed with $\mathcal{O}(n \log n)$ arithmetic operations, where $n$ is
the problem dimension. Finally, we provide a heuristic and theoretical
comparison of the vulnerability to some attacks for the $p$-th cyclotomic field
versus the maximal totally real subextension of the $4p$-th cyclotomic field
for a reasonable set of parameters of cryptographic size.",http://arxiv.org/pdf/2410.00792v1
User-Guided Verification of Security Protocols via Sound Animation,"Kangfeng Ye, Roberto Metere, Poonam Yadav",2024-10-01,"Current formal verification of security protocols relies on specialized
researchers and complex tools, inaccessible to protocol designers who
informally evaluate their work with emulators. This paper addresses this gap by
embedding symbolic analysis into the design process. Our approach implements
the Dolev-Yao attack model using a variant of CSP based on Interaction Trees
(ITrees) to compile protocols into animators -- executable programs that
designers can use for debugging and inspection. To guarantee the soundness of
our compilation, we mechanised our approach in the theorem prover Isabelle/HOL.
As traditionally done with symbolic tools, we refer to the Diffie-Hellman key
exchange and the Needham-Schroeder public-key protocol (and Lowe's patched
variant). We demonstrate how our animator can easily reveal the mechanics of
attacks and verify corrections. This work facilitates security integration at
the design level and supports further security property analysis and
software-engineered integrations.",http://arxiv.org/pdf/2410.00676v1
Integrating PETs into Software Applications: A Game-Based Learning Approach,"Maisha Boteju, Thilina Ranbaduge, Dinusha Vatsalan, Nalin Arachchilage",2024-10-01,"The absence of data protection measures in software applications leads to
data breaches, threatening end-user privacy and causing instabilities in
organisations that developed those software. Privacy Enhancing Technologies
(PETs) emerge as promising safeguards against data breaches. PETs minimise
threats to personal data while enabling software to extract valuable insights
from them. However, software developers often lack the adequate knowledge and
awareness to develop PETs integrated software. This issue is exacerbated by
insufficient PETs related learning approaches customised for software
developers. Therefore, we propose ""PETs-101"", a novel game-based learning
framework that motivates developers to integrate PETs into software. By doing
so, it aims to improve developers' privacy-preserving software development
behaviour rather than simply delivering the learning content on PETs. In
future, the proposed framework will be empirically investigated and used as a
foundation for developing an educational gaming intervention that trains
developers to put PETs into practice.",http://arxiv.org/pdf/2410.00661v1
Differentially Private Active Learning: Balancing Effective Data Selection and Privacy,"Kristian Schwethelm, Johannes Kaiser, Jonas Kuntzer, Mehmet Yigitsoy, Daniel Rueckert, Georgios Kaissis",2024-10-01,"Active learning (AL) is a widely used technique for optimizing data labeling
in machine learning by iteratively selecting, labeling, and training on the
most informative data. However, its integration with formal privacy-preserving
methods, particularly differential privacy (DP), remains largely underexplored.
While some works have explored differentially private AL for specialized
scenarios like online learning, the fundamental challenge of combining AL with
DP in standard learning settings has remained unaddressed, severely limiting
AL's applicability in privacy-sensitive domains. This work addresses this gap
by introducing differentially private active learning (DP-AL) for standard
learning settings. We demonstrate that naively integrating DP-SGD training into
AL presents substantial challenges in privacy budget allocation and data
utilization. To overcome these challenges, we propose step amplification, which
leverages individual sampling probabilities in batch creation to maximize data
point participation in training steps, thus optimizing data utilization.
Additionally, we investigate the effectiveness of various acquisition functions
for data selection under privacy constraints, revealing that many commonly used
functions become impractical. Our experiments on vision and natural language
processing tasks show that DP-AL can improve performance for specific datasets
and model architectures. However, our findings also highlight the limitations
of AL in privacy-constrained environments, emphasizing the trade-offs between
privacy, model accuracy, and data selection accuracy.",http://arxiv.org/pdf/2410.00542v1
A Scheduling-Aware Defense Against Prefetching-Based Side-Channel Attacks,"Till Schlüter, Nils Ole Tippenhauer",2024-10-01,"Modern computer processors use microarchitectural optimization mechanisms to
improve performance. As a downside, such optimizations are prone to introducing
side-channel vulnerabilities. Speculative loading of memory, called
prefetching, is common in real-world CPUs and may cause such side-channel
vulnerabilities: Prior work has shown that it can be exploited to bypass
process isolation and leak secrets, such as keys used in RSA, AES, and ECDH
implementations. However, to this date, no effective and efficient
countermeasure has been presented that secures software on systems with
affected prefetchers.
  In this work, we answer the question: How can a process defend against
prefetch-based side channels? We first systematize prefetching-based
side-channel vulnerabilities presented in academic literature so far. Next, we
design and implement PreFence, a scheduling-aware defense against these side
channels that allows processes to disable the prefetcher temporarily during
security-critical operations. We implement our countermeasure for an x86_64 and
an ARM processor; it can be adapted to any platform that allows to disable the
prefetcher. We evaluate our defense and find that our solution reliably stops
prefetch leakage. Our countermeasure causes negligible performance impact while
no security-relevant code is executed, and its worst case performance is
comparable to completely turning off the prefetcher. The expected average
performance impact depends on the security-relevant code in the application and
can be negligible as we demonstrate with a simple web server application.
  We expect our countermeasure could widely be integrated in commodity OS, and
even be extended to signal generally security-relevant code to the kernel to
allow coordinated application of countermeasures.",http://arxiv.org/pdf/2410.00452v1
Adversarial Suffixes May Be Features Too!,"Wei Zhao, Zhe Li, Yige Li, Jun Sun",2024-10-01,"Despite significant ongoing efforts in safety alignment, large language
models (LLMs) such as GPT-4 and LLaMA 3 remain vulnerable to jailbreak attacks
that can induce harmful behaviors, including those triggered by adversarial
suffixes. Building on prior research, we hypothesize that these adversarial
suffixes are not mere bugs but may represent features that can dominate the
LLM's behavior. To evaluate this hypothesis, we conduct several experiments.
First, we demonstrate that benign features can be effectively made to function
as adversarial suffixes, i.e., we develop a feature extraction method to
extract sample-agnostic features from benign dataset in the form of suffixes
and show that these suffixes may effectively compromise safety alignment.
Second, we show that adversarial suffixes generated from jailbreak attacks may
contain meaningful features, i.e., appending the same suffix to different
prompts results in responses exhibiting specific characteristics. Third, we
show that such benign-yet-safety-compromising features can be easily introduced
through fine-tuning using only benign datasets, i.e., even in the absence of
harmful content. This highlights the critical risk posed by dominating benign
features in the training data and calls for further research to reinforce LLM
safety alignment. Our code and data is available at
\url{https://github.com/anonymous}.",http://arxiv.org/pdf/2410.00451v1
PrivTuner with Homomorphic Encryption and LoRA: A P3EFT Scheme for Privacy-Preserving Parameter-Efficient Fine-Tuning of AI Foundation Models,"Yang Li, Wenhan Yu, Jun Zhao",2024-10-01,"AI foundation models have recently demonstrated impressive capabilities
across a wide range of tasks. Fine-tuning (FT) is a method of customizing a
pre-trained AI foundation model by further training it on a smaller, targeted
dataset. In this paper, we initiate the study of the Privacy-Preserving
Parameter-Efficient FT (P3EFT) framework, which can be viewed as the
intersection of Parameter-Efficient FT (PEFT) and Privacy-Preserving FT (PPFT).
PEFT modifies only a small subset of the model's parameters to achieve FT
(i.e., adapting a pre-trained model to a specific dataset), while PPFT uses
privacy-preserving technologies to protect the confidentiality of the model
during the FT process. There have been many studies on PEFT or PPFT but very
few on their fusion, which motivates our work on P3EFT to achieve both
parameter efficiency and model privacy. To exemplify our P3EFT, we present the
PrivTuner scheme, which incorporates Fully Homomorphic Encryption (FHE) enabled
privacy protection into LoRA (short for ``Low-Rank Adapter''). Intuitively
speaking, PrivTuner allows the model owner and the external data owners to
collaboratively implement PEFT with encrypted data. After describing PrivTuner
in detail, we further investigate its energy consumption and privacy
protection. Then, we consider a PrivTuner system over wireless communications
and formulate a joint optimization problem to adaptively minimize energy while
maximizing privacy protection, with the optimization variables including FDMA
bandwidth allocation, wireless transmission power, computational resource
allocation, and privacy protection. A resource allocation algorithm is devised
to solve the problem. Experiments demonstrate that our algorithm can
significantly reduce energy consumption while adapting to different privacy
requirements.",http://arxiv.org/pdf/2410.00433v1
LinkThief: Combining Generalized Structure Knowledge with Node Similarity for Link Stealing Attack against GNN,"Yuxing Zhang, Siyuan Meng, Chunchun Chen, Mengyao Peng, Hongyan Gu, Xinli Huang",2024-10-01,"Graph neural networks(GNNs) have a wide range of applications in
multimedia.Recent studies have shown that Graph neural networks(GNNs) are
vulnerable to link stealing attacks,which infers the existence of edges in the
target GNN's training graph.Existing attacks are usually based on the
assumption that links exist between two nodes that share similar
posteriors;however,they fail to focus on links that do not hold under this
assumption.To this end,we propose LinkThief,an improved link stealing attack
that combines generalized structure knowledge with node similarity,in a
scenario where the attackers' background knowledge contains partially leaked
target graph and shadow graph.Specifically,to equip the attack model with
insights into the link structure spanning both the shadow graph and the target
graph,we introduce the idea of creating a Shadow-Target Bridge Graph and
extracting edge subgraph structure features from it.Through theoretical
analysis from the perspective of privacy theft,we first explore how to
implement the aforementioned ideas.Building upon the findings,we design the
Bridge Graph Generator to construct the Shadow-Target Bridge Graph.Then,the
subgraph around the link is sampled by the Edge Subgraph Preparation
Module.Finally,the Edge Structure Feature Extractor is designed to obtain
generalized structure knowledge,which is combined with node similarity to form
the features provided to the attack model.Extensive experiments validate the
correctness of theoretical analysis and demonstrate that LinkThief still
effectively steals links without extra assumptions.",http://arxiv.org/pdf/2410.02826v1
VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data,"Xuefeng Du, Reshmi Ghosh, Robert Sim, Ahmed Salem, Vitor Carvalho, Emily Lawton, Yixuan Li, Jack W. Stokes",2024-10-01,"Vision-language models (VLMs) are essential for contextual understanding of
both visual and textual information. However, their vulnerability to
adversarially manipulated inputs presents significant risks, leading to
compromised outputs and raising concerns about the reliability in
VLM-integrated applications. Detecting these malicious prompts is thus crucial
for maintaining trust in VLM generations. A major challenge in developing a
safeguarding prompt classifier is the lack of a large amount of labeled benign
and malicious data. To address the issue, we introduce VLMGuard, a novel
learning framework that leverages the unlabeled user prompts in the wild for
malicious prompt detection. These unlabeled prompts, which naturally arise when
VLMs are deployed in the open world, consist of both benign and malicious
information. To harness the unlabeled data, we present an automated
maliciousness estimation score for distinguishing between benign and malicious
samples within this unlabeled mixture, thereby enabling the training of a
binary prompt classifier on top. Notably, our framework does not require extra
human annotations, offering strong flexibility and practicality for real-world
applications. Extensive experiment shows VLMGuard achieves superior detection
results, significantly outperforming state-of-the-art methods. Disclaimer: This
paper may contain offensive examples; reader discretion is advised.",http://arxiv.org/pdf/2410.00296v1
Towards Precise Detection of Personal Information Leaks in Mobile Health Apps,"Alireza Ardalani, Joseph Antonucci, Iulian Neamtiu",2024-09-30,"Mobile apps are used in a variety of health settings, from apps that help
providers, to apps designed for patients, to health and fitness apps designed
for the general public. These apps ask the user for, and then collect and leak
a wealth of Personal Information (PI). We analyze the PI that apps collect via
their user interface, whether the app or third-party code is processing this
information, and finally where the data is sent or stored. Prior work on leak
detection in Android has focused on detecting leaks of (hardware)
device-identifying information, or policy violations; however no work has
looked at processing and leaking of PI in the context of health apps. The first
challenge we tackle is extracting the semantic information contained in app UIs
to discern the extent, and nature, of personal information. The second
challenge we tackle is disambiguating between first-party, legitimate leaks
(e.g,. the app storing data in its database) and third-party, problematic
leaks, e.g., processing this information by, or sending it to, advertisers and
analytics. We conducted a study on 1,243 Android apps: 623 medical apps and 621
health&fitness apps. We categorize PI into 16 types, grouped in 3 main
categories: identity, medical, anthropometric. We found that the typical app
has one first-party leak and five third-party leaks, though 221 apps had 20 or
more leaks. Next, we show that third-party leaks (e.g., advertisers, analytics)
are 5x more frequent than first-party leaks. Then, we show that 71% of leaks
are to local storage (i.e., the phone, where data could be accessed by
unauthorized apps) whereas 29% of leaks are to the network (e.g., Cloud).
Finally, medical apps have 20% more PI leaks than health&fitness apps, due to
collecting additional medical PI.",http://arxiv.org/pdf/2410.00277v1
Enhancing Pre-Trained Language Models for Vulnerability Detection via Semantic-Preserving Data Augmentation,"Weiliang Qi, Jiahao Cao, Darsh Poddar, Sophia Li, Xinda Wang",2024-09-30,"With the rapid development and widespread use of advanced network systems,
software vulnerabilities pose a significant threat to secure communications and
networking. Learning-based vulnerability detection systems, particularly those
leveraging pre-trained language models, have demonstrated significant potential
in promptly identifying vulnerabilities in communication networks and reducing
the risk of exploitation. However, the shortage of accurately labeled
vulnerability datasets hinders further progress in this field. Failing to
represent real-world vulnerability data variety and preserve vulnerability
semantics, existing augmentation approaches provide limited or even
counterproductive contributions to model training. In this paper, we propose a
data augmentation technique aimed at enhancing the performance of pre-trained
language models for vulnerability detection. Given the vulnerability dataset,
our method performs natural semantic-preserving program transformation to
generate a large volume of new samples with enriched data diversity and
variety. By incorporating our augmented dataset in fine-tuning a series of
representative code pre-trained models (i.e., CodeBERT, GraphCodeBERT,
UnixCoder, and PDBERT), up to 10.1% increase in accuracy and 23.6% increase in
F1 can be achieved in the vulnerability detection task. Comparison results also
show that our proposed method can substantially outperform other prominent
vulnerability augmentation approaches.",http://arxiv.org/pdf/2410.00249v2
Ingest-And-Ground: Dispelling Hallucinations from Continually-Pretrained LLMs with RAG,"Chenhao Fang, Derek Larson, Shitong Zhu, Sophie Zeng, Wendy Summer, Yanqing Peng, Yuriy Hulovatyy, Rajeev Rao, Gabriel Forgues, Arya Pudota, Alex Goncalves, Hervé Robert",2024-09-30,"This paper presents new methods that have the potential to improve privacy
process efficiency with LLM and RAG. To reduce hallucination, we continually
pre-train the base LLM model with a privacy-specific knowledge base and then
augment it with a semantic RAG layer. Our evaluations demonstrate that this
approach enhances the model performance (as much as doubled metrics compared to
out-of-box LLM) in handling privacy-related queries, by grounding responses
with factual information which reduces inaccuracies.",http://arxiv.org/pdf/2410.02825v1
Propelling Innovation to Defeat Data-Leakage Hardware Trojans: From Theory to Practice,"Kevin Kwiat, Jason Kulick, Paul Ratazzi",2024-09-30,"Many design companies have gone fabless and rely on external fabrication
facilities to produce chips due to increasing cost of semiconductor
manufacturing. However, not all of these facilities can be considered
trustworthy; some may inject hardware Trojans and jeopardize the security of
the system. One common objective of hardware Trojans is to establish a side
channel for data leakage. While extensive literature exists on various
defensive measures, almost all of them focus on preventing the establishment of
side channels, and can be compromised if attackers gain access to the physical
chip and can perform reverse engineering between multiple fabrication runs. In
this paper, we advance (from theory to practice) RECORD: Randomized Encoding of
COmbinational Logic for Resistance to Data Leakage. RECORD is a novel scheme of
temporarily randomized encoding for combinational logic that, with the aid of
Quilt Packaging, prevents attackers from interpreting the data.",http://arxiv.org/pdf/2409.20486v1
Fine-Tuning Personalization in Federated Learning to Mitigate Adversarial Clients,"Youssef Allouah, Abdellah El Mrini, Rachid Guerraoui, Nirupam Gupta, Rafael Pinot",2024-09-30,"Federated learning (FL) is an appealing paradigm that allows a group of
machines (a.k.a. clients) to learn collectively while keeping their data local.
However, due to the heterogeneity between the clients' data distributions, the
model obtained through the use of FL algorithms may perform poorly on some
client's data. Personalization addresses this issue by enabling each client to
have a different model tailored to their own data while simultaneously
benefiting from the other clients' data. We consider an FL setting where some
clients can be adversarial, and we derive conditions under which full
collaboration fails. Specifically, we analyze the generalization performance of
an interpolated personalized FL framework in the presence of adversarial
clients, and we precisely characterize situations when full collaboration
performs strictly worse than fine-tuned personalization. Our analysis
determines how much we should scale down the level of collaboration, according
to data heterogeneity and the tolerable fraction of adversarial clients. We
support our findings with empirical results on mean estimation and binary
classification problems, considering synthetic and benchmark image
classification datasets.",http://arxiv.org/pdf/2409.20329v1
MNT Elliptic Curves with Non-Prime Order,Maciej Grześkowiak,2024-09-30,"Miyaji, Nakabayashi, and Takano proposed the algorithm for the construction
of prime order pairing-friendly elliptic curves with embedding degrees
$k=3,4,6$. We present a method for generating generalized MNT curves. The order
of such pairing-friendly curves is the product of two prime numbers.",http://arxiv.org/pdf/2409.20254v1
Quantum Fast Implementation of Private Information Retrieval and Functional Bootstrapping,"Guangsheng Ma, Hongbo Li",2024-09-30,"Quantum computation has found greater efficiency and security across various
fields. We show that, in a near-term hybrid cloud computing scenario with only
one single quantum server and an entirely classical client, critical
bottlenecks in privacy-preserving computation can be addressed.
  First, we propose an efficient quantum functional bootstrapping algorithm
with a runtime polynomial in the plaintext-size, providing an exponential
quantum speedup over classical algorithms. Second, we present a secure and fast
quantum private information retrieval protocol with logarithmic query time. The
security relies on the learning with errors (LWE) problem with polynomial
modulus, greatly improving the security of classical fast PIR protocol based on
ring-LWE with super-polynomial modulus.
  Technically, we extend an important classical homomorphic operation, known as
blind rotation, to the quantum case by an encrypted conditional rotation
technique. This technique holds promise for broader applications in quantum
cryptography.",http://arxiv.org/pdf/2409.20182v1
Professor X: Manipulating EEG BCI with Invisible and Robust Backdoor Attack,"Xuan-Hao Liu, Xinhao Song, Dexuan He, Bao-Liang Lu, Wei-Long Zheng",2024-09-30,"While electroencephalogram (EEG) based brain-computer interface (BCI) has
been widely used for medical diagnosis, health care, and device control, the
safety of EEG BCI has long been neglected. In this paper, we propose Professor
X, an invisible and robust ""mind-controller"" that can arbitrarily manipulate
the outputs of EEG BCI through backdoor attack, to alert the EEG community of
the potential hazard. However, existing EEG attacks mainly focus on
single-target class attacks, and they either require engaging the training
stage of the target BCI, or fail to maintain high stealthiness. Addressing
these limitations, Professor X exploits a three-stage clean label poisoning
attack: 1) selecting one trigger for each class; 2) learning optimal injecting
EEG electrodes and frequencies strategy with reinforcement learning for each
trigger; 3) generating poisoned samples by injecting the corresponding
trigger's frequencies into poisoned data for each class by linearly
interpolating the spectral amplitude of both data according to previously
learned strategies. Experiments on datasets of three common EEG tasks
demonstrate the effectiveness and robustness of Professor X, which also easily
bypasses existing backdoor defenses.",http://arxiv.org/pdf/2409.20158v1
"An interdisciplinary exploration of trade-offs between energy, privacy and accuracy aspects of data","Pepijn de Reus, Kyra Dresen, Ana Oprescu, Kristina Irion, Ans Kolk",2024-09-30,"The digital era has raised many societal challenges, including ICT's rising
energy consumption and protecting privacy of personal data processing. This
paper considers both aspects in relation to machine learning accuracy in an
interdisciplinary exploration. We first present a method to measure the effects
of privacy-enhancing techniques on data utility and energy consumption. The
environmental-privacy-accuracy trade-offs are discovered through an
experimental set-up. We subsequently take a storytelling approach to translate
these technical findings to experts in non-ICT fields. We draft two examples
for a governmental and auditing setting to contextualise our results.
Ultimately, users face the task of optimising their data processing operations
in a trade-off between energy, privacy, and accuracy considerations where the
impact of their decisions is context-sensitive.",http://arxiv.org/pdf/2410.00069v1
DBNode: A Decentralized Storage System for Big Data Storage in Consortium Blockchains,"Narges Dadkhah, Xuyang Ma, Katinka Wolter, Gerhard Wunder",2024-09-30,"Storing big data directly on a blockchain poses a substantial burden due to
the need to maintain a consistent ledger across all nodes. Numerous studies in
decentralized storage systems have been conducted to tackle this particular
challenge. Most state-of-the-art research concentrates on developing a general
storage system that can accommodate diverse blockchain categories. However, it
is essential to recognize the unique attributes of a consortium blockchain,
such as data privacy and access control. Beyond ensuring high performance,
these specific needs are often overlooked by general storage systems. This
paper proposes a decentralized storage system for Hyperledger Fabric, which is
a well-known consortium blockchain. First, we employ erasure coding to
partition files, subsequently organizing these chunks into a hierarchical
structure that fosters efficient and dependable data storage. Second, we design
a two-layer hash-slots mechanism and a mirror strategy, enabling high data
availability. Third, we design an access control mechanism based on a smart
contract to regulate file access.",http://arxiv.org/pdf/2409.20123v1
Robust LLM safeguarding via refusal feature adversarial training,"Lei Yu, Virginie Do, Karen Hambardzumyan, Nicola Cancedda",2024-09-30,"Large language models (LLMs) are vulnerable to adversarial attacks that can
elicit harmful responses. Defending against such attacks remains challenging
due to the opacity of jailbreaking mechanisms and the high computational cost
of training LLMs robustly. We demonstrate that adversarial attacks share a
universal mechanism for circumventing LLM safeguards that works by ablating a
dimension in the residual stream embedding space called the refusal feature. We
further show that the operation of refusal feature ablation (RFA) approximates
the worst-case perturbation of offsetting model safety. Based on these
findings, we propose Refusal Feature Adversarial Training (ReFAT), a novel
algorithm that efficiently performs LLM adversarial training by simulating the
effect of input-level attacks via RFA. Experiment results show that ReFAT
significantly improves the robustness of three popular LLMs against a wide
range of adversarial attacks, with considerably less computational overhead
compared to existing adversarial training methods.",http://arxiv.org/pdf/2409.20089v1
Building Touch-Less Trust in IoT Devices,Steve Kerrison,2024-09-30,"Trust mechanisms for Internet of Things (IoT) devices are commonly used by
manufacturers and other ecosystem participants. However, end users face a
challenge in establishing trust in devices, particularly as device encounters
become more frequent thanks to the proliferation of new and unique products.
Communication or even physical interaction with a device can expose a user to
various threats, such as biometric theft or exploit of their own device. To
address this, we propose a mechanism for verifying the integrity and
trustworthiness of an IoT device before physical interaction or any significant
communication has taken place.",http://arxiv.org/pdf/2409.20047v1
The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM Serving Systems,"Linke Song, Zixuan Pang, Wenhao Wang, Zihao Wang, XiaoFeng Wang, Hongbo Chen, Wei Song, Yier Jin, Dan Meng, Rui Hou",2024-09-30,"The wide deployment of Large Language Models (LLMs) has given rise to strong
demands for optimizing their inference performance. Today's techniques serving
this purpose primarily focus on reducing latency and improving throughput
through algorithmic and hardware enhancements, while largely overlooking their
privacy side effects, particularly in a multi-user environment. In our
research, for the first time, we discovered a set of new timing side channels
in LLM systems, arising from shared caches and GPU memory allocations, which
can be exploited to infer both confidential system prompts and those issued by
other users. These vulnerabilities echo security challenges observed in
traditional computing systems, highlighting an urgent need to address potential
information leakage in LLM serving infrastructures. In this paper, we report
novel attack strategies designed to exploit such timing side channels inherent
in LLM deployments, specifically targeting the Key-Value (KV) cache and
semantic cache widely used to enhance LLM inference performance. Our approach
leverages timing measurements and classification models to detect cache hits,
allowing an adversary to infer private prompts with high accuracy. We also
propose a token-by-token search algorithm to efficiently recover shared prompt
prefixes in the caches, showing the feasibility of stealing system prompts and
those produced by peer users. Our experimental studies on black-box testing of
popular online LLM services demonstrate that such privacy risks are completely
realistic, with significant consequences. Our findings underscore the need for
robust mitigation to protect LLM systems against such emerging threats.",http://arxiv.org/pdf/2409.20002v1
Mitigating Backdoor Threats to Large Language Models: Advancement and Challenges,"Qin Liu, Wenjie Mo, Terry Tong, Jiashu Xu, Fei Wang, Chaowei Xiao, Muhao Chen",2024-09-30,"The advancement of Large Language Models (LLMs) has significantly impacted
various domains, including Web search, healthcare, and software development.
However, as these models scale, they become more vulnerable to cybersecurity
risks, particularly backdoor attacks. By exploiting the potent memorization
capacity of LLMs, adversaries can easily inject backdoors into LLMs by
manipulating a small portion of training data, leading to malicious behaviors
in downstream applications whenever the hidden backdoor is activated by the
pre-defined triggers. Moreover, emerging learning paradigms like instruction
tuning and reinforcement learning from human feedback (RLHF) exacerbate these
risks as they rely heavily on crowdsourced data and human feedback, which are
not fully controlled. In this paper, we present a comprehensive survey of
emerging backdoor threats to LLMs that appear during LLM development or
inference, and cover recent advancement in both defense and detection
strategies for mitigating backdoor threats to LLMs. We also outline key
challenges in addressing these threats, highlighting areas for future research.",http://arxiv.org/pdf/2409.19993v1
Enhancing Security Using Random Binary Weights in Privacy-Preserving Federated Learning,"Hiroto Sawada, Shoko Imaizumi, Hitoshi Kiya",2024-09-30,"In this paper, we propose a novel method for enhancing security in
privacy-preserving federated learning using the Vision Transformer. In
federated learning, learning is performed by collecting updated information
without collecting raw data from each client. However, the problem is that this
raw data may be inferred from the updated information. Conventional
data-guessing countermeasures (security enhancement methods) for addressing
this issue have a trade-off relationship between privacy protection strength
and learning efficiency, and they generally degrade model performance. In this
paper, we propose a novel method of federated learning that does not degrade
model performance and that is robust against data-guessing attacks on updated
information. In the proposed method, each client independently prepares a
sequence of binary (0 or 1) random numbers, multiplies it by the updated
information, and sends it to a server for model learning. In experiments, the
effectiveness of the proposed method is confirmed in terms of model performance
and resistance to the APRIL (Attention PRIvacy Leakage) restoration attack.",http://arxiv.org/pdf/2409.19988v1
"Comments on ""Privacy-Enhanced Federated Learning Against Poisoning Adversaries""","Thomas Schneider, Ajith Suresh, Hossein Yalame",2024-09-30,"In August 2021, Liu et al. (IEEE TIFS'21) proposed a privacy-enhanced
framework named PEFL to efficiently detect poisoning behaviours in Federated
Learning (FL) using homomorphic encryption. In this article, we show that PEFL
does not preserve privacy. In particular, we illustrate that PEFL reveals the
entire gradient vector of all users in clear to one of the participating
entities, thereby violating privacy. Furthermore, we clearly show that an
immediate fix for this issue is still insufficient to achieve privacy by
pointing out multiple flaws in the proposed system.
  Note: Although our privacy issues mentioned in Section II have been published
in January 2023 (Schneider et. al., IEEE TIFS'23), several subsequent papers
continued to reference Liu et al. (IEEE TIFS'21) as a potential solution for
private federated learning. While a few works have acknowledged the privacy
concerns we raised, several of subsequent works either propagate these errors
or adopt the constructions from Liu et al. (IEEE TIFS'21), thereby
unintentionally inheriting the same privacy vulnerabilities. We believe this
oversight is partly due to the limited visibility of our comments paper at
TIFS'23 (Schneider et. al., IEEE TIFS'23). Consequently, to prevent the
continued propagation of the flawed algorithms in Liu et al. (IEEE TIFS'21)
into future research, we also put this article to an ePrint.",http://arxiv.org/pdf/2409.19964v1
HYDRA-FL: Hybrid Knowledge Distillation for Robust and Accurate Federated Learning,"Momin Ahmad Khan, Yasra Chandio, Fatima Muhammad Anwar",2024-09-30,"Data heterogeneity among Federated Learning (FL) users poses a significant
challenge, resulting in reduced global model performance. The community has
designed various techniques to tackle this issue, among which Knowledge
Distillation (KD)-based techniques are common.
  While these techniques effectively improve performance under high
heterogeneity, they inadvertently cause higher accuracy degradation under model
poisoning attacks (known as attack amplification). This paper presents a case
study to reveal this critical vulnerability in KD-based FL systems. We show why
KD causes this issue through empirical evidence and use it as motivation to
design a hybrid distillation technique. We introduce a novel algorithm, Hybrid
Knowledge Distillation for Robust and Accurate FL (HYDRA-FL), which reduces the
impact of attacks in attack scenarios by offloading some of the KD loss to a
shallow layer via an auxiliary classifier. We model HYDRA-FL as a generic
framework and adapt it to two KD-based FL algorithms, FedNTD and MOON. Using
these two as case studies, we demonstrate that our technique outperforms
baselines in attack settings while maintaining comparable performance in benign
settings.",http://arxiv.org/pdf/2409.19912v2
Optimal RANDAO Manipulation in Ethereum,"Kaya Alpturer, S. Matthew Weinberg",2024-09-30,"It is well-known that RANDAO manipulation is possible in Ethereum if an
adversary controls the proposers assigned to the last slots in an epoch. We
provide a methodology to compute, for any fraction $\alpha$ of stake owned by
an adversary, the maximum fraction $f(\alpha)$ of rounds that a strategic
adversary can propose. We further implement our methodology and compute
$f(\cdot)$ for all $\alpha$. For example, we conclude that an optimal strategic
participant with $5\%$ of the stake can propose a $5.048\%$ fraction of rounds,
$10\%$ of the stake can propose a $10.19\%$ fraction of rounds, and $20\%$ of
the stake can propose a $20.68\%$ fraction of rounds.",http://arxiv.org/pdf/2409.19883v1
Blockchain-enhanced Integrity Verification in Educational Content Assessment Platform: A Lightweight and Cost-Efficient Approach,"Talgar Bayan, Richard Banach, Askar Nurbekov, Makhmud Mustafabek Galy, Adi Sabyrbayev, Zhanat Nurbekova",2024-09-29,"The growing digitization of education presents significant challenges in
maintaining the integrity and trustworthiness of educational content.
Traditional systems often fail to ensure data authenticity and prevent
unauthorized alterations, particularly in the evaluation of teachers'
professional activities, where demand for transparent and secure assessment
mechanisms is increasing. In this context, Blockchain technology offers a novel
solution to address these issues. This paper introduces a Blockchain-enhanced
framework for the Electronic Platform for Expertise of Content (EPEC), a
platform used for reviewing and assessing educational materials. Our approach
integrates the Polygon network, a Layer-2 solution for Ethereum, to securely
store and retrieve encrypted reviews, ensuring both privacy and accountability.
By leveraging Python, Flask, and Web3.py, we interact with a Solidity-based
smart contract to securely link each review to a unique identifier (UID) that
connects on-chain data with real-world databases. The system, containerized
using Docker, facilitates easy deployment and integration through API
endpoints. Our implementation demonstrates significant cost savings, with a
98\% reduction in gas fees compared to Ethereum, making it a scalable and
cost-effective solution. This research contributes to the ongoing effort to
implement Blockchain in educational content verification, offering a practical
and secure framework that enhances trust and transparency in the digital
education landscape.",http://arxiv.org/pdf/2409.19828v1
PhishGuard: A Multi-Layered Ensemble Model for Optimal Phishing Website Detection,"Md Sultanul Islam Ovi, Md. Hasibur Rahman, Mohammad Arif Hossain",2024-09-29,"Phishing attacks are a growing cybersecurity threat, leveraging deceptive
techniques to steal sensitive information through malicious websites. To combat
these attacks, this paper introduces PhishGuard, an optimal custom ensemble
model designed to improve phishing site detection. The model combines multiple
machine learning classifiers, including Random Forest, Gradient Boosting,
CatBoost, and XGBoost, to enhance detection accuracy. Through advanced feature
selection methods such as SelectKBest and RFECV, and optimizations like
hyperparameter tuning and data balancing, the model was trained and evaluated
on four publicly available datasets. PhishGuard outperformed state-of-the-art
models, achieving a detection accuracy of 99.05% on one of the datasets, with
similarly high results across other datasets. This research demonstrates that
optimization methods in conjunction with ensemble learning greatly improve
phishing detection performance.",http://arxiv.org/pdf/2409.19825v1
Differentially Private Bilevel Optimization,Guy Kornowski,2024-09-29,"We present differentially private (DP) algorithms for bilevel optimization, a
problem class that received significant attention lately in various machine
learning applications. These are the first DP algorithms for this task that are
able to provide any desired privacy, while also avoiding Hessian computations
which are prohibitive in large-scale settings. Under the well-studied setting
in which the upper-level is not necessarily convex and the lower-level problem
is strongly-convex, our proposed gradient-based $(\epsilon,\delta)$-DP
algorithm returns a point with hypergradient norm at most
$\widetilde{\mathcal{O}}\left((\sqrt{d_\mathrm{up}}/\epsilon
n)^{1/2}+(\sqrt{d_\mathrm{low}}/\epsilon n)^{1/3}\right)$ where $n$ is the
dataset size, and $d_\mathrm{up}/d_\mathrm{low}$ are the upper/lower level
dimensions. Our analysis covers constrained and unconstrained problems alike,
accounts for mini-batch gradients, and applies to both empirical and population
losses.",http://arxiv.org/pdf/2409.19800v1
Membership Inference Attacks Cannot Prove that a Model Was Trained On Your Data,"Jie Zhang, Debeshee Das, Gautam Kamath, Florian Tramèr",2024-09-29,"We consider the problem of a training data proof, where a data creator or
owner wants to demonstrate to a third party that some machine learning model
was trained on their data. Training data proofs play a key role in recent
lawsuits against foundation models trained on web-scale data. Many prior works
suggest to instantiate training data proofs using membership inference attacks.
We argue that this approach is fundamentally unsound: to provide convincing
evidence, the data creator needs to demonstrate that their attack has a low
false positive rate, i.e., that the attack's output is unlikely under the null
hypothesis that the model was not trained on the target data. Yet, sampling
from this null hypothesis is impossible, as we do not know the exact contents
of the training set, nor can we (efficiently) retrain a large foundation model.
We conclude by offering two paths forward, by showing that data extraction
attacks and membership inference on special canary data can be used to create
sound training data proofs.",http://arxiv.org/pdf/2409.19798v1
Advances in Privacy Preserving Federated Learning to Realize a Truly Learning Healthcare System,"Ravi Madduri, Zilinghan Li, Tarak Nandi, Kibaek Kim, Minseok Ryu, Alex Rodriguez",2024-09-29,"The concept of a learning healthcare system (LHS) envisions a self-improving
network where multimodal data from patient care are continuously analyzed to
enhance future healthcare outcomes. However, realizing this vision faces
significant challenges in data sharing and privacy protection.
Privacy-Preserving Federated Learning (PPFL) is a transformative and promising
approach that has the potential to address these challenges by enabling
collaborative learning from decentralized data while safeguarding patient
privacy. This paper proposes a vision for integrating PPFL into the healthcare
ecosystem to achieve a truly LHS as defined by the Institute of Medicine (IOM)
Roundtable.",http://arxiv.org/pdf/2409.19756v1
